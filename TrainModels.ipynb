{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "from model01 import MLPModel01\n",
    "from metrics import performance_report\n",
    "import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_recall_curve, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_categories = 2 # implicit in prepare_data (maybe parameterise)\n",
    "lookahead = 1\n",
    "window = 25\n",
    "sym = 'EURUSD'\n",
    "dataset_name = 'DS3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds_name = sym + '_' + dataset_name + '_20092014' \n",
    "X_train, Y_train, prices_train = \\\n",
    "    datasets.load(datasets.filename(dataset_name, lookahead, window, sym, 20092014))\n",
    "\n",
    "X_dev, Y_dev, prices_dev = \\\n",
    "    datasets.load(datasets.filename(dataset_name, lookahead, window, sym, 2015))\n",
    "\n",
    "# sample 50k records from 2015 as dev set\n",
    "dev_idx = np.random.choice(len(X_dev), 50000, replace=False)\n",
    "X_dev, Y_dev, prices_dev = \\\n",
    "    X_dev.ix[dev_idx], Y_dev.ix[dev_idx], prices_dev.ix[dev_idx]\n",
    "\n",
    "#X_test, Y_test, prices_test = \\\n",
    "#    datasets.load(datasets.filename(dataset_name, lookahead, window, sym, 2016))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EURUSD_DS3_20092014\n",
      "train (2101596, 99)\n",
      "dev (50000, 99)\n",
      "n_features: 99\n"
     ]
    }
   ],
   "source": [
    "print(train_ds_name)\n",
    "print(\"train\", X_train.shape)\n",
    "print(\"dev\", X_dev.shape)\n",
    "#print(\"test\", X_test.shape)\n",
    "n_features = X_train.shape[1]\n",
    "print (\"n_features:\", n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import SGDClassifier\n",
    "# lin_model = SGDClassifier(loss='hinge', penalty='l2', n_iter=200, n_jobs=10)\n",
    "# lin_model.fit(X_train.as_matrix(), Y_train)\n",
    "\n",
    "# print( \"train f1\", f1_score(Y_train, lin_model.predict(X_train), average='weighted') )\n",
    "# print( \"test f1\", f1_score(Y_test, lin_model.predict(X_test), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# lr_model = LogisticRegression(n_jobs=10)\n",
    "# lr_model.fit(X_train.as_matrix(), Y_train)\n",
    "\n",
    "# print( \"train f1\", f1_score(Y_train, lr_model.predict(X_train), average='weighted') )\n",
    "# print( \"test f1\", f1_score(Y_test, lr_model.predict(X_test), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# gnb_model = GaussianNB()\n",
    "# gnb_model.fit(X_train.as_matrix(), Y_train)\n",
    "\n",
    "# print( \"train f1\", f1_score(Y_train, gnb_model.predict(X_train), average='weighted') )\n",
    "# print( \"dev f1\", f1_score(Y_dev, gnb_model.predict(X_dev), average='weighted'))\n",
    "# print( \"test f1\", f1_score(Y_test, gnb_model.predict(X_test), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.externals import joblib\n",
    "# joblib.dump(gnb_model, './output/gnb_{}.pkl'.format(train_ds_name)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# adb_model = AdaBoostClassifier()\n",
    "# adb_model.fit(X_train.as_matrix(), Y_train)\n",
    "\n",
    "# print( \"train f1\", f1_score(Y_train, adb_model.predict(X_train), average='weighted') )\n",
    "# print( \"dev f1\", f1_score(Y_dev, adb_model.predict(X_dev), average='weighted'))\n",
    "# print( \"test f1\", f1_score(Y_test, adb_model.predict(X_test), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train f1 0.542881653974\n",
      "dev f1 0.525201303209\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(max_depth=5, n_jobs=12)\n",
    "rf_model.fit(X_train.as_matrix(), Y_train)\n",
    "\n",
    "print( \"train f1\", f1_score(Y_train, rf_model.predict(X_train), average='weighted') )\n",
    "print( \"dev f1\", f1_score(Y_dev, rf_model.predict(X_dev), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./output/rf5_EURUSD_DS3_20092014.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(rf_model, './output/rf5_{}.pkl'.format(train_ds_name)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_widths = [500,500] \n",
    "\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compilation took: 0.3 seconds\n",
      "Model id:  MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                  (None, 500)           50000       dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 500)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 500)           250500      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 500)           0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             501         dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 301,001\n",
      "Trainable params: 301,001\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = MLPModel01(train_ds_name, lookahead, n_features, n_categories, layer_widths, dropout)\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss: 1.07110 val_loss 0.74823  train_accuracy 0.5024 val_accuracy 0.5017  train_f1 0.4783 val_f1 0.6487\n",
      "Epoch 1, train_loss: 0.81735 val_loss 0.69655  train_accuracy 0.5047 val_accuracy 0.5097  train_f1 0.4925 val_f1 0.4888\n",
      "Epoch 2, train_loss: 0.75079 val_loss 0.69431  train_accuracy 0.5053 val_accuracy 0.5079  train_f1 0.4956 val_f1 0.5404\n",
      "Epoch 3, train_loss: 0.72528 val_loss 0.69315  train_accuracy 0.5065 val_accuracy 0.5075  train_f1 0.4984 val_f1 0.5294\n",
      "Epoch 4, train_loss: 0.71344 val_loss 0.69261  train_accuracy 0.5074 val_accuracy 0.5119  train_f1 0.4970 val_f1 0.5063\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-005-0.6926.hdf5\n",
      "Epoch 5, train_loss: 0.70653 val_loss 0.69246  train_accuracy 0.5089 val_accuracy 0.5131  train_f1 0.4959 val_f1 0.4985\n",
      "Epoch 6, train_loss: 0.70216 val_loss 0.69238  train_accuracy 0.5109 val_accuracy 0.5140  train_f1 0.4970 val_f1 0.4858\n",
      "Epoch 7, train_loss: 0.69961 val_loss 0.69232  train_accuracy 0.5116 val_accuracy 0.5161  train_f1 0.4970 val_f1 0.4775\n",
      "Epoch 8, train_loss: 0.69785 val_loss 0.69228  train_accuracy 0.5125 val_accuracy 0.5175  train_f1 0.4957 val_f1 0.4856\n",
      "Epoch 9, train_loss: 0.69636 val_loss 0.69226  train_accuracy 0.5138 val_accuracy 0.5175  train_f1 0.4959 val_f1 0.4775\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-010-0.6923.hdf5\n",
      "Epoch 10, train_loss: 0.69536 val_loss 0.69222  train_accuracy 0.5156 val_accuracy 0.5184  train_f1 0.4969 val_f1 0.4841\n",
      "Epoch 11, train_loss: 0.69473 val_loss 0.69219  train_accuracy 0.5158 val_accuracy 0.5185  train_f1 0.4963 val_f1 0.4725\n",
      "Epoch 12, train_loss: 0.69408 val_loss 0.69215  train_accuracy 0.5169 val_accuracy 0.5196  train_f1 0.4954 val_f1 0.4704\n",
      "Epoch 13, train_loss: 0.69364 val_loss 0.69211  train_accuracy 0.5172 val_accuracy 0.5189  train_f1 0.4923 val_f1 0.4779\n",
      "Epoch 14, train_loss: 0.69336 val_loss 0.69202  train_accuracy 0.5183 val_accuracy 0.5189  train_f1 0.4957 val_f1 0.4680\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-015-0.6920.hdf5\n",
      "Epoch 15, train_loss: 0.69293 val_loss 0.69199  train_accuracy 0.5198 val_accuracy 0.5194  train_f1 0.4960 val_f1 0.4691\n",
      "Epoch 16, train_loss: 0.69266 val_loss 0.69191  train_accuracy 0.5204 val_accuracy 0.5203  train_f1 0.4957 val_f1 0.4822\n",
      "Epoch 17, train_loss: 0.69251 val_loss 0.69190  train_accuracy 0.5208 val_accuracy 0.5195  train_f1 0.4952 val_f1 0.4809\n",
      "Epoch 18, train_loss: 0.69225 val_loss 0.69188  train_accuracy 0.5221 val_accuracy 0.5202  train_f1 0.4984 val_f1 0.4807\n",
      "Epoch 19, train_loss: 0.69213 val_loss 0.69186  train_accuracy 0.5224 val_accuracy 0.5207  train_f1 0.4951 val_f1 0.4865\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-020-0.6919.hdf5\n",
      "Epoch 20, train_loss: 0.69191 val_loss 0.69180  train_accuracy 0.5229 val_accuracy 0.5203  train_f1 0.4972 val_f1 0.4830\n",
      "Epoch 21, train_loss: 0.69182 val_loss 0.69181  train_accuracy 0.5236 val_accuracy 0.5201  train_f1 0.4972 val_f1 0.4823\n",
      "Epoch 22, train_loss: 0.69171 val_loss 0.69180  train_accuracy 0.5237 val_accuracy 0.5200  train_f1 0.4969 val_f1 0.5008\n",
      "Epoch 23, train_loss: 0.69159 val_loss 0.69176  train_accuracy 0.5242 val_accuracy 0.5196  train_f1 0.4978 val_f1 0.4947\n",
      "Epoch 24, train_loss: 0.69149 val_loss 0.69175  train_accuracy 0.5252 val_accuracy 0.5196  train_f1 0.4986 val_f1 0.4933\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-025-0.6918.hdf5\n",
      "Epoch 25, train_loss: 0.69139 val_loss 0.69168  train_accuracy 0.5258 val_accuracy 0.5192  train_f1 0.5008 val_f1 0.4897\n",
      "Epoch 26, train_loss: 0.69133 val_loss 0.69166  train_accuracy 0.5260 val_accuracy 0.5195  train_f1 0.5017 val_f1 0.4943\n",
      "Epoch 27, train_loss: 0.69119 val_loss 0.69166  train_accuracy 0.5266 val_accuracy 0.5196  train_f1 0.4997 val_f1 0.4923\n",
      "Epoch 28, train_loss: 0.69110 val_loss 0.69165  train_accuracy 0.5267 val_accuracy 0.5191  train_f1 0.5001 val_f1 0.4951\n",
      "Epoch 29, train_loss: 0.69103 val_loss 0.69161  train_accuracy 0.5272 val_accuracy 0.5196  train_f1 0.5025 val_f1 0.4970\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-030-0.6916.hdf5\n",
      "Epoch 30, train_loss: 0.69104 val_loss 0.69157  train_accuracy 0.5272 val_accuracy 0.5195  train_f1 0.5023 val_f1 0.4893\n",
      "Epoch 31, train_loss: 0.69098 val_loss 0.69156  train_accuracy 0.5275 val_accuracy 0.5194  train_f1 0.5020 val_f1 0.4875\n",
      "Epoch 32, train_loss: 0.69085 val_loss 0.69152  train_accuracy 0.5284 val_accuracy 0.5199  train_f1 0.5061 val_f1 0.4925\n",
      "Epoch 33, train_loss: 0.69087 val_loss 0.69151  train_accuracy 0.5285 val_accuracy 0.5207  train_f1 0.5051 val_f1 0.5039\n",
      "Epoch 34, train_loss: 0.69076 val_loss 0.69154  train_accuracy 0.5288 val_accuracy 0.5207  train_f1 0.5049 val_f1 0.5002\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-035-0.6915.hdf5\n",
      "Epoch 35, train_loss: 0.69072 val_loss 0.69147  train_accuracy 0.5291 val_accuracy 0.5212  train_f1 0.5060 val_f1 0.4953\n",
      "Epoch 36, train_loss: 0.69070 val_loss 0.69146  train_accuracy 0.5289 val_accuracy 0.5219  train_f1 0.5067 val_f1 0.5007\n",
      "Epoch 37, train_loss: 0.69064 val_loss 0.69145  train_accuracy 0.5297 val_accuracy 0.5222  train_f1 0.5068 val_f1 0.5069\n",
      "Epoch 38, train_loss: 0.69062 val_loss 0.69148  train_accuracy 0.5297 val_accuracy 0.5226  train_f1 0.5066 val_f1 0.5150\n",
      "Epoch 39, train_loss: 0.69057 val_loss 0.69140  train_accuracy 0.5300 val_accuracy 0.5222  train_f1 0.5095 val_f1 0.5156\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-040-0.6914.hdf5\n",
      "Epoch 40, train_loss: 0.69053 val_loss 0.69143  train_accuracy 0.5299 val_accuracy 0.5223  train_f1 0.5102 val_f1 0.5081\n",
      "Epoch 41, train_loss: 0.69048 val_loss 0.69142  train_accuracy 0.5303 val_accuracy 0.5223  train_f1 0.5102 val_f1 0.5151\n",
      "Epoch 42, train_loss: 0.69044 val_loss 0.69143  train_accuracy 0.5307 val_accuracy 0.5217  train_f1 0.5097 val_f1 0.5140\n",
      "Epoch 43, train_loss: 0.69040 val_loss 0.69139  train_accuracy 0.5303 val_accuracy 0.5229  train_f1 0.5086 val_f1 0.5134\n",
      "\n",
      "Epoch 00043: reducing learning rate to 0.000200000009499.\n",
      "Epoch 44, train_loss: 0.69032 val_loss 0.69137  train_accuracy 0.5313 val_accuracy 0.5227  train_f1 0.5125 val_f1 0.5098\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-045-0.6914.hdf5\n",
      "Epoch 45, train_loss: 0.69033 val_loss 0.69138  train_accuracy 0.5312 val_accuracy 0.5227  train_f1 0.5104 val_f1 0.5153\n",
      "Epoch 46, train_loss: 0.69031 val_loss 0.69137  train_accuracy 0.5312 val_accuracy 0.5224  train_f1 0.5124 val_f1 0.5076\n",
      "Epoch 47, train_loss: 0.69026 val_loss 0.69137  train_accuracy 0.5318 val_accuracy 0.5224  train_f1 0.5099 val_f1 0.5108\n",
      "Epoch 48, train_loss: 0.69030 val_loss 0.69136  train_accuracy 0.5313 val_accuracy 0.5223  train_f1 0.5109 val_f1 0.5147\n",
      "Epoch 49, train_loss: 0.69029 val_loss 0.69135  train_accuracy 0.5312 val_accuracy 0.5228  train_f1 0.5140 val_f1 0.5123\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-050-0.6914.hdf5\n",
      "Epoch 50, train_loss: 0.69028 val_loss 0.69134  train_accuracy 0.5319 val_accuracy 0.5224  train_f1 0.5123 val_f1 0.5128\n",
      "Epoch 51, train_loss: 0.69032 val_loss 0.69134  train_accuracy 0.5317 val_accuracy 0.5225  train_f1 0.5134 val_f1 0.5127\n",
      "Epoch 52, train_loss: 0.69024 val_loss 0.69135  train_accuracy 0.5315 val_accuracy 0.5227  train_f1 0.5111 val_f1 0.5136\n",
      "Epoch 53, train_loss: 0.69025 val_loss 0.69133  train_accuracy 0.5316 val_accuracy 0.5227  train_f1 0.5127 val_f1 0.5139\n",
      "Epoch 54, train_loss: 0.69032 val_loss 0.69134  train_accuracy 0.5315 val_accuracy 0.5222  train_f1 0.5127 val_f1 0.5141\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-055-0.6913.hdf5\n",
      "Epoch 55, train_loss: 0.69027 val_loss 0.69134  train_accuracy 0.5314 val_accuracy 0.5221  train_f1 0.5126 val_f1 0.5102\n",
      "Epoch 56, train_loss: 0.69024 val_loss 0.69135  train_accuracy 0.5320 val_accuracy 0.5224  train_f1 0.5098 val_f1 0.5131\n",
      "\n",
      "Epoch 00056: reducing learning rate to 5e-05.\n",
      "Epoch 57, train_loss: 0.69025 val_loss 0.69134  train_accuracy 0.5313 val_accuracy 0.5225  train_f1 0.5132 val_f1 0.5136\n",
      "Epoch 58, train_loss: 0.69024 val_loss 0.69134  train_accuracy 0.5314 val_accuracy 0.5227  train_f1 0.5125 val_f1 0.5126\n",
      "Epoch 59, train_loss: 0.69023 val_loss 0.69134  train_accuracy 0.5310 val_accuracy 0.5226  train_f1 0.5115 val_f1 0.5118\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-060-0.6913.hdf5\n",
      "Epoch 60, train_loss: 0.69023 val_loss 0.69134  train_accuracy 0.5319 val_accuracy 0.5226  train_f1 0.5117 val_f1 0.5116\n",
      "Epoch 61, train_loss: 0.69022 val_loss 0.69134  train_accuracy 0.5317 val_accuracy 0.5227  train_f1 0.5122 val_f1 0.5129\n",
      "Epoch 62, train_loss: 0.69024 val_loss 0.69133  train_accuracy 0.5316 val_accuracy 0.5226  train_f1 0.5122 val_f1 0.5121\n",
      "Epoch 63, train_loss: 0.69017 val_loss 0.69133  train_accuracy 0.5319 val_accuracy 0.5228  train_f1 0.5111 val_f1 0.5110\n",
      "Epoch 64, train_loss: 0.69022 val_loss 0.69133  train_accuracy 0.5318 val_accuracy 0.5227  train_f1 0.5111 val_f1 0.5128\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-065-0.6913.hdf5\n",
      "Epoch 65, train_loss: 0.69019 val_loss 0.69133  train_accuracy 0.5320 val_accuracy 0.5229  train_f1 0.5128 val_f1 0.5132\n",
      "Epoch 66, train_loss: 0.69021 val_loss 0.69133  train_accuracy 0.5319 val_accuracy 0.5229  train_f1 0.5127 val_f1 0.5133\n",
      "Epoch 67, train_loss: 0.69022 val_loss 0.69132  train_accuracy 0.5319 val_accuracy 0.5230  train_f1 0.5117 val_f1 0.5125\n",
      "Epoch 68, train_loss: 0.69021 val_loss 0.69133  train_accuracy 0.5317 val_accuracy 0.5230  train_f1 0.5120 val_f1 0.5132\n",
      "Epoch 69, train_loss: 0.69017 val_loss 0.69133  train_accuracy 0.5320 val_accuracy 0.5229  train_f1 0.5121 val_f1 0.5134\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-070-0.6913.hdf5\n",
      "Epoch 70, train_loss: 0.69022 val_loss 0.69132  train_accuracy 0.5319 val_accuracy 0.5230  train_f1 0.5124 val_f1 0.5137\n",
      "Epoch 71, train_loss: 0.69017 val_loss 0.69132  train_accuracy 0.5321 val_accuracy 0.5229  train_f1 0.5122 val_f1 0.5126\n",
      "Epoch 72, train_loss: 0.69020 val_loss 0.69132  train_accuracy 0.5313 val_accuracy 0.5230  train_f1 0.5117 val_f1 0.5143\n",
      "Epoch 73, train_loss: 0.69021 val_loss 0.69132  train_accuracy 0.5316 val_accuracy 0.5232  train_f1 0.5129 val_f1 0.5140\n",
      "Epoch 74, train_loss: 0.69014 val_loss 0.69132  train_accuracy 0.5319 val_accuracy 0.5229  train_f1 0.5123 val_f1 0.5135\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-075-0.6913.hdf5\n",
      "Epoch 75, train_loss: 0.69020 val_loss 0.69132  train_accuracy 0.5319 val_accuracy 0.5229  train_f1 0.5127 val_f1 0.5143\n",
      "Epoch 76, train_loss: 0.69022 val_loss 0.69132  train_accuracy 0.5317 val_accuracy 0.5231  train_f1 0.5126 val_f1 0.5145\n",
      "Epoch 77, train_loss: 0.69016 val_loss 0.69132  train_accuracy 0.5319 val_accuracy 0.5232  train_f1 0.5130 val_f1 0.5145\n",
      "Epoch 78, train_loss: 0.69019 val_loss 0.69131  train_accuracy 0.5317 val_accuracy 0.5230  train_f1 0.5126 val_f1 0.5137\n",
      "Epoch 79, train_loss: 0.69019 val_loss 0.69131  train_accuracy 0.5318 val_accuracy 0.5231  train_f1 0.5118 val_f1 0.5137\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-080-0.6913.hdf5\n",
      "Epoch 80, train_loss: 0.69014 val_loss 0.69131  train_accuracy 0.5321 val_accuracy 0.5233  train_f1 0.5134 val_f1 0.5154\n",
      "Epoch 81, train_loss: 0.69019 val_loss 0.69132  train_accuracy 0.5321 val_accuracy 0.5230  train_f1 0.5132 val_f1 0.5136\n",
      "Epoch 82, train_loss: 0.69018 val_loss 0.69131  train_accuracy 0.5323 val_accuracy 0.5232  train_f1 0.5130 val_f1 0.5148\n",
      "Epoch 83, train_loss: 0.69016 val_loss 0.69131  train_accuracy 0.5320 val_accuracy 0.5229  train_f1 0.5128 val_f1 0.5136\n",
      "Epoch 84, train_loss: 0.69021 val_loss 0.69131  train_accuracy 0.5317 val_accuracy 0.5231  train_f1 0.5122 val_f1 0.5141\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-085-0.6913.hdf5\n",
      "Epoch 85, train_loss: 0.69018 val_loss 0.69131  train_accuracy 0.5319 val_accuracy 0.5231  train_f1 0.5130 val_f1 0.5151\n",
      "Epoch 86, train_loss: 0.69019 val_loss 0.69131  train_accuracy 0.5320 val_accuracy 0.5230  train_f1 0.5141 val_f1 0.5145\n",
      "Epoch 87, train_loss: 0.69014 val_loss 0.69130  train_accuracy 0.5325 val_accuracy 0.5234  train_f1 0.5137 val_f1 0.5147\n",
      "Epoch 88, train_loss: 0.69013 val_loss 0.69130  train_accuracy 0.5322 val_accuracy 0.5233  train_f1 0.5123 val_f1 0.5133\n",
      "Epoch 89, train_loss: 0.69020 val_loss 0.69130  train_accuracy 0.5320 val_accuracy 0.5233  train_f1 0.5126 val_f1 0.5147\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-090-0.6913.hdf5\n",
      "Epoch 90, train_loss: 0.69017 val_loss 0.69130  train_accuracy 0.5320 val_accuracy 0.5235  train_f1 0.5126 val_f1 0.5142\n",
      "Epoch 91, train_loss: 0.69011 val_loss 0.69129  train_accuracy 0.5322 val_accuracy 0.5233  train_f1 0.5134 val_f1 0.5142\n",
      "Epoch 92, train_loss: 0.69010 val_loss 0.69130  train_accuracy 0.5323 val_accuracy 0.5235  train_f1 0.5132 val_f1 0.5154\n",
      "Epoch 93, train_loss: 0.69012 val_loss 0.69130  train_accuracy 0.5322 val_accuracy 0.5237  train_f1 0.5142 val_f1 0.5163\n",
      "Epoch 94, train_loss: 0.69020 val_loss 0.69129  train_accuracy 0.5319 val_accuracy 0.5238  train_f1 0.5145 val_f1 0.5158\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-095-0.6913.hdf5\n",
      "Epoch 95, train_loss: 0.69016 val_loss 0.69130  train_accuracy 0.5322 val_accuracy 0.5236  train_f1 0.5134 val_f1 0.5144\n",
      "Epoch 96, train_loss: 0.69011 val_loss 0.69130  train_accuracy 0.5324 val_accuracy 0.5236  train_f1 0.5137 val_f1 0.5159\n",
      "Epoch 97, train_loss: 0.69011 val_loss 0.69130  train_accuracy 0.5323 val_accuracy 0.5238  train_f1 0.5136 val_f1 0.5149\n",
      "Epoch 98, train_loss: 0.69012 val_loss 0.69130  train_accuracy 0.5321 val_accuracy 0.5236  train_f1 0.5140 val_f1 0.5164\n",
      "Epoch 99, train_loss: 0.69017 val_loss 0.69130  train_accuracy 0.5321 val_accuracy 0.5237  train_f1 0.5143 val_f1 0.5162\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-100-0.6913.hdf5\n",
      "Epoch 100, train_loss: 0.69015 val_loss 0.69130  train_accuracy 0.5320 val_accuracy 0.5234  train_f1 0.5129 val_f1 0.5137\n",
      "Epoch 101, train_loss: 0.69012 val_loss 0.69130  train_accuracy 0.5325 val_accuracy 0.5236  train_f1 0.5138 val_f1 0.5154\n",
      "Epoch 102, train_loss: 0.69011 val_loss 0.69129  train_accuracy 0.5322 val_accuracy 0.5239  train_f1 0.5138 val_f1 0.5162\n",
      "Epoch 103, train_loss: 0.69013 val_loss 0.69129  train_accuracy 0.5321 val_accuracy 0.5238  train_f1 0.5141 val_f1 0.5164\n",
      "Epoch 104, train_loss: 0.69010 val_loss 0.69128  train_accuracy 0.5325 val_accuracy 0.5240  train_f1 0.5139 val_f1 0.5155\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-105-0.6913.hdf5\n",
      "Epoch 105, train_loss: 0.69010 val_loss 0.69129  train_accuracy 0.5323 val_accuracy 0.5241  train_f1 0.5134 val_f1 0.5156\n",
      "Epoch 106, train_loss: 0.69015 val_loss 0.69129  train_accuracy 0.5324 val_accuracy 0.5238  train_f1 0.5133 val_f1 0.5158\n",
      "Epoch 107, train_loss: 0.69013 val_loss 0.69128  train_accuracy 0.5321 val_accuracy 0.5238  train_f1 0.5143 val_f1 0.5164\n",
      "Epoch 108, train_loss: 0.69013 val_loss 0.69128  train_accuracy 0.5320 val_accuracy 0.5239  train_f1 0.5135 val_f1 0.5155\n",
      "Epoch 109, train_loss: 0.69012 val_loss 0.69128  train_accuracy 0.5322 val_accuracy 0.5240  train_f1 0.5126 val_f1 0.5147\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-110-0.6913.hdf5\n",
      "Epoch 110, train_loss: 0.69012 val_loss 0.69128  train_accuracy 0.5320 val_accuracy 0.5238  train_f1 0.5136 val_f1 0.5158\n",
      "Epoch 111, train_loss: 0.69007 val_loss 0.69128  train_accuracy 0.5324 val_accuracy 0.5238  train_f1 0.5140 val_f1 0.5162\n",
      "Epoch 112, train_loss: 0.69013 val_loss 0.69128  train_accuracy 0.5324 val_accuracy 0.5240  train_f1 0.5136 val_f1 0.5162\n",
      "Epoch 113, train_loss: 0.69010 val_loss 0.69128  train_accuracy 0.5327 val_accuracy 0.5241  train_f1 0.5139 val_f1 0.5162\n",
      "Epoch 114, train_loss: 0.69011 val_loss 0.69128  train_accuracy 0.5324 val_accuracy 0.5241  train_f1 0.5157 val_f1 0.5179\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-115-0.6913.hdf5\n",
      "Epoch 115, train_loss: 0.69009 val_loss 0.69128  train_accuracy 0.5330 val_accuracy 0.5243  train_f1 0.5152 val_f1 0.5169\n",
      "Epoch 116, train_loss: 0.69006 val_loss 0.69127  train_accuracy 0.5326 val_accuracy 0.5241  train_f1 0.5141 val_f1 0.5162\n",
      "Epoch 117, train_loss: 0.69012 val_loss 0.69128  train_accuracy 0.5326 val_accuracy 0.5239  train_f1 0.5135 val_f1 0.5165\n",
      "Epoch 118, train_loss: 0.69007 val_loss 0.69127  train_accuracy 0.5325 val_accuracy 0.5243  train_f1 0.5142 val_f1 0.5169\n",
      "Epoch 119, train_loss: 0.69007 val_loss 0.69127  train_accuracy 0.5323 val_accuracy 0.5244  train_f1 0.5141 val_f1 0.5176\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-120-0.6913.hdf5\n",
      "Epoch 120, train_loss: 0.69007 val_loss 0.69127  train_accuracy 0.5327 val_accuracy 0.5242  train_f1 0.5135 val_f1 0.5154\n",
      "Epoch 121, train_loss: 0.68998 val_loss 0.69127  train_accuracy 0.5329 val_accuracy 0.5245  train_f1 0.5138 val_f1 0.5166\n",
      "Epoch 122, train_loss: 0.69005 val_loss 0.69126  train_accuracy 0.5329 val_accuracy 0.5243  train_f1 0.5135 val_f1 0.5154\n",
      "Epoch 123, train_loss: 0.69004 val_loss 0.69127  train_accuracy 0.5325 val_accuracy 0.5242  train_f1 0.5141 val_f1 0.5175\n",
      "Epoch 124, train_loss: 0.69001 val_loss 0.69126  train_accuracy 0.5328 val_accuracy 0.5241  train_f1 0.5158 val_f1 0.5175\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-125-0.6913.hdf5\n",
      "Epoch 125, train_loss: 0.69006 val_loss 0.69126  train_accuracy 0.5329 val_accuracy 0.5241  train_f1 0.5142 val_f1 0.5159\n",
      "Epoch 126, train_loss: 0.69006 val_loss 0.69127  train_accuracy 0.5325 val_accuracy 0.5239  train_f1 0.5139 val_f1 0.5180\n",
      "Epoch 127, train_loss: 0.69006 val_loss 0.69126  train_accuracy 0.5328 val_accuracy 0.5238  train_f1 0.5151 val_f1 0.5174\n",
      "Epoch 128, train_loss: 0.69002 val_loss 0.69126  train_accuracy 0.5328 val_accuracy 0.5240  train_f1 0.5159 val_f1 0.5183\n",
      "Epoch 129, train_loss: 0.69003 val_loss 0.69126  train_accuracy 0.5328 val_accuracy 0.5244  train_f1 0.5146 val_f1 0.5172\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-130-0.6913.hdf5\n",
      "Epoch 130, train_loss: 0.69004 val_loss 0.69126  train_accuracy 0.5324 val_accuracy 0.5238  train_f1 0.5142 val_f1 0.5177\n",
      "Epoch 131, train_loss: 0.69001 val_loss 0.69126  train_accuracy 0.5332 val_accuracy 0.5242  train_f1 0.5152 val_f1 0.5168\n",
      "Epoch 132, train_loss: 0.69008 val_loss 0.69126  train_accuracy 0.5324 val_accuracy 0.5239  train_f1 0.5141 val_f1 0.5173\n",
      "Epoch 133, train_loss: 0.69007 val_loss 0.69126  train_accuracy 0.5325 val_accuracy 0.5239  train_f1 0.5153 val_f1 0.5182\n",
      "Epoch 134, train_loss: 0.69001 val_loss 0.69125  train_accuracy 0.5326 val_accuracy 0.5240  train_f1 0.5152 val_f1 0.5181\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-135-0.6913.hdf5\n",
      "Epoch 135, train_loss: 0.69004 val_loss 0.69125  train_accuracy 0.5327 val_accuracy 0.5242  train_f1 0.5137 val_f1 0.5174\n",
      "Epoch 136, train_loss: 0.69003 val_loss 0.69126  train_accuracy 0.5329 val_accuracy 0.5240  train_f1 0.5155 val_f1 0.5182\n",
      "Epoch 137, train_loss: 0.69001 val_loss 0.69125  train_accuracy 0.5328 val_accuracy 0.5241  train_f1 0.5139 val_f1 0.5175\n",
      "Epoch 138, train_loss: 0.69002 val_loss 0.69125  train_accuracy 0.5330 val_accuracy 0.5243  train_f1 0.5157 val_f1 0.5187\n",
      "Epoch 139, train_loss: 0.69006 val_loss 0.69125  train_accuracy 0.5324 val_accuracy 0.5244  train_f1 0.5146 val_f1 0.5181\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-140-0.6913.hdf5\n",
      "Epoch 140, train_loss: 0.69000 val_loss 0.69125  train_accuracy 0.5330 val_accuracy 0.5245  train_f1 0.5150 val_f1 0.5188\n",
      "Epoch 141, train_loss: 0.69004 val_loss 0.69125  train_accuracy 0.5328 val_accuracy 0.5246  train_f1 0.5150 val_f1 0.5186\n",
      "Epoch 142, train_loss: 0.68998 val_loss 0.69125  train_accuracy 0.5330 val_accuracy 0.5247  train_f1 0.5155 val_f1 0.5187\n",
      "Epoch 143, train_loss: 0.69002 val_loss 0.69125  train_accuracy 0.5324 val_accuracy 0.5242  train_f1 0.5148 val_f1 0.5186\n",
      "Epoch 144, train_loss: 0.68998 val_loss 0.69125  train_accuracy 0.5332 val_accuracy 0.5241  train_f1 0.5160 val_f1 0.5186\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-145-0.6912.hdf5\n",
      "Epoch 145, train_loss: 0.69003 val_loss 0.69125  train_accuracy 0.5326 val_accuracy 0.5242  train_f1 0.5152 val_f1 0.5186\n",
      "Epoch 146, train_loss: 0.69000 val_loss 0.69125  train_accuracy 0.5328 val_accuracy 0.5244  train_f1 0.5152 val_f1 0.5190\n",
      "Epoch 147, train_loss: 0.69003 val_loss 0.69125  train_accuracy 0.5325 val_accuracy 0.5246  train_f1 0.5150 val_f1 0.5196\n",
      "Epoch 148, train_loss: 0.68999 val_loss 0.69125  train_accuracy 0.5329 val_accuracy 0.5245  train_f1 0.5164 val_f1 0.5198\n",
      "Epoch 149, train_loss: 0.69000 val_loss 0.69124  train_accuracy 0.5332 val_accuracy 0.5245  train_f1 0.5155 val_f1 0.5195\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-150-0.6912.hdf5\n",
      "Epoch 150, train_loss: 0.68998 val_loss 0.69124  train_accuracy 0.5328 val_accuracy 0.5244  train_f1 0.5164 val_f1 0.5193\n",
      "Epoch 151, train_loss: 0.68996 val_loss 0.69124  train_accuracy 0.5329 val_accuracy 0.5243  train_f1 0.5154 val_f1 0.5183\n",
      "Epoch 152, train_loss: 0.68993 val_loss 0.69124  train_accuracy 0.5332 val_accuracy 0.5241  train_f1 0.5156 val_f1 0.5183\n",
      "Epoch 153, train_loss: 0.68995 val_loss 0.69124  train_accuracy 0.5333 val_accuracy 0.5243  train_f1 0.5157 val_f1 0.5193\n",
      "Epoch 154, train_loss: 0.68993 val_loss 0.69123  train_accuracy 0.5330 val_accuracy 0.5242  train_f1 0.5166 val_f1 0.5191\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-155-0.6912.hdf5\n",
      "Epoch 155, train_loss: 0.68999 val_loss 0.69123  train_accuracy 0.5326 val_accuracy 0.5243  train_f1 0.5154 val_f1 0.5175\n",
      "Epoch 156, train_loss: 0.68994 val_loss 0.69123  train_accuracy 0.5331 val_accuracy 0.5244  train_f1 0.5144 val_f1 0.5179\n",
      "Epoch 157, train_loss: 0.68999 val_loss 0.69123  train_accuracy 0.5330 val_accuracy 0.5243  train_f1 0.5166 val_f1 0.5193\n",
      "Epoch 158, train_loss: 0.68986 val_loss 0.69123  train_accuracy 0.5335 val_accuracy 0.5242  train_f1 0.5162 val_f1 0.5181\n",
      "Epoch 159, train_loss: 0.68998 val_loss 0.69122  train_accuracy 0.5332 val_accuracy 0.5242  train_f1 0.5173 val_f1 0.5184\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-160-0.6912.hdf5\n",
      "Epoch 160, train_loss: 0.68993 val_loss 0.69122  train_accuracy 0.5333 val_accuracy 0.5243  train_f1 0.5157 val_f1 0.5179\n",
      "Epoch 161, train_loss: 0.68993 val_loss 0.69122  train_accuracy 0.5334 val_accuracy 0.5243  train_f1 0.5158 val_f1 0.5187\n",
      "Epoch 162, train_loss: 0.68991 val_loss 0.69122  train_accuracy 0.5334 val_accuracy 0.5243  train_f1 0.5176 val_f1 0.5206\n",
      "Epoch 163, train_loss: 0.68997 val_loss 0.69122  train_accuracy 0.5327 val_accuracy 0.5244  train_f1 0.5150 val_f1 0.5190\n",
      "Epoch 164, train_loss: 0.68991 val_loss 0.69122  train_accuracy 0.5335 val_accuracy 0.5243  train_f1 0.5165 val_f1 0.5197\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-165-0.6912.hdf5\n",
      "Epoch 165, train_loss: 0.68988 val_loss 0.69122  train_accuracy 0.5334 val_accuracy 0.5242  train_f1 0.5165 val_f1 0.5194\n",
      "Epoch 166, train_loss: 0.68989 val_loss 0.69122  train_accuracy 0.5338 val_accuracy 0.5242  train_f1 0.5160 val_f1 0.5193\n",
      "Epoch 167, train_loss: 0.68992 val_loss 0.69122  train_accuracy 0.5332 val_accuracy 0.5246  train_f1 0.5172 val_f1 0.5196\n",
      "Epoch 168, train_loss: 0.68990 val_loss 0.69122  train_accuracy 0.5333 val_accuracy 0.5248  train_f1 0.5168 val_f1 0.5190\n",
      "Epoch 169, train_loss: 0.68994 val_loss 0.69122  train_accuracy 0.5330 val_accuracy 0.5249  train_f1 0.5153 val_f1 0.5190\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-170-0.6912.hdf5\n",
      "Epoch 170, train_loss: 0.68994 val_loss 0.69121  train_accuracy 0.5332 val_accuracy 0.5250  train_f1 0.5159 val_f1 0.5200\n",
      "Epoch 171, train_loss: 0.68987 val_loss 0.69121  train_accuracy 0.5335 val_accuracy 0.5247  train_f1 0.5167 val_f1 0.5194\n",
      "Epoch 172, train_loss: 0.68992 val_loss 0.69121  train_accuracy 0.5334 val_accuracy 0.5248  train_f1 0.5158 val_f1 0.5186\n",
      "Epoch 173, train_loss: 0.68987 val_loss 0.69121  train_accuracy 0.5338 val_accuracy 0.5246  train_f1 0.5173 val_f1 0.5197\n",
      "Epoch 174, train_loss: 0.68996 val_loss 0.69122  train_accuracy 0.5329 val_accuracy 0.5248  train_f1 0.5159 val_f1 0.5192\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-175-0.6912.hdf5\n",
      "Epoch 175, train_loss: 0.68986 val_loss 0.69121  train_accuracy 0.5336 val_accuracy 0.5251  train_f1 0.5157 val_f1 0.5196\n",
      "Epoch 176, train_loss: 0.68989 val_loss 0.69121  train_accuracy 0.5335 val_accuracy 0.5247  train_f1 0.5163 val_f1 0.5197\n",
      "Epoch 177, train_loss: 0.68988 val_loss 0.69121  train_accuracy 0.5332 val_accuracy 0.5247  train_f1 0.5162 val_f1 0.5193\n",
      "Epoch 178, train_loss: 0.68997 val_loss 0.69121  train_accuracy 0.5331 val_accuracy 0.5248  train_f1 0.5161 val_f1 0.5199\n",
      "Epoch 179, train_loss: 0.68982 val_loss 0.69121  train_accuracy 0.5336 val_accuracy 0.5247  train_f1 0.5173 val_f1 0.5208\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-180-0.6912.hdf5\n",
      "Epoch 180, train_loss: 0.68986 val_loss 0.69121  train_accuracy 0.5338 val_accuracy 0.5250  train_f1 0.5161 val_f1 0.5197\n",
      "Epoch 181, train_loss: 0.68992 val_loss 0.69120  train_accuracy 0.5334 val_accuracy 0.5247  train_f1 0.5167 val_f1 0.5201\n",
      "Epoch 182, train_loss: 0.68988 val_loss 0.69121  train_accuracy 0.5336 val_accuracy 0.5249  train_f1 0.5169 val_f1 0.5195\n",
      "Epoch 183, train_loss: 0.68988 val_loss 0.69121  train_accuracy 0.5334 val_accuracy 0.5248  train_f1 0.5169 val_f1 0.5210\n",
      "Epoch 184, train_loss: 0.68984 val_loss 0.69120  train_accuracy 0.5339 val_accuracy 0.5243  train_f1 0.5167 val_f1 0.5186\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-185-0.6912.hdf5\n",
      "Epoch 185, train_loss: 0.68984 val_loss 0.69120  train_accuracy 0.5337 val_accuracy 0.5244  train_f1 0.5170 val_f1 0.5202\n",
      "Epoch 186, train_loss: 0.68983 val_loss 0.69120  train_accuracy 0.5336 val_accuracy 0.5244  train_f1 0.5174 val_f1 0.5204\n",
      "Epoch 187, train_loss: 0.68988 val_loss 0.69119  train_accuracy 0.5337 val_accuracy 0.5245  train_f1 0.5170 val_f1 0.5195\n",
      "Epoch 188, train_loss: 0.68988 val_loss 0.69120  train_accuracy 0.5334 val_accuracy 0.5246  train_f1 0.5173 val_f1 0.5204\n",
      "Epoch 189, train_loss: 0.68985 val_loss 0.69120  train_accuracy 0.5336 val_accuracy 0.5246  train_f1 0.5174 val_f1 0.5195\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-190-0.6912.hdf5\n",
      "Epoch 190, train_loss: 0.68989 val_loss 0.69120  train_accuracy 0.5335 val_accuracy 0.5245  train_f1 0.5164 val_f1 0.5196\n",
      "Epoch 191, train_loss: 0.68989 val_loss 0.69119  train_accuracy 0.5336 val_accuracy 0.5244  train_f1 0.5160 val_f1 0.5198\n",
      "Epoch 192, train_loss: 0.68984 val_loss 0.69119  train_accuracy 0.5337 val_accuracy 0.5247  train_f1 0.5174 val_f1 0.5206\n",
      "Epoch 193, train_loss: 0.68984 val_loss 0.69118  train_accuracy 0.5339 val_accuracy 0.5244  train_f1 0.5178 val_f1 0.5201\n",
      "Epoch 194, train_loss: 0.68982 val_loss 0.69119  train_accuracy 0.5337 val_accuracy 0.5243  train_f1 0.5188 val_f1 0.5200\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-195-0.6912.hdf5\n",
      "Epoch 195, train_loss: 0.68984 val_loss 0.69118  train_accuracy 0.5338 val_accuracy 0.5245  train_f1 0.5168 val_f1 0.5196\n",
      "Epoch 196, train_loss: 0.68983 val_loss 0.69119  train_accuracy 0.5339 val_accuracy 0.5246  train_f1 0.5173 val_f1 0.5196\n",
      "Epoch 197, train_loss: 0.68982 val_loss 0.69118  train_accuracy 0.5336 val_accuracy 0.5242  train_f1 0.5168 val_f1 0.5197\n",
      "Epoch 198, train_loss: 0.68985 val_loss 0.69118  train_accuracy 0.5337 val_accuracy 0.5243  train_f1 0.5180 val_f1 0.5198\n",
      "Epoch 199, train_loss: 0.68979 val_loss 0.69118  train_accuracy 0.5338 val_accuracy 0.5244  train_f1 0.5178 val_f1 0.5203\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-200-0.6912.hdf5\n",
      "Epoch 200, train_loss: 0.68987 val_loss 0.69118  train_accuracy 0.5336 val_accuracy 0.5243  train_f1 0.5174 val_f1 0.5194\n",
      "Epoch 201, train_loss: 0.68980 val_loss 0.69117  train_accuracy 0.5336 val_accuracy 0.5241  train_f1 0.5172 val_f1 0.5196\n",
      "Epoch 202, train_loss: 0.68981 val_loss 0.69117  train_accuracy 0.5335 val_accuracy 0.5242  train_f1 0.5166 val_f1 0.5190\n",
      "Epoch 203, train_loss: 0.68986 val_loss 0.69117  train_accuracy 0.5338 val_accuracy 0.5244  train_f1 0.5171 val_f1 0.5192\n",
      "Epoch 204, train_loss: 0.68984 val_loss 0.69117  train_accuracy 0.5336 val_accuracy 0.5246  train_f1 0.5170 val_f1 0.5198\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-205-0.6912.hdf5\n",
      "Epoch 205, train_loss: 0.68979 val_loss 0.69118  train_accuracy 0.5339 val_accuracy 0.5248  train_f1 0.5177 val_f1 0.5205\n",
      "Epoch 206, train_loss: 0.68976 val_loss 0.69117  train_accuracy 0.5343 val_accuracy 0.5244  train_f1 0.5180 val_f1 0.5189\n",
      "Epoch 207, train_loss: 0.68979 val_loss 0.69117  train_accuracy 0.5343 val_accuracy 0.5247  train_f1 0.5169 val_f1 0.5207\n",
      "Epoch 208, train_loss: 0.68975 val_loss 0.69117  train_accuracy 0.5342 val_accuracy 0.5246  train_f1 0.5197 val_f1 0.5217\n",
      "Epoch 209, train_loss: 0.68979 val_loss 0.69116  train_accuracy 0.5339 val_accuracy 0.5245  train_f1 0.5167 val_f1 0.5195\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-210-0.6912.hdf5\n",
      "Epoch 210, train_loss: 0.68973 val_loss 0.69116  train_accuracy 0.5344 val_accuracy 0.5245  train_f1 0.5196 val_f1 0.5205\n",
      "Epoch 211, train_loss: 0.68972 val_loss 0.69116  train_accuracy 0.5342 val_accuracy 0.5243  train_f1 0.5171 val_f1 0.5196\n",
      "Epoch 212, train_loss: 0.68975 val_loss 0.69117  train_accuracy 0.5342 val_accuracy 0.5245  train_f1 0.5190 val_f1 0.5194\n",
      "Epoch 213, train_loss: 0.68977 val_loss 0.69117  train_accuracy 0.5340 val_accuracy 0.5249  train_f1 0.5165 val_f1 0.5210\n",
      "Epoch 214, train_loss: 0.68976 val_loss 0.69117  train_accuracy 0.5346 val_accuracy 0.5246  train_f1 0.5187 val_f1 0.5199\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-215-0.6912.hdf5\n",
      "Epoch 215, train_loss: 0.68976 val_loss 0.69117  train_accuracy 0.5342 val_accuracy 0.5246  train_f1 0.5173 val_f1 0.5205\n",
      "Epoch 216, train_loss: 0.68970 val_loss 0.69116  train_accuracy 0.5345 val_accuracy 0.5250  train_f1 0.5196 val_f1 0.5207\n",
      "Epoch 217, train_loss: 0.68977 val_loss 0.69116  train_accuracy 0.5341 val_accuracy 0.5249  train_f1 0.5161 val_f1 0.5211\n",
      "Epoch 218, train_loss: 0.68973 val_loss 0.69116  train_accuracy 0.5339 val_accuracy 0.5249  train_f1 0.5194 val_f1 0.5213\n",
      "Epoch 219, train_loss: 0.68973 val_loss 0.69116  train_accuracy 0.5342 val_accuracy 0.5249  train_f1 0.5161 val_f1 0.5204\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-220-0.6912.hdf5\n",
      "Epoch 220, train_loss: 0.68979 val_loss 0.69116  train_accuracy 0.5341 val_accuracy 0.5244  train_f1 0.5176 val_f1 0.5206\n",
      "Epoch 221, train_loss: 0.68977 val_loss 0.69116  train_accuracy 0.5339 val_accuracy 0.5245  train_f1 0.5179 val_f1 0.5205\n",
      "Epoch 222, train_loss: 0.68974 val_loss 0.69115  train_accuracy 0.5343 val_accuracy 0.5247  train_f1 0.5191 val_f1 0.5196\n",
      "Epoch 223, train_loss: 0.68970 val_loss 0.69115  train_accuracy 0.5344 val_accuracy 0.5244  train_f1 0.5181 val_f1 0.5194\n",
      "Epoch 224, train_loss: 0.68972 val_loss 0.69114  train_accuracy 0.5342 val_accuracy 0.5244  train_f1 0.5167 val_f1 0.5193\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-225-0.6911.hdf5\n",
      "Epoch 225, train_loss: 0.68975 val_loss 0.69115  train_accuracy 0.5341 val_accuracy 0.5247  train_f1 0.5179 val_f1 0.5207\n",
      "Epoch 226, train_loss: 0.68971 val_loss 0.69115  train_accuracy 0.5345 val_accuracy 0.5244  train_f1 0.5178 val_f1 0.5195\n",
      "Epoch 227, train_loss: 0.68972 val_loss 0.69115  train_accuracy 0.5342 val_accuracy 0.5246  train_f1 0.5191 val_f1 0.5205\n",
      "Epoch 228, train_loss: 0.68970 val_loss 0.69115  train_accuracy 0.5345 val_accuracy 0.5245  train_f1 0.5192 val_f1 0.5207\n",
      "Epoch 229, train_loss: 0.68971 val_loss 0.69114  train_accuracy 0.5341 val_accuracy 0.5247  train_f1 0.5180 val_f1 0.5196\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-230-0.6911.hdf5\n",
      "Epoch 230, train_loss: 0.68972 val_loss 0.69114  train_accuracy 0.5341 val_accuracy 0.5250  train_f1 0.5194 val_f1 0.5218\n",
      "Epoch 231, train_loss: 0.68969 val_loss 0.69114  train_accuracy 0.5342 val_accuracy 0.5247  train_f1 0.5189 val_f1 0.5211\n",
      "Epoch 232, train_loss: 0.68970 val_loss 0.69113  train_accuracy 0.5343 val_accuracy 0.5252  train_f1 0.5177 val_f1 0.5217\n",
      "Epoch 233, train_loss: 0.68968 val_loss 0.69114  train_accuracy 0.5345 val_accuracy 0.5249  train_f1 0.5192 val_f1 0.5208\n",
      "Epoch 234, train_loss: 0.68971 val_loss 0.69114  train_accuracy 0.5344 val_accuracy 0.5248  train_f1 0.5180 val_f1 0.5215\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-235-0.6911.hdf5\n",
      "Epoch 235, train_loss: 0.68965 val_loss 0.69113  train_accuracy 0.5346 val_accuracy 0.5249  train_f1 0.5205 val_f1 0.5220\n",
      "Epoch 236, train_loss: 0.68971 val_loss 0.69113  train_accuracy 0.5346 val_accuracy 0.5248  train_f1 0.5191 val_f1 0.5204\n",
      "Epoch 237, train_loss: 0.68969 val_loss 0.69113  train_accuracy 0.5344 val_accuracy 0.5244  train_f1 0.5166 val_f1 0.5211\n",
      "Epoch 238, train_loss: 0.68973 val_loss 0.69114  train_accuracy 0.5342 val_accuracy 0.5247  train_f1 0.5199 val_f1 0.5217\n",
      "Epoch 239, train_loss: 0.68966 val_loss 0.69114  train_accuracy 0.5351 val_accuracy 0.5247  train_f1 0.5185 val_f1 0.5206\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-240-0.6911.hdf5\n",
      "Epoch 240, train_loss: 0.68966 val_loss 0.69113  train_accuracy 0.5339 val_accuracy 0.5249  train_f1 0.5179 val_f1 0.5216\n",
      "Epoch 241, train_loss: 0.68963 val_loss 0.69112  train_accuracy 0.5347 val_accuracy 0.5244  train_f1 0.5206 val_f1 0.5199\n",
      "Epoch 242, train_loss: 0.68963 val_loss 0.69112  train_accuracy 0.5346 val_accuracy 0.5249  train_f1 0.5170 val_f1 0.5209\n",
      "Epoch 243, train_loss: 0.68965 val_loss 0.69113  train_accuracy 0.5345 val_accuracy 0.5245  train_f1 0.5191 val_f1 0.5211\n",
      "Epoch 244, train_loss: 0.68959 val_loss 0.69113  train_accuracy 0.5346 val_accuracy 0.5245  train_f1 0.5163 val_f1 0.5207\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-245-0.6911.hdf5\n",
      "Epoch 245, train_loss: 0.68961 val_loss 0.69112  train_accuracy 0.5350 val_accuracy 0.5246  train_f1 0.5212 val_f1 0.5222\n",
      "Epoch 246, train_loss: 0.68966 val_loss 0.69113  train_accuracy 0.5346 val_accuracy 0.5245  train_f1 0.5201 val_f1 0.5213\n",
      "Epoch 247, train_loss: 0.68962 val_loss 0.69112  train_accuracy 0.5346 val_accuracy 0.5249  train_f1 0.5179 val_f1 0.5211\n",
      "Epoch 248, train_loss: 0.68964 val_loss 0.69112  train_accuracy 0.5347 val_accuracy 0.5245  train_f1 0.5193 val_f1 0.5219\n",
      "Epoch 249, train_loss: 0.68959 val_loss 0.69113  train_accuracy 0.5348 val_accuracy 0.5243  train_f1 0.5196 val_f1 0.5222\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-250-0.6911.hdf5\n",
      "Epoch 250, train_loss: 0.68964 val_loss 0.69112  train_accuracy 0.5346 val_accuracy 0.5246  train_f1 0.5202 val_f1 0.5224\n",
      "Epoch 251, train_loss: 0.68965 val_loss 0.69113  train_accuracy 0.5345 val_accuracy 0.5249  train_f1 0.5193 val_f1 0.5218\n",
      "Epoch 252, train_loss: 0.68960 val_loss 0.69112  train_accuracy 0.5348 val_accuracy 0.5248  train_f1 0.5188 val_f1 0.5205\n",
      "Epoch 253, train_loss: 0.68958 val_loss 0.69112  train_accuracy 0.5350 val_accuracy 0.5248  train_f1 0.5190 val_f1 0.5214\n",
      "Epoch 254, train_loss: 0.68961 val_loss 0.69111  train_accuracy 0.5346 val_accuracy 0.5250  train_f1 0.5184 val_f1 0.5214\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-255-0.6911.hdf5\n",
      "Epoch 255, train_loss: 0.68962 val_loss 0.69110  train_accuracy 0.5349 val_accuracy 0.5249  train_f1 0.5205 val_f1 0.5228\n",
      "Epoch 256, train_loss: 0.68957 val_loss 0.69111  train_accuracy 0.5352 val_accuracy 0.5249  train_f1 0.5183 val_f1 0.5223\n",
      "Epoch 257, train_loss: 0.68962 val_loss 0.69111  train_accuracy 0.5345 val_accuracy 0.5250  train_f1 0.5190 val_f1 0.5221\n",
      "Epoch 258, train_loss: 0.68954 val_loss 0.69112  train_accuracy 0.5348 val_accuracy 0.5247  train_f1 0.5204 val_f1 0.5220\n",
      "Epoch 259, train_loss: 0.68960 val_loss 0.69111  train_accuracy 0.5346 val_accuracy 0.5249  train_f1 0.5194 val_f1 0.5220\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-260-0.6911.hdf5\n",
      "Epoch 260, train_loss: 0.68958 val_loss 0.69110  train_accuracy 0.5349 val_accuracy 0.5249  train_f1 0.5193 val_f1 0.5214\n",
      "Epoch 261, train_loss: 0.68956 val_loss 0.69110  train_accuracy 0.5349 val_accuracy 0.5246  train_f1 0.5215 val_f1 0.5215\n",
      "Epoch 262, train_loss: 0.68958 val_loss 0.69110  train_accuracy 0.5351 val_accuracy 0.5246  train_f1 0.5194 val_f1 0.5211\n",
      "Epoch 263, train_loss: 0.68962 val_loss 0.69111  train_accuracy 0.5345 val_accuracy 0.5243  train_f1 0.5199 val_f1 0.5222\n",
      "Epoch 264, train_loss: 0.68955 val_loss 0.69111  train_accuracy 0.5350 val_accuracy 0.5242  train_f1 0.5201 val_f1 0.5218\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-265-0.6911.hdf5\n",
      "Epoch 265, train_loss: 0.68960 val_loss 0.69110  train_accuracy 0.5350 val_accuracy 0.5246  train_f1 0.5209 val_f1 0.5231\n",
      "Epoch 266, train_loss: 0.68957 val_loss 0.69109  train_accuracy 0.5348 val_accuracy 0.5245  train_f1 0.5185 val_f1 0.5216\n",
      "Epoch 267, train_loss: 0.68961 val_loss 0.69111  train_accuracy 0.5351 val_accuracy 0.5244  train_f1 0.5194 val_f1 0.5239\n",
      "Epoch 268, train_loss: 0.68958 val_loss 0.69110  train_accuracy 0.5349 val_accuracy 0.5246  train_f1 0.5206 val_f1 0.5224\n",
      "Epoch 269, train_loss: 0.68955 val_loss 0.69110  train_accuracy 0.5349 val_accuracy 0.5248  train_f1 0.5193 val_f1 0.5225\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-270-0.6911.hdf5\n",
      "Epoch 270, train_loss: 0.68959 val_loss 0.69110  train_accuracy 0.5349 val_accuracy 0.5250  train_f1 0.5204 val_f1 0.5229\n",
      "Epoch 271, train_loss: 0.68954 val_loss 0.69110  train_accuracy 0.5352 val_accuracy 0.5248  train_f1 0.5203 val_f1 0.5228\n",
      "Epoch 272, train_loss: 0.68952 val_loss 0.69110  train_accuracy 0.5351 val_accuracy 0.5249  train_f1 0.5213 val_f1 0.5215\n",
      "Epoch 273, train_loss: 0.68955 val_loss 0.69109  train_accuracy 0.5353 val_accuracy 0.5244  train_f1 0.5190 val_f1 0.5211\n",
      "Epoch 274, train_loss: 0.68954 val_loss 0.69109  train_accuracy 0.5349 val_accuracy 0.5245  train_f1 0.5194 val_f1 0.5225\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-275-0.6911.hdf5\n",
      "Epoch 275, train_loss: 0.68958 val_loss 0.69110  train_accuracy 0.5348 val_accuracy 0.5244  train_f1 0.5207 val_f1 0.5224\n",
      "Epoch 276, train_loss: 0.68953 val_loss 0.69110  train_accuracy 0.5350 val_accuracy 0.5244  train_f1 0.5212 val_f1 0.5214\n",
      "Epoch 277, train_loss: 0.68955 val_loss 0.69110  train_accuracy 0.5347 val_accuracy 0.5248  train_f1 0.5182 val_f1 0.5236\n",
      "Epoch 278, train_loss: 0.68953 val_loss 0.69109  train_accuracy 0.5351 val_accuracy 0.5248  train_f1 0.5224 val_f1 0.5230\n",
      "Epoch 279, train_loss: 0.68955 val_loss 0.69110  train_accuracy 0.5352 val_accuracy 0.5244  train_f1 0.5202 val_f1 0.5220\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-280-0.6911.hdf5\n",
      "Epoch 280, train_loss: 0.68953 val_loss 0.69109  train_accuracy 0.5349 val_accuracy 0.5246  train_f1 0.5190 val_f1 0.5233\n",
      "Epoch 281, train_loss: 0.68954 val_loss 0.69109  train_accuracy 0.5354 val_accuracy 0.5248  train_f1 0.5212 val_f1 0.5238\n",
      "Epoch 282, train_loss: 0.68951 val_loss 0.69109  train_accuracy 0.5352 val_accuracy 0.5250  train_f1 0.5211 val_f1 0.5236\n",
      "Epoch 283, train_loss: 0.68952 val_loss 0.69109  train_accuracy 0.5353 val_accuracy 0.5250  train_f1 0.5207 val_f1 0.5230\n",
      "Epoch 284, train_loss: 0.68945 val_loss 0.69109  train_accuracy 0.5355 val_accuracy 0.5251  train_f1 0.5208 val_f1 0.5236\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-285-0.6911.hdf5\n",
      "Epoch 285, train_loss: 0.68950 val_loss 0.69109  train_accuracy 0.5359 val_accuracy 0.5247  train_f1 0.5213 val_f1 0.5235\n",
      "Epoch 286, train_loss: 0.68945 val_loss 0.69108  train_accuracy 0.5355 val_accuracy 0.5246  train_f1 0.5199 val_f1 0.5239\n",
      "Epoch 287, train_loss: 0.68949 val_loss 0.69108  train_accuracy 0.5352 val_accuracy 0.5242  train_f1 0.5209 val_f1 0.5216\n",
      "Epoch 288, train_loss: 0.68952 val_loss 0.69109  train_accuracy 0.5351 val_accuracy 0.5239  train_f1 0.5203 val_f1 0.5224\n",
      "Epoch 289, train_loss: 0.68952 val_loss 0.69110  train_accuracy 0.5353 val_accuracy 0.5243  train_f1 0.5220 val_f1 0.5230\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-290-0.6911.hdf5\n",
      "Epoch 290, train_loss: 0.68951 val_loss 0.69109  train_accuracy 0.5352 val_accuracy 0.5248  train_f1 0.5192 val_f1 0.5233\n",
      "Epoch 291, train_loss: 0.68947 val_loss 0.69109  train_accuracy 0.5357 val_accuracy 0.5250  train_f1 0.5224 val_f1 0.5251\n",
      "Epoch 292, train_loss: 0.68950 val_loss 0.69108  train_accuracy 0.5350 val_accuracy 0.5245  train_f1 0.5212 val_f1 0.5232\n",
      "Epoch 293, train_loss: 0.68947 val_loss 0.69108  train_accuracy 0.5355 val_accuracy 0.5244  train_f1 0.5208 val_f1 0.5225\n",
      "Epoch 294, train_loss: 0.68943 val_loss 0.69109  train_accuracy 0.5355 val_accuracy 0.5245  train_f1 0.5206 val_f1 0.5228\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-295-0.6911.hdf5\n",
      "Epoch 295, train_loss: 0.68946 val_loss 0.69109  train_accuracy 0.5354 val_accuracy 0.5247  train_f1 0.5193 val_f1 0.5250\n",
      "Epoch 296, train_loss: 0.68946 val_loss 0.69108  train_accuracy 0.5356 val_accuracy 0.5250  train_f1 0.5225 val_f1 0.5231\n",
      "Epoch 297, train_loss: 0.68951 val_loss 0.69108  train_accuracy 0.5352 val_accuracy 0.5247  train_f1 0.5200 val_f1 0.5237\n",
      "Epoch 298, train_loss: 0.68949 val_loss 0.69107  train_accuracy 0.5356 val_accuracy 0.5245  train_f1 0.5221 val_f1 0.5225\n",
      "Epoch 299, train_loss: 0.68949 val_loss 0.69107  train_accuracy 0.5355 val_accuracy 0.5245  train_f1 0.5205 val_f1 0.5235\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-300-0.6911.hdf5\n",
      "Epoch 300, train_loss: 0.68945 val_loss 0.69107  train_accuracy 0.5352 val_accuracy 0.5251  train_f1 0.5203 val_f1 0.5224\n",
      "Epoch 301, train_loss: 0.68945 val_loss 0.69108  train_accuracy 0.5355 val_accuracy 0.5247  train_f1 0.5210 val_f1 0.5221\n",
      "Epoch 302, train_loss: 0.68948 val_loss 0.69108  train_accuracy 0.5354 val_accuracy 0.5249  train_f1 0.5203 val_f1 0.5238\n",
      "Epoch 303, train_loss: 0.68941 val_loss 0.69107  train_accuracy 0.5356 val_accuracy 0.5248  train_f1 0.5208 val_f1 0.5232\n",
      "Epoch 304, train_loss: 0.68946 val_loss 0.69107  train_accuracy 0.5354 val_accuracy 0.5246  train_f1 0.5204 val_f1 0.5238\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-305-0.6911.hdf5\n",
      "Epoch 305, train_loss: 0.68948 val_loss 0.69107  train_accuracy 0.5355 val_accuracy 0.5248  train_f1 0.5223 val_f1 0.5228\n",
      "Epoch 306, train_loss: 0.68942 val_loss 0.69107  train_accuracy 0.5356 val_accuracy 0.5248  train_f1 0.5208 val_f1 0.5233\n",
      "Epoch 307, train_loss: 0.68945 val_loss 0.69107  train_accuracy 0.5355 val_accuracy 0.5247  train_f1 0.5214 val_f1 0.5236\n",
      "Epoch 308, train_loss: 0.68941 val_loss 0.69107  train_accuracy 0.5356 val_accuracy 0.5248  train_f1 0.5213 val_f1 0.5229\n",
      "Epoch 309, train_loss: 0.68945 val_loss 0.69107  train_accuracy 0.5355 val_accuracy 0.5248  train_f1 0.5206 val_f1 0.5233\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-310-0.6911.hdf5\n",
      "Epoch 310, train_loss: 0.68941 val_loss 0.69107  train_accuracy 0.5357 val_accuracy 0.5248  train_f1 0.5214 val_f1 0.5237\n",
      "Epoch 311, train_loss: 0.68945 val_loss 0.69106  train_accuracy 0.5351 val_accuracy 0.5252  train_f1 0.5214 val_f1 0.5251\n",
      "Epoch 312, train_loss: 0.68943 val_loss 0.69107  train_accuracy 0.5356 val_accuracy 0.5250  train_f1 0.5218 val_f1 0.5245\n",
      "Epoch 313, train_loss: 0.68943 val_loss 0.69106  train_accuracy 0.5352 val_accuracy 0.5254  train_f1 0.5218 val_f1 0.5253\n",
      "Epoch 314, train_loss: 0.68937 val_loss 0.69106  train_accuracy 0.5355 val_accuracy 0.5253  train_f1 0.5208 val_f1 0.5242\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-315-0.6911.hdf5\n",
      "Epoch 315, train_loss: 0.68944 val_loss 0.69106  train_accuracy 0.5350 val_accuracy 0.5254  train_f1 0.5219 val_f1 0.5265\n",
      "Epoch 316, train_loss: 0.68937 val_loss 0.69105  train_accuracy 0.5355 val_accuracy 0.5258  train_f1 0.5200 val_f1 0.5261\n",
      "Epoch 317, train_loss: 0.68941 val_loss 0.69106  train_accuracy 0.5356 val_accuracy 0.5254  train_f1 0.5229 val_f1 0.5261\n",
      "Epoch 318, train_loss: 0.68937 val_loss 0.69105  train_accuracy 0.5355 val_accuracy 0.5252  train_f1 0.5217 val_f1 0.5253\n",
      "Epoch 319, train_loss: 0.68941 val_loss 0.69105  train_accuracy 0.5358 val_accuracy 0.5255  train_f1 0.5212 val_f1 0.5257\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-320-0.6910.hdf5\n",
      "Epoch 320, train_loss: 0.68940 val_loss 0.69105  train_accuracy 0.5353 val_accuracy 0.5254  train_f1 0.5223 val_f1 0.5264\n",
      "Epoch 321, train_loss: 0.68938 val_loss 0.69105  train_accuracy 0.5356 val_accuracy 0.5256  train_f1 0.5227 val_f1 0.5240\n",
      "Epoch 322, train_loss: 0.68939 val_loss 0.69105  train_accuracy 0.5356 val_accuracy 0.5257  train_f1 0.5194 val_f1 0.5259\n",
      "Epoch 323, train_loss: 0.68937 val_loss 0.69106  train_accuracy 0.5360 val_accuracy 0.5257  train_f1 0.5232 val_f1 0.5263\n",
      "Epoch 324, train_loss: 0.68938 val_loss 0.69105  train_accuracy 0.5359 val_accuracy 0.5255  train_f1 0.5204 val_f1 0.5250\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-325-0.6910.hdf5\n",
      "Epoch 325, train_loss: 0.68936 val_loss 0.69104  train_accuracy 0.5360 val_accuracy 0.5253  train_f1 0.5226 val_f1 0.5251\n",
      "Epoch 326, train_loss: 0.68936 val_loss 0.69105  train_accuracy 0.5358 val_accuracy 0.5251  train_f1 0.5207 val_f1 0.5249\n",
      "Epoch 327, train_loss: 0.68940 val_loss 0.69104  train_accuracy 0.5357 val_accuracy 0.5252  train_f1 0.5222 val_f1 0.5245\n",
      "Epoch 328, train_loss: 0.68936 val_loss 0.69106  train_accuracy 0.5357 val_accuracy 0.5252  train_f1 0.5220 val_f1 0.5252\n",
      "Epoch 329, train_loss: 0.68932 val_loss 0.69105  train_accuracy 0.5360 val_accuracy 0.5255  train_f1 0.5213 val_f1 0.5252\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-330-0.6910.hdf5\n",
      "Epoch 330, train_loss: 0.68935 val_loss 0.69105  train_accuracy 0.5359 val_accuracy 0.5257  train_f1 0.5226 val_f1 0.5263\n",
      "Epoch 331, train_loss: 0.68931 val_loss 0.69105  train_accuracy 0.5359 val_accuracy 0.5256  train_f1 0.5213 val_f1 0.5258\n",
      "Epoch 332, train_loss: 0.68938 val_loss 0.69104  train_accuracy 0.5357 val_accuracy 0.5259  train_f1 0.5222 val_f1 0.5265\n",
      "Epoch 333, train_loss: 0.68936 val_loss 0.69104  train_accuracy 0.5356 val_accuracy 0.5256  train_f1 0.5213 val_f1 0.5249\n",
      "Epoch 334, train_loss: 0.68930 val_loss 0.69103  train_accuracy 0.5364 val_accuracy 0.5257  train_f1 0.5225 val_f1 0.5260\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-335-0.6910.hdf5\n",
      "Epoch 335, train_loss: 0.68931 val_loss 0.69103  train_accuracy 0.5359 val_accuracy 0.5255  train_f1 0.5217 val_f1 0.5262\n",
      "Epoch 336, train_loss: 0.68929 val_loss 0.69103  train_accuracy 0.5358 val_accuracy 0.5258  train_f1 0.5225 val_f1 0.5267\n",
      "Epoch 337, train_loss: 0.68936 val_loss 0.69104  train_accuracy 0.5358 val_accuracy 0.5259  train_f1 0.5215 val_f1 0.5268\n",
      "Epoch 338, train_loss: 0.68930 val_loss 0.69104  train_accuracy 0.5357 val_accuracy 0.5259  train_f1 0.5218 val_f1 0.5259\n",
      "Epoch 339, train_loss: 0.68932 val_loss 0.69104  train_accuracy 0.5361 val_accuracy 0.5258  train_f1 0.5231 val_f1 0.5262\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-340-0.6910.hdf5\n",
      "Epoch 340, train_loss: 0.68934 val_loss 0.69103  train_accuracy 0.5362 val_accuracy 0.5258  train_f1 0.5218 val_f1 0.5263\n",
      "Epoch 341, train_loss: 0.68932 val_loss 0.69103  train_accuracy 0.5360 val_accuracy 0.5256  train_f1 0.5237 val_f1 0.5250\n",
      "Epoch 342, train_loss: 0.68930 val_loss 0.69102  train_accuracy 0.5359 val_accuracy 0.5259  train_f1 0.5189 val_f1 0.5258\n",
      "Epoch 343, train_loss: 0.68931 val_loss 0.69103  train_accuracy 0.5358 val_accuracy 0.5258  train_f1 0.5231 val_f1 0.5266\n",
      "Epoch 344, train_loss: 0.68934 val_loss 0.69102  train_accuracy 0.5360 val_accuracy 0.5255  train_f1 0.5209 val_f1 0.5255\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-345-0.6910.hdf5\n",
      "Epoch 345, train_loss: 0.68932 val_loss 0.69102  train_accuracy 0.5362 val_accuracy 0.5259  train_f1 0.5235 val_f1 0.5268\n",
      "Epoch 346, train_loss: 0.68931 val_loss 0.69102  train_accuracy 0.5361 val_accuracy 0.5261  train_f1 0.5228 val_f1 0.5276\n",
      "Epoch 347, train_loss: 0.68929 val_loss 0.69103  train_accuracy 0.5360 val_accuracy 0.5260  train_f1 0.5221 val_f1 0.5275\n",
      "Epoch 348, train_loss: 0.68930 val_loss 0.69103  train_accuracy 0.5360 val_accuracy 0.5259  train_f1 0.5234 val_f1 0.5276\n",
      "Epoch 349, train_loss: 0.68925 val_loss 0.69103  train_accuracy 0.5362 val_accuracy 0.5261  train_f1 0.5228 val_f1 0.5277\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-350-0.6910.hdf5\n",
      "Epoch 350, train_loss: 0.68929 val_loss 0.69102  train_accuracy 0.5360 val_accuracy 0.5260  train_f1 0.5226 val_f1 0.5269\n",
      "Epoch 351, train_loss: 0.68927 val_loss 0.69102  train_accuracy 0.5362 val_accuracy 0.5258  train_f1 0.5226 val_f1 0.5274\n",
      "Epoch 352, train_loss: 0.68927 val_loss 0.69102  train_accuracy 0.5360 val_accuracy 0.5260  train_f1 0.5231 val_f1 0.5278\n",
      "Epoch 353, train_loss: 0.68928 val_loss 0.69102  train_accuracy 0.5364 val_accuracy 0.5259  train_f1 0.5239 val_f1 0.5264\n",
      "Epoch 354, train_loss: 0.68928 val_loss 0.69101  train_accuracy 0.5363 val_accuracy 0.5262  train_f1 0.5225 val_f1 0.5270\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-355-0.6910.hdf5\n",
      "Epoch 355, train_loss: 0.68928 val_loss 0.69101  train_accuracy 0.5364 val_accuracy 0.5259  train_f1 0.5224 val_f1 0.5270\n",
      "Epoch 356, train_loss: 0.68926 val_loss 0.69102  train_accuracy 0.5363 val_accuracy 0.5259  train_f1 0.5227 val_f1 0.5263\n",
      "Epoch 357, train_loss: 0.68928 val_loss 0.69101  train_accuracy 0.5362 val_accuracy 0.5260  train_f1 0.5220 val_f1 0.5276\n",
      "Epoch 358, train_loss: 0.68924 val_loss 0.69101  train_accuracy 0.5362 val_accuracy 0.5260  train_f1 0.5238 val_f1 0.5276\n",
      "Epoch 359, train_loss: 0.68924 val_loss 0.69101  train_accuracy 0.5361 val_accuracy 0.5262  train_f1 0.5235 val_f1 0.5278\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-360-0.6910.hdf5\n",
      "Epoch 360, train_loss: 0.68925 val_loss 0.69102  train_accuracy 0.5361 val_accuracy 0.5260  train_f1 0.5227 val_f1 0.5280\n",
      "Epoch 361, train_loss: 0.68925 val_loss 0.69102  train_accuracy 0.5361 val_accuracy 0.5262  train_f1 0.5232 val_f1 0.5283\n",
      "Epoch 362, train_loss: 0.68924 val_loss 0.69101  train_accuracy 0.5362 val_accuracy 0.5263  train_f1 0.5236 val_f1 0.5280\n",
      "Epoch 363, train_loss: 0.68923 val_loss 0.69101  train_accuracy 0.5364 val_accuracy 0.5262  train_f1 0.5225 val_f1 0.5267\n",
      "Epoch 364, train_loss: 0.68924 val_loss 0.69100  train_accuracy 0.5363 val_accuracy 0.5265  train_f1 0.5226 val_f1 0.5291\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-365-0.6910.hdf5\n",
      "Epoch 365, train_loss: 0.68923 val_loss 0.69100  train_accuracy 0.5363 val_accuracy 0.5264  train_f1 0.5230 val_f1 0.5282\n",
      "Epoch 366, train_loss: 0.68926 val_loss 0.69099  train_accuracy 0.5363 val_accuracy 0.5263  train_f1 0.5244 val_f1 0.5265\n",
      "Epoch 367, train_loss: 0.68928 val_loss 0.69099  train_accuracy 0.5363 val_accuracy 0.5265  train_f1 0.5217 val_f1 0.5279\n",
      "Epoch 368, train_loss: 0.68922 val_loss 0.69100  train_accuracy 0.5361 val_accuracy 0.5258  train_f1 0.5218 val_f1 0.5273\n",
      "Epoch 369, train_loss: 0.68920 val_loss 0.69100  train_accuracy 0.5362 val_accuracy 0.5254  train_f1 0.5248 val_f1 0.5258\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-370-0.6910.hdf5\n",
      "Epoch 370, train_loss: 0.68917 val_loss 0.69100  train_accuracy 0.5367 val_accuracy 0.5258  train_f1 0.5212 val_f1 0.5268\n",
      "Epoch 371, train_loss: 0.68924 val_loss 0.69100  train_accuracy 0.5363 val_accuracy 0.5257  train_f1 0.5237 val_f1 0.5268\n",
      "Epoch 372, train_loss: 0.68920 val_loss 0.69100  train_accuracy 0.5364 val_accuracy 0.5260  train_f1 0.5227 val_f1 0.5271\n",
      "Epoch 373, train_loss: 0.68922 val_loss 0.69100  train_accuracy 0.5363 val_accuracy 0.5261  train_f1 0.5218 val_f1 0.5272\n",
      "Epoch 374, train_loss: 0.68927 val_loss 0.69100  train_accuracy 0.5362 val_accuracy 0.5261  train_f1 0.5235 val_f1 0.5279\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-375-0.6910.hdf5\n",
      "Epoch 375, train_loss: 0.68915 val_loss 0.69100  train_accuracy 0.5367 val_accuracy 0.5260  train_f1 0.5232 val_f1 0.5267\n",
      "Epoch 376, train_loss: 0.68919 val_loss 0.69100  train_accuracy 0.5364 val_accuracy 0.5258  train_f1 0.5236 val_f1 0.5269\n",
      "Epoch 377, train_loss: 0.68917 val_loss 0.69099  train_accuracy 0.5364 val_accuracy 0.5257  train_f1 0.5228 val_f1 0.5274\n",
      "Epoch 378, train_loss: 0.68919 val_loss 0.69098  train_accuracy 0.5365 val_accuracy 0.5259  train_f1 0.5238 val_f1 0.5277\n",
      "Epoch 379, train_loss: 0.68920 val_loss 0.69100  train_accuracy 0.5365 val_accuracy 0.5260  train_f1 0.5237 val_f1 0.5282\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-380-0.6910.hdf5\n",
      "Epoch 380, train_loss: 0.68916 val_loss 0.69099  train_accuracy 0.5365 val_accuracy 0.5261  train_f1 0.5232 val_f1 0.5285\n",
      "Epoch 381, train_loss: 0.68921 val_loss 0.69100  train_accuracy 0.5364 val_accuracy 0.5263  train_f1 0.5242 val_f1 0.5263\n",
      "Epoch 382, train_loss: 0.68922 val_loss 0.69100  train_accuracy 0.5363 val_accuracy 0.5258  train_f1 0.5220 val_f1 0.5295\n",
      "Epoch 383, train_loss: 0.68915 val_loss 0.69099  train_accuracy 0.5367 val_accuracy 0.5262  train_f1 0.5241 val_f1 0.5285\n",
      "Epoch 384, train_loss: 0.68917 val_loss 0.69099  train_accuracy 0.5366 val_accuracy 0.5263  train_f1 0.5238 val_f1 0.5286\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-385-0.6910.hdf5\n",
      "Epoch 385, train_loss: 0.68916 val_loss 0.69098  train_accuracy 0.5365 val_accuracy 0.5262  train_f1 0.5238 val_f1 0.5277\n",
      "Epoch 386, train_loss: 0.68916 val_loss 0.69099  train_accuracy 0.5364 val_accuracy 0.5260  train_f1 0.5223 val_f1 0.5285\n",
      "Epoch 387, train_loss: 0.68916 val_loss 0.69099  train_accuracy 0.5367 val_accuracy 0.5260  train_f1 0.5238 val_f1 0.5277\n",
      "Epoch 388, train_loss: 0.68914 val_loss 0.69098  train_accuracy 0.5370 val_accuracy 0.5257  train_f1 0.5240 val_f1 0.5271\n",
      "Epoch 389, train_loss: 0.68917 val_loss 0.69099  train_accuracy 0.5363 val_accuracy 0.5257  train_f1 0.5236 val_f1 0.5278\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-390-0.6910.hdf5\n",
      "Epoch 390, train_loss: 0.68915 val_loss 0.69099  train_accuracy 0.5367 val_accuracy 0.5258  train_f1 0.5234 val_f1 0.5280\n",
      "Epoch 391, train_loss: 0.68911 val_loss 0.69098  train_accuracy 0.5368 val_accuracy 0.5259  train_f1 0.5242 val_f1 0.5279\n",
      "Epoch 392, train_loss: 0.68913 val_loss 0.69098  train_accuracy 0.5365 val_accuracy 0.5261  train_f1 0.5240 val_f1 0.5284\n",
      "Epoch 393, train_loss: 0.68917 val_loss 0.69097  train_accuracy 0.5367 val_accuracy 0.5258  train_f1 0.5240 val_f1 0.5275\n",
      "Epoch 394, train_loss: 0.68917 val_loss 0.69098  train_accuracy 0.5366 val_accuracy 0.5261  train_f1 0.5234 val_f1 0.5290\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-395-0.6910.hdf5\n",
      "Epoch 395, train_loss: 0.68909 val_loss 0.69098  train_accuracy 0.5367 val_accuracy 0.5257  train_f1 0.5233 val_f1 0.5269\n",
      "Epoch 396, train_loss: 0.68912 val_loss 0.69098  train_accuracy 0.5366 val_accuracy 0.5260  train_f1 0.5237 val_f1 0.5292\n",
      "Epoch 397, train_loss: 0.68914 val_loss 0.69098  train_accuracy 0.5367 val_accuracy 0.5263  train_f1 0.5242 val_f1 0.5285\n",
      "Epoch 398, train_loss: 0.68911 val_loss 0.69099  train_accuracy 0.5367 val_accuracy 0.5263  train_f1 0.5236 val_f1 0.5295\n",
      "Epoch 399, train_loss: 0.68913 val_loss 0.69098  train_accuracy 0.5370 val_accuracy 0.5261  train_f1 0.5246 val_f1 0.5292\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-400-0.6910.hdf5\n",
      "Epoch 400, train_loss: 0.68910 val_loss 0.69097  train_accuracy 0.5366 val_accuracy 0.5262  train_f1 0.5234 val_f1 0.5280\n",
      "Epoch 401, train_loss: 0.68908 val_loss 0.69098  train_accuracy 0.5369 val_accuracy 0.5263  train_f1 0.5241 val_f1 0.5286\n",
      "Epoch 402, train_loss: 0.68910 val_loss 0.69098  train_accuracy 0.5369 val_accuracy 0.5264  train_f1 0.5252 val_f1 0.5291\n",
      "Epoch 403, train_loss: 0.68906 val_loss 0.69098  train_accuracy 0.5371 val_accuracy 0.5264  train_f1 0.5238 val_f1 0.5294\n",
      "Epoch 404, train_loss: 0.68913 val_loss 0.69098  train_accuracy 0.5367 val_accuracy 0.5263  train_f1 0.5228 val_f1 0.5291\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-405-0.6910.hdf5\n",
      "Epoch 405, train_loss: 0.68911 val_loss 0.69097  train_accuracy 0.5369 val_accuracy 0.5264  train_f1 0.5241 val_f1 0.5296\n",
      "Epoch 406, train_loss: 0.68909 val_loss 0.69097  train_accuracy 0.5370 val_accuracy 0.5264  train_f1 0.5243 val_f1 0.5299\n",
      "Epoch 407, train_loss: 0.68909 val_loss 0.69098  train_accuracy 0.5367 val_accuracy 0.5262  train_f1 0.5234 val_f1 0.5291\n",
      "Epoch 408, train_loss: 0.68910 val_loss 0.69098  train_accuracy 0.5366 val_accuracy 0.5262  train_f1 0.5257 val_f1 0.5290\n",
      "Epoch 409, train_loss: 0.68910 val_loss 0.69097  train_accuracy 0.5367 val_accuracy 0.5267  train_f1 0.5224 val_f1 0.5300\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-410-0.6910.hdf5\n",
      "Epoch 410, train_loss: 0.68911 val_loss 0.69097  train_accuracy 0.5367 val_accuracy 0.5265  train_f1 0.5248 val_f1 0.5297\n",
      "Epoch 411, train_loss: 0.68910 val_loss 0.69097  train_accuracy 0.5369 val_accuracy 0.5265  train_f1 0.5226 val_f1 0.5309\n",
      "Epoch 412, train_loss: 0.68910 val_loss 0.69098  train_accuracy 0.5369 val_accuracy 0.5265  train_f1 0.5269 val_f1 0.5306\n",
      "Epoch 413, train_loss: 0.68908 val_loss 0.69097  train_accuracy 0.5371 val_accuracy 0.5266  train_f1 0.5224 val_f1 0.5303\n",
      "Epoch 414, train_loss: 0.68910 val_loss 0.69096  train_accuracy 0.5366 val_accuracy 0.5261  train_f1 0.5259 val_f1 0.5309\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-415-0.6910.hdf5\n",
      "Epoch 415, train_loss: 0.68905 val_loss 0.69096  train_accuracy 0.5372 val_accuracy 0.5264  train_f1 0.5239 val_f1 0.5289\n",
      "Epoch 416, train_loss: 0.68903 val_loss 0.69097  train_accuracy 0.5376 val_accuracy 0.5260  train_f1 0.5248 val_f1 0.5303\n",
      "Epoch 417, train_loss: 0.68907 val_loss 0.69097  train_accuracy 0.5370 val_accuracy 0.5262  train_f1 0.5255 val_f1 0.5297\n",
      "Epoch 418, train_loss: 0.68908 val_loss 0.69096  train_accuracy 0.5370 val_accuracy 0.5263  train_f1 0.5241 val_f1 0.5291\n",
      "Epoch 419, train_loss: 0.68902 val_loss 0.69096  train_accuracy 0.5369 val_accuracy 0.5263  train_f1 0.5244 val_f1 0.5290\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-420-0.6910.hdf5\n",
      "Epoch 420, train_loss: 0.68904 val_loss 0.69097  train_accuracy 0.5369 val_accuracy 0.5260  train_f1 0.5244 val_f1 0.5306\n",
      "Epoch 421, train_loss: 0.68900 val_loss 0.69096  train_accuracy 0.5371 val_accuracy 0.5265  train_f1 0.5255 val_f1 0.5314\n",
      "Epoch 422, train_loss: 0.68905 val_loss 0.69096  train_accuracy 0.5370 val_accuracy 0.5262  train_f1 0.5250 val_f1 0.5300\n",
      "Epoch 423, train_loss: 0.68901 val_loss 0.69096  train_accuracy 0.5374 val_accuracy 0.5264  train_f1 0.5247 val_f1 0.5321\n",
      "Epoch 424, train_loss: 0.68905 val_loss 0.69096  train_accuracy 0.5369 val_accuracy 0.5263  train_f1 0.5250 val_f1 0.5304\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-425-0.6910.hdf5\n",
      "Epoch 425, train_loss: 0.68909 val_loss 0.69096  train_accuracy 0.5368 val_accuracy 0.5262  train_f1 0.5242 val_f1 0.5302\n",
      "Epoch 426, train_loss: 0.68897 val_loss 0.69096  train_accuracy 0.5371 val_accuracy 0.5262  train_f1 0.5258 val_f1 0.5310\n",
      "Epoch 427, train_loss: 0.68902 val_loss 0.69096  train_accuracy 0.5371 val_accuracy 0.5261  train_f1 0.5247 val_f1 0.5300\n",
      "Epoch 428, train_loss: 0.68905 val_loss 0.69097  train_accuracy 0.5370 val_accuracy 0.5261  train_f1 0.5249 val_f1 0.5302\n",
      "Epoch 429, train_loss: 0.68900 val_loss 0.69097  train_accuracy 0.5374 val_accuracy 0.5259  train_f1 0.5244 val_f1 0.5296\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-430-0.6910.hdf5\n",
      "Epoch 430, train_loss: 0.68899 val_loss 0.69095  train_accuracy 0.5371 val_accuracy 0.5261  train_f1 0.5262 val_f1 0.5303\n",
      "Epoch 431, train_loss: 0.68903 val_loss 0.69096  train_accuracy 0.5369 val_accuracy 0.5263  train_f1 0.5248 val_f1 0.5294\n",
      "Epoch 432, train_loss: 0.68898 val_loss 0.69096  train_accuracy 0.5373 val_accuracy 0.5264  train_f1 0.5245 val_f1 0.5310\n",
      "Epoch 433, train_loss: 0.68901 val_loss 0.69095  train_accuracy 0.5374 val_accuracy 0.5261  train_f1 0.5257 val_f1 0.5303\n",
      "Epoch 434, train_loss: 0.68901 val_loss 0.69096  train_accuracy 0.5374 val_accuracy 0.5265  train_f1 0.5244 val_f1 0.5299\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-435-0.6910.hdf5\n",
      "Epoch 435, train_loss: 0.68902 val_loss 0.69096  train_accuracy 0.5370 val_accuracy 0.5263  train_f1 0.5256 val_f1 0.5309\n",
      "Epoch 436, train_loss: 0.68902 val_loss 0.69094  train_accuracy 0.5369 val_accuracy 0.5264  train_f1 0.5243 val_f1 0.5295\n",
      "Epoch 437, train_loss: 0.68899 val_loss 0.69094  train_accuracy 0.5371 val_accuracy 0.5265  train_f1 0.5252 val_f1 0.5306\n",
      "Epoch 438, train_loss: 0.68900 val_loss 0.69095  train_accuracy 0.5378 val_accuracy 0.5260  train_f1 0.5261 val_f1 0.5293\n",
      "Epoch 439, train_loss: 0.68897 val_loss 0.69094  train_accuracy 0.5374 val_accuracy 0.5260  train_f1 0.5251 val_f1 0.5290\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-440-0.6909.hdf5\n",
      "Epoch 440, train_loss: 0.68903 val_loss 0.69095  train_accuracy 0.5369 val_accuracy 0.5262  train_f1 0.5235 val_f1 0.5302\n",
      "Epoch 441, train_loss: 0.68902 val_loss 0.69094  train_accuracy 0.5369 val_accuracy 0.5261  train_f1 0.5259 val_f1 0.5296\n",
      "Epoch 442, train_loss: 0.68898 val_loss 0.69095  train_accuracy 0.5372 val_accuracy 0.5259  train_f1 0.5229 val_f1 0.5282\n",
      "Epoch 443, train_loss: 0.68898 val_loss 0.69096  train_accuracy 0.5373 val_accuracy 0.5255  train_f1 0.5273 val_f1 0.5294\n",
      "Epoch 444, train_loss: 0.68895 val_loss 0.69095  train_accuracy 0.5373 val_accuracy 0.5256  train_f1 0.5234 val_f1 0.5286\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-445-0.6910.hdf5\n",
      "Epoch 445, train_loss: 0.68900 val_loss 0.69095  train_accuracy 0.5372 val_accuracy 0.5256  train_f1 0.5268 val_f1 0.5302\n",
      "Epoch 446, train_loss: 0.68899 val_loss 0.69094  train_accuracy 0.5373 val_accuracy 0.5258  train_f1 0.5254 val_f1 0.5297\n",
      "Epoch 447, train_loss: 0.68896 val_loss 0.69094  train_accuracy 0.5373 val_accuracy 0.5258  train_f1 0.5245 val_f1 0.5296\n",
      "Epoch 448, train_loss: 0.68901 val_loss 0.69095  train_accuracy 0.5373 val_accuracy 0.5260  train_f1 0.5255 val_f1 0.5300\n",
      "Epoch 449, train_loss: 0.68896 val_loss 0.69094  train_accuracy 0.5373 val_accuracy 0.5265  train_f1 0.5251 val_f1 0.5317\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-450-0.6909.hdf5\n",
      "Epoch 450, train_loss: 0.68892 val_loss 0.69094  train_accuracy 0.5375 val_accuracy 0.5265  train_f1 0.5261 val_f1 0.5311\n",
      "Epoch 451, train_loss: 0.68897 val_loss 0.69094  train_accuracy 0.5372 val_accuracy 0.5267  train_f1 0.5264 val_f1 0.5331\n",
      "Epoch 452, train_loss: 0.68896 val_loss 0.69094  train_accuracy 0.5374 val_accuracy 0.5259  train_f1 0.5250 val_f1 0.5301\n",
      "Epoch 453, train_loss: 0.68894 val_loss 0.69093  train_accuracy 0.5377 val_accuracy 0.5264  train_f1 0.5263 val_f1 0.5317\n",
      "Epoch 454, train_loss: 0.68890 val_loss 0.69093  train_accuracy 0.5378 val_accuracy 0.5260  train_f1 0.5270 val_f1 0.5303\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-455-0.6909.hdf5\n",
      "Epoch 455, train_loss: 0.68885 val_loss 0.69093  train_accuracy 0.5377 val_accuracy 0.5262  train_f1 0.5259 val_f1 0.5303\n",
      "Epoch 456, train_loss: 0.68893 val_loss 0.69094  train_accuracy 0.5375 val_accuracy 0.5262  train_f1 0.5253 val_f1 0.5306\n",
      "Epoch 457, train_loss: 0.68895 val_loss 0.69093  train_accuracy 0.5373 val_accuracy 0.5266  train_f1 0.5265 val_f1 0.5312\n",
      "Epoch 458, train_loss: 0.68896 val_loss 0.69093  train_accuracy 0.5373 val_accuracy 0.5266  train_f1 0.5251 val_f1 0.5321\n",
      "Epoch 459, train_loss: 0.68896 val_loss 0.69092  train_accuracy 0.5374 val_accuracy 0.5264  train_f1 0.5267 val_f1 0.5309\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-460-0.6909.hdf5\n",
      "Epoch 460, train_loss: 0.68893 val_loss 0.69094  train_accuracy 0.5376 val_accuracy 0.5262  train_f1 0.5266 val_f1 0.5313\n",
      "Epoch 461, train_loss: 0.68890 val_loss 0.69093  train_accuracy 0.5377 val_accuracy 0.5262  train_f1 0.5251 val_f1 0.5288\n",
      "Epoch 462, train_loss: 0.68894 val_loss 0.69095  train_accuracy 0.5377 val_accuracy 0.5262  train_f1 0.5248 val_f1 0.5320\n",
      "Epoch 463, train_loss: 0.68890 val_loss 0.69093  train_accuracy 0.5373 val_accuracy 0.5265  train_f1 0.5259 val_f1 0.5303\n",
      "Epoch 464, train_loss: 0.68889 val_loss 0.69093  train_accuracy 0.5374 val_accuracy 0.5261  train_f1 0.5253 val_f1 0.5300\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-465-0.6909.hdf5\n",
      "Epoch 465, train_loss: 0.68892 val_loss 0.69093  train_accuracy 0.5374 val_accuracy 0.5260  train_f1 0.5242 val_f1 0.5300\n",
      "Epoch 466, train_loss: 0.68889 val_loss 0.69092  train_accuracy 0.5376 val_accuracy 0.5263  train_f1 0.5270 val_f1 0.5304\n",
      "Epoch 467, train_loss: 0.68894 val_loss 0.69093  train_accuracy 0.5376 val_accuracy 0.5264  train_f1 0.5246 val_f1 0.5304\n",
      "Epoch 468, train_loss: 0.68892 val_loss 0.69093  train_accuracy 0.5372 val_accuracy 0.5262  train_f1 0.5252 val_f1 0.5304\n",
      "Epoch 469, train_loss: 0.68891 val_loss 0.69093  train_accuracy 0.5375 val_accuracy 0.5265  train_f1 0.5266 val_f1 0.5306\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-470-0.6909.hdf5\n",
      "Epoch 470, train_loss: 0.68892 val_loss 0.69093  train_accuracy 0.5375 val_accuracy 0.5262  train_f1 0.5243 val_f1 0.5297\n",
      "Epoch 471, train_loss: 0.68886 val_loss 0.69093  train_accuracy 0.5377 val_accuracy 0.5262  train_f1 0.5265 val_f1 0.5298\n",
      "Epoch 472, train_loss: 0.68889 val_loss 0.69092  train_accuracy 0.5373 val_accuracy 0.5267  train_f1 0.5246 val_f1 0.5302\n",
      "Epoch 473, train_loss: 0.68885 val_loss 0.69093  train_accuracy 0.5377 val_accuracy 0.5264  train_f1 0.5258 val_f1 0.5302\n",
      "Epoch 474, train_loss: 0.68888 val_loss 0.69093  train_accuracy 0.5375 val_accuracy 0.5264  train_f1 0.5254 val_f1 0.5305\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-475-0.6909.hdf5\n",
      "Epoch 475, train_loss: 0.68885 val_loss 0.69093  train_accuracy 0.5378 val_accuracy 0.5263  train_f1 0.5261 val_f1 0.5310\n",
      "Epoch 476, train_loss: 0.68892 val_loss 0.69093  train_accuracy 0.5375 val_accuracy 0.5263  train_f1 0.5254 val_f1 0.5309\n",
      "Epoch 477, train_loss: 0.68886 val_loss 0.69092  train_accuracy 0.5376 val_accuracy 0.5267  train_f1 0.5258 val_f1 0.5315\n",
      "Epoch 478, train_loss: 0.68884 val_loss 0.69092  train_accuracy 0.5378 val_accuracy 0.5270  train_f1 0.5264 val_f1 0.5310\n",
      "Epoch 479, train_loss: 0.68887 val_loss 0.69093  train_accuracy 0.5376 val_accuracy 0.5266  train_f1 0.5257 val_f1 0.5308\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-480-0.6909.hdf5\n",
      "Epoch 480, train_loss: 0.68887 val_loss 0.69092  train_accuracy 0.5377 val_accuracy 0.5268  train_f1 0.5256 val_f1 0.5318\n",
      "Epoch 481, train_loss: 0.68883 val_loss 0.69092  train_accuracy 0.5378 val_accuracy 0.5267  train_f1 0.5261 val_f1 0.5317\n",
      "Epoch 482, train_loss: 0.68885 val_loss 0.69093  train_accuracy 0.5378 val_accuracy 0.5264  train_f1 0.5265 val_f1 0.5310\n",
      "Epoch 483, train_loss: 0.68885 val_loss 0.69092  train_accuracy 0.5378 val_accuracy 0.5263  train_f1 0.5246 val_f1 0.5299\n",
      "Epoch 484, train_loss: 0.68888 val_loss 0.69092  train_accuracy 0.5378 val_accuracy 0.5266  train_f1 0.5259 val_f1 0.5318\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-485-0.6909.hdf5\n",
      "Epoch 485, train_loss: 0.68884 val_loss 0.69092  train_accuracy 0.5379 val_accuracy 0.5265  train_f1 0.5256 val_f1 0.5320\n",
      "Epoch 486, train_loss: 0.68884 val_loss 0.69092  train_accuracy 0.5377 val_accuracy 0.5266  train_f1 0.5261 val_f1 0.5313\n",
      "Epoch 487, train_loss: 0.68888 val_loss 0.69092  train_accuracy 0.5374 val_accuracy 0.5264  train_f1 0.5268 val_f1 0.5308\n",
      "Epoch 488, train_loss: 0.68883 val_loss 0.69092  train_accuracy 0.5379 val_accuracy 0.5261  train_f1 0.5259 val_f1 0.5320\n",
      "Epoch 489, train_loss: 0.68882 val_loss 0.69092  train_accuracy 0.5380 val_accuracy 0.5262  train_f1 0.5272 val_f1 0.5316\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-490-0.6909.hdf5\n",
      "Epoch 490, train_loss: 0.68880 val_loss 0.69093  train_accuracy 0.5380 val_accuracy 0.5261  train_f1 0.5268 val_f1 0.5321\n",
      "Epoch 491, train_loss: 0.68885 val_loss 0.69093  train_accuracy 0.5376 val_accuracy 0.5259  train_f1 0.5254 val_f1 0.5311\n",
      "Epoch 492, train_loss: 0.68886 val_loss 0.69092  train_accuracy 0.5376 val_accuracy 0.5262  train_f1 0.5269 val_f1 0.5320\n",
      "Epoch 493, train_loss: 0.68880 val_loss 0.69092  train_accuracy 0.5381 val_accuracy 0.5263  train_f1 0.5270 val_f1 0.5316\n",
      "Epoch 494, train_loss: 0.68883 val_loss 0.69093  train_accuracy 0.5377 val_accuracy 0.5262  train_f1 0.5248 val_f1 0.5326\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-495-0.6909.hdf5\n",
      "Epoch 495, train_loss: 0.68882 val_loss 0.69093  train_accuracy 0.5380 val_accuracy 0.5265  train_f1 0.5271 val_f1 0.5331\n",
      "Epoch 496, train_loss: 0.68882 val_loss 0.69092  train_accuracy 0.5379 val_accuracy 0.5264  train_f1 0.5262 val_f1 0.5328\n",
      "Epoch 497, train_loss: 0.68880 val_loss 0.69091  train_accuracy 0.5378 val_accuracy 0.5263  train_f1 0.5264 val_f1 0.5332\n",
      "Epoch 498, train_loss: 0.68879 val_loss 0.69092  train_accuracy 0.5381 val_accuracy 0.5268  train_f1 0.5269 val_f1 0.5326\n",
      "Epoch 499, train_loss: 0.68880 val_loss 0.69092  train_accuracy 0.5377 val_accuracy 0.5267  train_f1 0.5252 val_f1 0.5318\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-500-0.6909.hdf5\n",
      "Epoch 500, train_loss: 0.68877 val_loss 0.69093  train_accuracy 0.5380 val_accuracy 0.5263  train_f1 0.5265 val_f1 0.5323\n",
      "Epoch 501, train_loss: 0.68873 val_loss 0.69092  train_accuracy 0.5382 val_accuracy 0.5266  train_f1 0.5258 val_f1 0.5324\n",
      "Epoch 502, train_loss: 0.68879 val_loss 0.69092  train_accuracy 0.5377 val_accuracy 0.5266  train_f1 0.5274 val_f1 0.5335\n",
      "Epoch 503, train_loss: 0.68879 val_loss 0.69093  train_accuracy 0.5378 val_accuracy 0.5265  train_f1 0.5250 val_f1 0.5324\n",
      "Epoch 504, train_loss: 0.68877 val_loss 0.69093  train_accuracy 0.5382 val_accuracy 0.5267  train_f1 0.5275 val_f1 0.5336\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-505-0.6909.hdf5\n",
      "Epoch 505, train_loss: 0.68878 val_loss 0.69092  train_accuracy 0.5378 val_accuracy 0.5266  train_f1 0.5256 val_f1 0.5326\n",
      "Epoch 506, train_loss: 0.68878 val_loss 0.69092  train_accuracy 0.5381 val_accuracy 0.5266  train_f1 0.5285 val_f1 0.5348\n",
      "Epoch 507, train_loss: 0.68877 val_loss 0.69091  train_accuracy 0.5382 val_accuracy 0.5269  train_f1 0.5267 val_f1 0.5331\n",
      "Epoch 508, train_loss: 0.68877 val_loss 0.69091  train_accuracy 0.5379 val_accuracy 0.5267  train_f1 0.5271 val_f1 0.5333\n",
      "Epoch 509, train_loss: 0.68874 val_loss 0.69091  train_accuracy 0.5383 val_accuracy 0.5265  train_f1 0.5266 val_f1 0.5329\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-510-0.6909.hdf5\n",
      "Epoch 510, train_loss: 0.68881 val_loss 0.69092  train_accuracy 0.5377 val_accuracy 0.5267  train_f1 0.5260 val_f1 0.5341\n",
      "Epoch 511, train_loss: 0.68873 val_loss 0.69091  train_accuracy 0.5381 val_accuracy 0.5266  train_f1 0.5277 val_f1 0.5333\n",
      "Epoch 512, train_loss: 0.68876 val_loss 0.69090  train_accuracy 0.5384 val_accuracy 0.5267  train_f1 0.5276 val_f1 0.5331\n",
      "Epoch 513, train_loss: 0.68875 val_loss 0.69090  train_accuracy 0.5382 val_accuracy 0.5265  train_f1 0.5274 val_f1 0.5330\n",
      "Epoch 514, train_loss: 0.68874 val_loss 0.69090  train_accuracy 0.5379 val_accuracy 0.5267  train_f1 0.5268 val_f1 0.5329\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-515-0.6909.hdf5\n",
      "Epoch 515, train_loss: 0.68875 val_loss 0.69091  train_accuracy 0.5382 val_accuracy 0.5266  train_f1 0.5271 val_f1 0.5326\n",
      "Epoch 516, train_loss: 0.68877 val_loss 0.69090  train_accuracy 0.5380 val_accuracy 0.5269  train_f1 0.5261 val_f1 0.5343\n",
      "Epoch 517, train_loss: 0.68874 val_loss 0.69090  train_accuracy 0.5383 val_accuracy 0.5266  train_f1 0.5271 val_f1 0.5328\n",
      "Epoch 518, train_loss: 0.68868 val_loss 0.69090  train_accuracy 0.5382 val_accuracy 0.5270  train_f1 0.5276 val_f1 0.5340\n",
      "Epoch 519, train_loss: 0.68874 val_loss 0.69090  train_accuracy 0.5382 val_accuracy 0.5271  train_f1 0.5273 val_f1 0.5330\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-520-0.6909.hdf5\n",
      "Epoch 520, train_loss: 0.68875 val_loss 0.69091  train_accuracy 0.5379 val_accuracy 0.5271  train_f1 0.5262 val_f1 0.5340\n",
      "Epoch 521, train_loss: 0.68871 val_loss 0.69092  train_accuracy 0.5382 val_accuracy 0.5268  train_f1 0.5272 val_f1 0.5318\n",
      "Epoch 522, train_loss: 0.68869 val_loss 0.69090  train_accuracy 0.5386 val_accuracy 0.5273  train_f1 0.5272 val_f1 0.5334\n",
      "Epoch 523, train_loss: 0.68872 val_loss 0.69091  train_accuracy 0.5383 val_accuracy 0.5271  train_f1 0.5274 val_f1 0.5338\n",
      "Epoch 524, train_loss: 0.68875 val_loss 0.69092  train_accuracy 0.5380 val_accuracy 0.5270  train_f1 0.5253 val_f1 0.5328\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-525-0.6909.hdf5\n",
      "Epoch 525, train_loss: 0.68873 val_loss 0.69090  train_accuracy 0.5381 val_accuracy 0.5272  train_f1 0.5265 val_f1 0.5328\n",
      "Epoch 526, train_loss: 0.68874 val_loss 0.69090  train_accuracy 0.5383 val_accuracy 0.5274  train_f1 0.5262 val_f1 0.5337\n",
      "Epoch 527, train_loss: 0.68873 val_loss 0.69090  train_accuracy 0.5382 val_accuracy 0.5271  train_f1 0.5265 val_f1 0.5324\n",
      "Epoch 528, train_loss: 0.68874 val_loss 0.69090  train_accuracy 0.5381 val_accuracy 0.5270  train_f1 0.5265 val_f1 0.5336\n",
      "Epoch 529, train_loss: 0.68876 val_loss 0.69091  train_accuracy 0.5383 val_accuracy 0.5269  train_f1 0.5281 val_f1 0.5335\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-530-0.6909.hdf5\n",
      "Epoch 530, train_loss: 0.68866 val_loss 0.69090  train_accuracy 0.5385 val_accuracy 0.5266  train_f1 0.5260 val_f1 0.5329\n",
      "Epoch 531, train_loss: 0.68869 val_loss 0.69091  train_accuracy 0.5383 val_accuracy 0.5266  train_f1 0.5286 val_f1 0.5335\n",
      "Epoch 532, train_loss: 0.68868 val_loss 0.69090  train_accuracy 0.5382 val_accuracy 0.5268  train_f1 0.5263 val_f1 0.5325\n",
      "Epoch 533, train_loss: 0.68866 val_loss 0.69091  train_accuracy 0.5386 val_accuracy 0.5268  train_f1 0.5277 val_f1 0.5338\n",
      "Epoch 534, train_loss: 0.68865 val_loss 0.69092  train_accuracy 0.5384 val_accuracy 0.5267  train_f1 0.5279 val_f1 0.5336\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-535-0.6909.hdf5\n",
      "Epoch 535, train_loss: 0.68871 val_loss 0.69090  train_accuracy 0.5382 val_accuracy 0.5269  train_f1 0.5262 val_f1 0.5341\n",
      "Epoch 536, train_loss: 0.68869 val_loss 0.69091  train_accuracy 0.5383 val_accuracy 0.5269  train_f1 0.5277 val_f1 0.5344\n",
      "Epoch 537, train_loss: 0.68866 val_loss 0.69091  train_accuracy 0.5381 val_accuracy 0.5269  train_f1 0.5273 val_f1 0.5342\n",
      "Epoch 538, train_loss: 0.68873 val_loss 0.69091  train_accuracy 0.5382 val_accuracy 0.5270  train_f1 0.5282 val_f1 0.5341\n",
      "Epoch 539, train_loss: 0.68865 val_loss 0.69090  train_accuracy 0.5384 val_accuracy 0.5271  train_f1 0.5256 val_f1 0.5337\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-540-0.6909.hdf5\n",
      "Epoch 540, train_loss: 0.68869 val_loss 0.69090  train_accuracy 0.5382 val_accuracy 0.5268  train_f1 0.5288 val_f1 0.5335\n",
      "Epoch 541, train_loss: 0.68871 val_loss 0.69091  train_accuracy 0.5384 val_accuracy 0.5271  train_f1 0.5268 val_f1 0.5346\n",
      "Epoch 542, train_loss: 0.68872 val_loss 0.69091  train_accuracy 0.5382 val_accuracy 0.5269  train_f1 0.5269 val_f1 0.5334\n",
      "Epoch 543, train_loss: 0.68861 val_loss 0.69089  train_accuracy 0.5386 val_accuracy 0.5270  train_f1 0.5282 val_f1 0.5339\n",
      "Epoch 544, train_loss: 0.68869 val_loss 0.69089  train_accuracy 0.5386 val_accuracy 0.5274  train_f1 0.5280 val_f1 0.5340\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-545-0.6909.hdf5\n",
      "Epoch 545, train_loss: 0.68868 val_loss 0.69090  train_accuracy 0.5382 val_accuracy 0.5273  train_f1 0.5287 val_f1 0.5345\n",
      "Epoch 546, train_loss: 0.68864 val_loss 0.69091  train_accuracy 0.5385 val_accuracy 0.5267  train_f1 0.5278 val_f1 0.5327\n",
      "Epoch 547, train_loss: 0.68864 val_loss 0.69090  train_accuracy 0.5385 val_accuracy 0.5267  train_f1 0.5272 val_f1 0.5334\n",
      "Epoch 548, train_loss: 0.68871 val_loss 0.69091  train_accuracy 0.5384 val_accuracy 0.5266  train_f1 0.5273 val_f1 0.5335\n",
      "Epoch 549, train_loss: 0.68862 val_loss 0.69091  train_accuracy 0.5386 val_accuracy 0.5267  train_f1 0.5276 val_f1 0.5339\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-550-0.6909.hdf5\n",
      "Epoch 550, train_loss: 0.68867 val_loss 0.69090  train_accuracy 0.5384 val_accuracy 0.5268  train_f1 0.5285 val_f1 0.5334\n",
      "Epoch 551, train_loss: 0.68867 val_loss 0.69091  train_accuracy 0.5381 val_accuracy 0.5267  train_f1 0.5275 val_f1 0.5340\n",
      "Epoch 552, train_loss: 0.68865 val_loss 0.69090  train_accuracy 0.5383 val_accuracy 0.5272  train_f1 0.5271 val_f1 0.5342\n",
      "Epoch 553, train_loss: 0.68858 val_loss 0.69090  train_accuracy 0.5390 val_accuracy 0.5272  train_f1 0.5280 val_f1 0.5348\n",
      "Epoch 554, train_loss: 0.68866 val_loss 0.69090  train_accuracy 0.5385 val_accuracy 0.5272  train_f1 0.5278 val_f1 0.5346\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-555-0.6909.hdf5\n",
      "Epoch 555, train_loss: 0.68859 val_loss 0.69091  train_accuracy 0.5386 val_accuracy 0.5270  train_f1 0.5287 val_f1 0.5343\n",
      "Epoch 556, train_loss: 0.68859 val_loss 0.69090  train_accuracy 0.5388 val_accuracy 0.5269  train_f1 0.5275 val_f1 0.5350\n",
      "Epoch 557, train_loss: 0.68861 val_loss 0.69090  train_accuracy 0.5383 val_accuracy 0.5270  train_f1 0.5274 val_f1 0.5340\n",
      "Epoch 558, train_loss: 0.68861 val_loss 0.69091  train_accuracy 0.5385 val_accuracy 0.5265  train_f1 0.5276 val_f1 0.5343\n",
      "Epoch 559, train_loss: 0.68861 val_loss 0.69090  train_accuracy 0.5384 val_accuracy 0.5266  train_f1 0.5280 val_f1 0.5345\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-560-0.6909.hdf5\n",
      "Epoch 560, train_loss: 0.68863 val_loss 0.69090  train_accuracy 0.5390 val_accuracy 0.5266  train_f1 0.5280 val_f1 0.5348\n",
      "Epoch 561, train_loss: 0.68860 val_loss 0.69090  train_accuracy 0.5386 val_accuracy 0.5267  train_f1 0.5288 val_f1 0.5359\n",
      "Epoch 562, train_loss: 0.68862 val_loss 0.69090  train_accuracy 0.5383 val_accuracy 0.5269  train_f1 0.5287 val_f1 0.5348\n",
      "Epoch 563, train_loss: 0.68858 val_loss 0.69090  train_accuracy 0.5385 val_accuracy 0.5266  train_f1 0.5277 val_f1 0.5341\n",
      "Epoch 564, train_loss: 0.68858 val_loss 0.69089  train_accuracy 0.5390 val_accuracy 0.5268  train_f1 0.5283 val_f1 0.5350\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-565-0.6909.hdf5\n",
      "Epoch 565, train_loss: 0.68857 val_loss 0.69088  train_accuracy 0.5387 val_accuracy 0.5268  train_f1 0.5276 val_f1 0.5340\n",
      "Epoch 566, train_loss: 0.68857 val_loss 0.69089  train_accuracy 0.5386 val_accuracy 0.5266  train_f1 0.5283 val_f1 0.5341\n",
      "Epoch 567, train_loss: 0.68862 val_loss 0.69088  train_accuracy 0.5384 val_accuracy 0.5267  train_f1 0.5279 val_f1 0.5341\n",
      "Epoch 568, train_loss: 0.68866 val_loss 0.69089  train_accuracy 0.5383 val_accuracy 0.5266  train_f1 0.5281 val_f1 0.5348\n",
      "Epoch 569, train_loss: 0.68857 val_loss 0.69086  train_accuracy 0.5386 val_accuracy 0.5272  train_f1 0.5280 val_f1 0.5342\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-570-0.6909.hdf5\n",
      "Epoch 570, train_loss: 0.68855 val_loss 0.69088  train_accuracy 0.5384 val_accuracy 0.5268  train_f1 0.5279 val_f1 0.5344\n",
      "Epoch 571, train_loss: 0.68858 val_loss 0.69088  train_accuracy 0.5389 val_accuracy 0.5265  train_f1 0.5291 val_f1 0.5331\n",
      "Epoch 572, train_loss: 0.68859 val_loss 0.69089  train_accuracy 0.5386 val_accuracy 0.5265  train_f1 0.5278 val_f1 0.5342\n",
      "Epoch 573, train_loss: 0.68856 val_loss 0.69089  train_accuracy 0.5388 val_accuracy 0.5264  train_f1 0.5289 val_f1 0.5338\n",
      "Epoch 574, train_loss: 0.68860 val_loss 0.69089  train_accuracy 0.5387 val_accuracy 0.5265  train_f1 0.5266 val_f1 0.5335\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-575-0.6909.hdf5\n",
      "Epoch 575, train_loss: 0.68859 val_loss 0.69088  train_accuracy 0.5387 val_accuracy 0.5266  train_f1 0.5293 val_f1 0.5352\n",
      "Epoch 576, train_loss: 0.68855 val_loss 0.69088  train_accuracy 0.5386 val_accuracy 0.5270  train_f1 0.5295 val_f1 0.5352\n",
      "Epoch 577, train_loss: 0.68859 val_loss 0.69088  train_accuracy 0.5383 val_accuracy 0.5267  train_f1 0.5279 val_f1 0.5338\n",
      "Epoch 578, train_loss: 0.68858 val_loss 0.69089  train_accuracy 0.5388 val_accuracy 0.5269  train_f1 0.5300 val_f1 0.5355\n",
      "Epoch 579, train_loss: 0.68852 val_loss 0.69088  train_accuracy 0.5387 val_accuracy 0.5271  train_f1 0.5277 val_f1 0.5347\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-580-0.6909.hdf5\n",
      "Epoch 580, train_loss: 0.68857 val_loss 0.69090  train_accuracy 0.5386 val_accuracy 0.5266  train_f1 0.5298 val_f1 0.5346\n",
      "Epoch 581, train_loss: 0.68856 val_loss 0.69089  train_accuracy 0.5386 val_accuracy 0.5270  train_f1 0.5263 val_f1 0.5338\n",
      "Epoch 582, train_loss: 0.68853 val_loss 0.69089  train_accuracy 0.5385 val_accuracy 0.5267  train_f1 0.5284 val_f1 0.5347\n",
      "Epoch 583, train_loss: 0.68853 val_loss 0.69089  train_accuracy 0.5387 val_accuracy 0.5268  train_f1 0.5296 val_f1 0.5353\n",
      "Epoch 584, train_loss: 0.68852 val_loss 0.69090  train_accuracy 0.5389 val_accuracy 0.5265  train_f1 0.5273 val_f1 0.5342\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-585-0.6909.hdf5\n",
      "Epoch 585, train_loss: 0.68855 val_loss 0.69089  train_accuracy 0.5389 val_accuracy 0.5265  train_f1 0.5283 val_f1 0.5344\n",
      "Epoch 586, train_loss: 0.68858 val_loss 0.69089  train_accuracy 0.5388 val_accuracy 0.5264  train_f1 0.5286 val_f1 0.5339\n",
      "Epoch 587, train_loss: 0.68858 val_loss 0.69089  train_accuracy 0.5390 val_accuracy 0.5267  train_f1 0.5278 val_f1 0.5345\n",
      "Epoch 588, train_loss: 0.68851 val_loss 0.69089  train_accuracy 0.5389 val_accuracy 0.5267  train_f1 0.5290 val_f1 0.5354\n",
      "Epoch 589, train_loss: 0.68856 val_loss 0.69088  train_accuracy 0.5389 val_accuracy 0.5267  train_f1 0.5279 val_f1 0.5346\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-590-0.6909.hdf5\n",
      "Epoch 590, train_loss: 0.68853 val_loss 0.69088  train_accuracy 0.5391 val_accuracy 0.5267  train_f1 0.5289 val_f1 0.5351\n",
      "Epoch 591, train_loss: 0.68857 val_loss 0.69089  train_accuracy 0.5391 val_accuracy 0.5267  train_f1 0.5278 val_f1 0.5348\n",
      "Epoch 592, train_loss: 0.68848 val_loss 0.69088  train_accuracy 0.5389 val_accuracy 0.5269  train_f1 0.5295 val_f1 0.5360\n",
      "Epoch 593, train_loss: 0.68852 val_loss 0.69088  train_accuracy 0.5387 val_accuracy 0.5266  train_f1 0.5296 val_f1 0.5352\n",
      "Epoch 594, train_loss: 0.68851 val_loss 0.69088  train_accuracy 0.5386 val_accuracy 0.5265  train_f1 0.5273 val_f1 0.5351\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-595-0.6909.hdf5\n",
      "Epoch 595, train_loss: 0.68850 val_loss 0.69088  train_accuracy 0.5388 val_accuracy 0.5266  train_f1 0.5289 val_f1 0.5353\n",
      "Epoch 596, train_loss: 0.68854 val_loss 0.69088  train_accuracy 0.5386 val_accuracy 0.5264  train_f1 0.5296 val_f1 0.5356\n",
      "Epoch 597, train_loss: 0.68854 val_loss 0.69088  train_accuracy 0.5388 val_accuracy 0.5267  train_f1 0.5282 val_f1 0.5345\n",
      "Epoch 598, train_loss: 0.68850 val_loss 0.69089  train_accuracy 0.5387 val_accuracy 0.5263  train_f1 0.5292 val_f1 0.5352\n",
      "Epoch 599, train_loss: 0.68850 val_loss 0.69088  train_accuracy 0.5388 val_accuracy 0.5268  train_f1 0.5284 val_f1 0.5340\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-600-0.6909.hdf5\n",
      "Epoch 600, train_loss: 0.68852 val_loss 0.69088  train_accuracy 0.5389 val_accuracy 0.5268  train_f1 0.5279 val_f1 0.5343\n",
      "Epoch 601, train_loss: 0.68852 val_loss 0.69088  train_accuracy 0.5388 val_accuracy 0.5268  train_f1 0.5293 val_f1 0.5363\n",
      "Epoch 602, train_loss: 0.68852 val_loss 0.69087  train_accuracy 0.5388 val_accuracy 0.5270  train_f1 0.5289 val_f1 0.5352\n",
      "Epoch 603, train_loss: 0.68854 val_loss 0.69089  train_accuracy 0.5389 val_accuracy 0.5266  train_f1 0.5282 val_f1 0.5346\n",
      "Epoch 604, train_loss: 0.68853 val_loss 0.69090  train_accuracy 0.5385 val_accuracy 0.5264  train_f1 0.5286 val_f1 0.5362\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-605-0.6909.hdf5\n",
      "Epoch 605, train_loss: 0.68845 val_loss 0.69088  train_accuracy 0.5389 val_accuracy 0.5266  train_f1 0.5297 val_f1 0.5349\n",
      "Epoch 606, train_loss: 0.68849 val_loss 0.69088  train_accuracy 0.5390 val_accuracy 0.5265  train_f1 0.5279 val_f1 0.5346\n",
      "Epoch 607, train_loss: 0.68846 val_loss 0.69088  train_accuracy 0.5393 val_accuracy 0.5264  train_f1 0.5298 val_f1 0.5342\n",
      "Epoch 608, train_loss: 0.68844 val_loss 0.69089  train_accuracy 0.5391 val_accuracy 0.5263  train_f1 0.5285 val_f1 0.5346\n",
      "Epoch 609, train_loss: 0.68847 val_loss 0.69089  train_accuracy 0.5389 val_accuracy 0.5261  train_f1 0.5288 val_f1 0.5349\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-610-0.6909.hdf5\n",
      "Epoch 610, train_loss: 0.68851 val_loss 0.69089  train_accuracy 0.5389 val_accuracy 0.5264  train_f1 0.5292 val_f1 0.5343\n",
      "Epoch 611, train_loss: 0.68848 val_loss 0.69089  train_accuracy 0.5389 val_accuracy 0.5264  train_f1 0.5288 val_f1 0.5347\n",
      "Epoch 612, train_loss: 0.68848 val_loss 0.69088  train_accuracy 0.5390 val_accuracy 0.5265  train_f1 0.5302 val_f1 0.5347\n",
      "Epoch 613, train_loss: 0.68843 val_loss 0.69087  train_accuracy 0.5386 val_accuracy 0.5269  train_f1 0.5277 val_f1 0.5336\n",
      "Epoch 614, train_loss: 0.68844 val_loss 0.69088  train_accuracy 0.5390 val_accuracy 0.5266  train_f1 0.5286 val_f1 0.5351\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-615-0.6909.hdf5\n",
      "Epoch 615, train_loss: 0.68845 val_loss 0.69088  train_accuracy 0.5390 val_accuracy 0.5265  train_f1 0.5288 val_f1 0.5341\n",
      "Epoch 616, train_loss: 0.68844 val_loss 0.69089  train_accuracy 0.5389 val_accuracy 0.5267  train_f1 0.5296 val_f1 0.5337\n",
      "Epoch 617, train_loss: 0.68842 val_loss 0.69088  train_accuracy 0.5390 val_accuracy 0.5262  train_f1 0.5275 val_f1 0.5339\n",
      "Epoch 618, train_loss: 0.68846 val_loss 0.69087  train_accuracy 0.5390 val_accuracy 0.5263  train_f1 0.5299 val_f1 0.5340\n",
      "Epoch 619, train_loss: 0.68840 val_loss 0.69087  train_accuracy 0.5391 val_accuracy 0.5262  train_f1 0.5290 val_f1 0.5335\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-620-0.6909.hdf5\n",
      "Epoch 620, train_loss: 0.68849 val_loss 0.69087  train_accuracy 0.5388 val_accuracy 0.5264  train_f1 0.5279 val_f1 0.5338\n",
      "Epoch 621, train_loss: 0.68844 val_loss 0.69087  train_accuracy 0.5392 val_accuracy 0.5264  train_f1 0.5286 val_f1 0.5346\n",
      "Epoch 622, train_loss: 0.68839 val_loss 0.69087  train_accuracy 0.5391 val_accuracy 0.5264  train_f1 0.5294 val_f1 0.5341\n",
      "Epoch 623, train_loss: 0.68843 val_loss 0.69087  train_accuracy 0.5396 val_accuracy 0.5262  train_f1 0.5290 val_f1 0.5332\n",
      "Epoch 624, train_loss: 0.68843 val_loss 0.69087  train_accuracy 0.5390 val_accuracy 0.5260  train_f1 0.5292 val_f1 0.5337\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-625-0.6909.hdf5\n",
      "Epoch 625, train_loss: 0.68849 val_loss 0.69086  train_accuracy 0.5388 val_accuracy 0.5264  train_f1 0.5275 val_f1 0.5324\n",
      "Epoch 626, train_loss: 0.68842 val_loss 0.69086  train_accuracy 0.5394 val_accuracy 0.5264  train_f1 0.5291 val_f1 0.5336\n",
      "Epoch 627, train_loss: 0.68842 val_loss 0.69087  train_accuracy 0.5391 val_accuracy 0.5263  train_f1 0.5295 val_f1 0.5320\n",
      "Epoch 628, train_loss: 0.68842 val_loss 0.69086  train_accuracy 0.5394 val_accuracy 0.5264  train_f1 0.5275 val_f1 0.5327\n",
      "Epoch 629, train_loss: 0.68838 val_loss 0.69086  train_accuracy 0.5394 val_accuracy 0.5266  train_f1 0.5297 val_f1 0.5325\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-630-0.6909.hdf5\n",
      "Epoch 630, train_loss: 0.68840 val_loss 0.69087  train_accuracy 0.5391 val_accuracy 0.5267  train_f1 0.5280 val_f1 0.5340\n",
      "Epoch 631, train_loss: 0.68844 val_loss 0.69087  train_accuracy 0.5394 val_accuracy 0.5266  train_f1 0.5304 val_f1 0.5351\n",
      "Epoch 632, train_loss: 0.68840 val_loss 0.69087  train_accuracy 0.5394 val_accuracy 0.5264  train_f1 0.5290 val_f1 0.5334\n",
      "Epoch 633, train_loss: 0.68839 val_loss 0.69086  train_accuracy 0.5395 val_accuracy 0.5266  train_f1 0.5287 val_f1 0.5336\n",
      "Epoch 634, train_loss: 0.68843 val_loss 0.69087  train_accuracy 0.5390 val_accuracy 0.5264  train_f1 0.5297 val_f1 0.5338\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-635-0.6909.hdf5\n",
      "Epoch 635, train_loss: 0.68837 val_loss 0.69086  train_accuracy 0.5394 val_accuracy 0.5264  train_f1 0.5297 val_f1 0.5327\n",
      "Epoch 636, train_loss: 0.68841 val_loss 0.69088  train_accuracy 0.5392 val_accuracy 0.5260  train_f1 0.5289 val_f1 0.5346\n",
      "Epoch 637, train_loss: 0.68836 val_loss 0.69087  train_accuracy 0.5394 val_accuracy 0.5260  train_f1 0.5304 val_f1 0.5328\n",
      "Epoch 638, train_loss: 0.68834 val_loss 0.69087  train_accuracy 0.5393 val_accuracy 0.5260  train_f1 0.5287 val_f1 0.5339\n",
      "Epoch 639, train_loss: 0.68834 val_loss 0.69087  train_accuracy 0.5394 val_accuracy 0.5263  train_f1 0.5303 val_f1 0.5338\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-640-0.6909.hdf5\n",
      "Epoch 640, train_loss: 0.68842 val_loss 0.69087  train_accuracy 0.5389 val_accuracy 0.5264  train_f1 0.5284 val_f1 0.5339\n",
      "Epoch 641, train_loss: 0.68839 val_loss 0.69087  train_accuracy 0.5393 val_accuracy 0.5267  train_f1 0.5291 val_f1 0.5340\n",
      "Epoch 642, train_loss: 0.68841 val_loss 0.69087  train_accuracy 0.5388 val_accuracy 0.5266  train_f1 0.5289 val_f1 0.5341\n",
      "Epoch 643, train_loss: 0.68839 val_loss 0.69087  train_accuracy 0.5394 val_accuracy 0.5267  train_f1 0.5292 val_f1 0.5351\n",
      "Epoch 644, train_loss: 0.68839 val_loss 0.69086  train_accuracy 0.5390 val_accuracy 0.5268  train_f1 0.5296 val_f1 0.5337\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-645-0.6909.hdf5\n",
      "Epoch 645, train_loss: 0.68836 val_loss 0.69086  train_accuracy 0.5393 val_accuracy 0.5265  train_f1 0.5292 val_f1 0.5345\n",
      "Epoch 646, train_loss: 0.68832 val_loss 0.69086  train_accuracy 0.5395 val_accuracy 0.5266  train_f1 0.5298 val_f1 0.5347\n",
      "Epoch 647, train_loss: 0.68835 val_loss 0.69085  train_accuracy 0.5395 val_accuracy 0.5266  train_f1 0.5296 val_f1 0.5343\n",
      "Epoch 648, train_loss: 0.68836 val_loss 0.69087  train_accuracy 0.5393 val_accuracy 0.5269  train_f1 0.5305 val_f1 0.5357\n",
      "Epoch 649, train_loss: 0.68836 val_loss 0.69086  train_accuracy 0.5392 val_accuracy 0.5270  train_f1 0.5284 val_f1 0.5346\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-650-0.6909.hdf5\n",
      "Epoch 650, train_loss: 0.68832 val_loss 0.69085  train_accuracy 0.5392 val_accuracy 0.5269  train_f1 0.5292 val_f1 0.5352\n",
      "Epoch 651, train_loss: 0.68837 val_loss 0.69086  train_accuracy 0.5394 val_accuracy 0.5267  train_f1 0.5297 val_f1 0.5339\n",
      "Epoch 652, train_loss: 0.68838 val_loss 0.69087  train_accuracy 0.5392 val_accuracy 0.5268  train_f1 0.5287 val_f1 0.5354\n",
      "Epoch 653, train_loss: 0.68833 val_loss 0.69088  train_accuracy 0.5393 val_accuracy 0.5266  train_f1 0.5282 val_f1 0.5357\n",
      "Epoch 654, train_loss: 0.68832 val_loss 0.69087  train_accuracy 0.5398 val_accuracy 0.5270  train_f1 0.5307 val_f1 0.5357\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-655-0.6909.hdf5\n",
      "Epoch 655, train_loss: 0.68830 val_loss 0.69087  train_accuracy 0.5397 val_accuracy 0.5266  train_f1 0.5293 val_f1 0.5355\n",
      "Epoch 656, train_loss: 0.68831 val_loss 0.69087  train_accuracy 0.5395 val_accuracy 0.5265  train_f1 0.5299 val_f1 0.5356\n",
      "Epoch 657, train_loss: 0.68831 val_loss 0.69088  train_accuracy 0.5396 val_accuracy 0.5268  train_f1 0.5295 val_f1 0.5362\n",
      "Epoch 658, train_loss: 0.68833 val_loss 0.69087  train_accuracy 0.5397 val_accuracy 0.5265  train_f1 0.5303 val_f1 0.5358\n",
      "Epoch 659, train_loss: 0.68832 val_loss 0.69087  train_accuracy 0.5395 val_accuracy 0.5265  train_f1 0.5295 val_f1 0.5355\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-660-0.6909.hdf5\n",
      "Epoch 660, train_loss: 0.68835 val_loss 0.69088  train_accuracy 0.5396 val_accuracy 0.5267  train_f1 0.5306 val_f1 0.5359\n",
      "Epoch 661, train_loss: 0.68834 val_loss 0.69089  train_accuracy 0.5395 val_accuracy 0.5267  train_f1 0.5288 val_f1 0.5363\n",
      "Epoch 662, train_loss: 0.68832 val_loss 0.69089  train_accuracy 0.5397 val_accuracy 0.5269  train_f1 0.5306 val_f1 0.5366\n",
      "Epoch 663, train_loss: 0.68835 val_loss 0.69088  train_accuracy 0.5392 val_accuracy 0.5267  train_f1 0.5296 val_f1 0.5361\n",
      "Epoch 664, train_loss: 0.68827 val_loss 0.69087  train_accuracy 0.5395 val_accuracy 0.5269  train_f1 0.5293 val_f1 0.5368\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-665-0.6909.hdf5\n",
      "Epoch 665, train_loss: 0.68830 val_loss 0.69087  train_accuracy 0.5393 val_accuracy 0.5269  train_f1 0.5297 val_f1 0.5364\n",
      "Epoch 666, train_loss: 0.68830 val_loss 0.69086  train_accuracy 0.5395 val_accuracy 0.5266  train_f1 0.5310 val_f1 0.5365\n",
      "Epoch 667, train_loss: 0.68831 val_loss 0.69088  train_accuracy 0.5395 val_accuracy 0.5268  train_f1 0.5287 val_f1 0.5372\n",
      "Epoch 668, train_loss: 0.68828 val_loss 0.69088  train_accuracy 0.5396 val_accuracy 0.5269  train_f1 0.5312 val_f1 0.5373\n",
      "Epoch 669, train_loss: 0.68831 val_loss 0.69088  train_accuracy 0.5396 val_accuracy 0.5268  train_f1 0.5297 val_f1 0.5361\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-670-0.6909.hdf5\n",
      "Epoch 670, train_loss: 0.68828 val_loss 0.69088  train_accuracy 0.5392 val_accuracy 0.5269  train_f1 0.5290 val_f1 0.5370\n",
      "Epoch 671, train_loss: 0.68825 val_loss 0.69088  train_accuracy 0.5396 val_accuracy 0.5268  train_f1 0.5305 val_f1 0.5362\n",
      "Epoch 672, train_loss: 0.68831 val_loss 0.69089  train_accuracy 0.5391 val_accuracy 0.5269  train_f1 0.5304 val_f1 0.5373\n",
      "Epoch 673, train_loss: 0.68826 val_loss 0.69088  train_accuracy 0.5397 val_accuracy 0.5267  train_f1 0.5288 val_f1 0.5361\n",
      "Epoch 674, train_loss: 0.68833 val_loss 0.69088  train_accuracy 0.5394 val_accuracy 0.5264  train_f1 0.5305 val_f1 0.5360\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-675-0.6909.hdf5\n",
      "Epoch 675, train_loss: 0.68835 val_loss 0.69087  train_accuracy 0.5393 val_accuracy 0.5267  train_f1 0.5296 val_f1 0.5355\n",
      "Epoch 676, train_loss: 0.68829 val_loss 0.69087  train_accuracy 0.5392 val_accuracy 0.5266  train_f1 0.5277 val_f1 0.5359\n",
      "Epoch 677, train_loss: 0.68835 val_loss 0.69086  train_accuracy 0.5395 val_accuracy 0.5266  train_f1 0.5300 val_f1 0.5366\n",
      "Epoch 678, train_loss: 0.68839 val_loss 0.69086  train_accuracy 0.5391 val_accuracy 0.5267  train_f1 0.5297 val_f1 0.5364\n",
      "Epoch 679, train_loss: 0.68832 val_loss 0.69087  train_accuracy 0.5397 val_accuracy 0.5268  train_f1 0.5303 val_f1 0.5363\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-680-0.6909.hdf5\n",
      "Epoch 680, train_loss: 0.68824 val_loss 0.69087  train_accuracy 0.5398 val_accuracy 0.5269  train_f1 0.5310 val_f1 0.5368\n",
      "Epoch 681, train_loss: 0.68824 val_loss 0.69087  train_accuracy 0.5397 val_accuracy 0.5266  train_f1 0.5298 val_f1 0.5355\n",
      "Epoch 682, train_loss: 0.68824 val_loss 0.69087  train_accuracy 0.5397 val_accuracy 0.5269  train_f1 0.5306 val_f1 0.5359\n",
      "Epoch 683, train_loss: 0.68824 val_loss 0.69087  train_accuracy 0.5398 val_accuracy 0.5269  train_f1 0.5295 val_f1 0.5365\n",
      "Epoch 684, train_loss: 0.68822 val_loss 0.69087  train_accuracy 0.5399 val_accuracy 0.5266  train_f1 0.5309 val_f1 0.5357\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-685-0.6909.hdf5\n",
      "Epoch 685, train_loss: 0.68830 val_loss 0.69086  train_accuracy 0.5395 val_accuracy 0.5265  train_f1 0.5295 val_f1 0.5362\n",
      "Epoch 686, train_loss: 0.68825 val_loss 0.69086  train_accuracy 0.5397 val_accuracy 0.5265  train_f1 0.5306 val_f1 0.5355\n",
      "Epoch 687, train_loss: 0.68825 val_loss 0.69086  train_accuracy 0.5395 val_accuracy 0.5268  train_f1 0.5289 val_f1 0.5363\n",
      "Epoch 688, train_loss: 0.68821 val_loss 0.69087  train_accuracy 0.5399 val_accuracy 0.5269  train_f1 0.5314 val_f1 0.5363\n",
      "Epoch 689, train_loss: 0.68828 val_loss 0.69086  train_accuracy 0.5396 val_accuracy 0.5266  train_f1 0.5300 val_f1 0.5359\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-690-0.6909.hdf5\n",
      "Epoch 690, train_loss: 0.68826 val_loss 0.69087  train_accuracy 0.5395 val_accuracy 0.5268  train_f1 0.5299 val_f1 0.5354\n",
      "Epoch 691, train_loss: 0.68822 val_loss 0.69087  train_accuracy 0.5400 val_accuracy 0.5270  train_f1 0.5294 val_f1 0.5365\n",
      "Epoch 692, train_loss: 0.68827 val_loss 0.69087  train_accuracy 0.5396 val_accuracy 0.5269  train_f1 0.5303 val_f1 0.5363\n",
      "Epoch 693, train_loss: 0.68823 val_loss 0.69087  train_accuracy 0.5396 val_accuracy 0.5270  train_f1 0.5301 val_f1 0.5373\n",
      "Epoch 694, train_loss: 0.68817 val_loss 0.69087  train_accuracy 0.5395 val_accuracy 0.5272  train_f1 0.5305 val_f1 0.5371\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-695-0.6909.hdf5\n",
      "Epoch 695, train_loss: 0.68821 val_loss 0.69086  train_accuracy 0.5399 val_accuracy 0.5269  train_f1 0.5300 val_f1 0.5368\n",
      "Epoch 696, train_loss: 0.68825 val_loss 0.69087  train_accuracy 0.5398 val_accuracy 0.5271  train_f1 0.5312 val_f1 0.5373\n",
      "Epoch 697, train_loss: 0.68822 val_loss 0.69086  train_accuracy 0.5397 val_accuracy 0.5271  train_f1 0.5297 val_f1 0.5355\n",
      "Epoch 698, train_loss: 0.68824 val_loss 0.69087  train_accuracy 0.5397 val_accuracy 0.5270  train_f1 0.5303 val_f1 0.5375\n",
      "Epoch 699, train_loss: 0.68824 val_loss 0.69087  train_accuracy 0.5401 val_accuracy 0.5271  train_f1 0.5311 val_f1 0.5364\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-700-0.6909.hdf5\n",
      "Epoch 700, train_loss: 0.68826 val_loss 0.69088  train_accuracy 0.5398 val_accuracy 0.5271  train_f1 0.5302 val_f1 0.5373\n",
      "Epoch 701, train_loss: 0.68821 val_loss 0.69087  train_accuracy 0.5400 val_accuracy 0.5272  train_f1 0.5308 val_f1 0.5363\n",
      "Epoch 702, train_loss: 0.68823 val_loss 0.69087  train_accuracy 0.5398 val_accuracy 0.5274  train_f1 0.5299 val_f1 0.5368\n",
      "Epoch 703, train_loss: 0.68822 val_loss 0.69085  train_accuracy 0.5398 val_accuracy 0.5275  train_f1 0.5303 val_f1 0.5363\n",
      "Epoch 704, train_loss: 0.68818 val_loss 0.69085  train_accuracy 0.5398 val_accuracy 0.5275  train_f1 0.5296 val_f1 0.5360\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-705-0.6909.hdf5\n",
      "Epoch 705, train_loss: 0.68822 val_loss 0.69087  train_accuracy 0.5398 val_accuracy 0.5272  train_f1 0.5312 val_f1 0.5366\n",
      "Epoch 706, train_loss: 0.68817 val_loss 0.69086  train_accuracy 0.5396 val_accuracy 0.5274  train_f1 0.5301 val_f1 0.5356\n",
      "Epoch 707, train_loss: 0.68821 val_loss 0.69087  train_accuracy 0.5397 val_accuracy 0.5271  train_f1 0.5287 val_f1 0.5368\n",
      "Epoch 708, train_loss: 0.68819 val_loss 0.69086  train_accuracy 0.5396 val_accuracy 0.5273  train_f1 0.5316 val_f1 0.5367\n",
      "Epoch 709, train_loss: 0.68820 val_loss 0.69085  train_accuracy 0.5401 val_accuracy 0.5275  train_f1 0.5299 val_f1 0.5368\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-710-0.6909.hdf5\n",
      "Epoch 710, train_loss: 0.68819 val_loss 0.69086  train_accuracy 0.5399 val_accuracy 0.5275  train_f1 0.5304 val_f1 0.5366\n",
      "Epoch 711, train_loss: 0.68818 val_loss 0.69087  train_accuracy 0.5399 val_accuracy 0.5270  train_f1 0.5301 val_f1 0.5370\n",
      "Epoch 712, train_loss: 0.68817 val_loss 0.69084  train_accuracy 0.5399 val_accuracy 0.5278  train_f1 0.5310 val_f1 0.5362\n",
      "Epoch 713, train_loss: 0.68818 val_loss 0.69086  train_accuracy 0.5400 val_accuracy 0.5277  train_f1 0.5305 val_f1 0.5372\n",
      "Epoch 714, train_loss: 0.68824 val_loss 0.69087  train_accuracy 0.5396 val_accuracy 0.5274  train_f1 0.5301 val_f1 0.5370\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-715-0.6909.hdf5\n",
      "Epoch 715, train_loss: 0.68818 val_loss 0.69087  train_accuracy 0.5397 val_accuracy 0.5274  train_f1 0.5300 val_f1 0.5373\n",
      "Epoch 716, train_loss: 0.68818 val_loss 0.69086  train_accuracy 0.5400 val_accuracy 0.5279  train_f1 0.5309 val_f1 0.5379\n",
      "Epoch 717, train_loss: 0.68816 val_loss 0.69085  train_accuracy 0.5401 val_accuracy 0.5278  train_f1 0.5324 val_f1 0.5373\n",
      "Epoch 718, train_loss: 0.68817 val_loss 0.69086  train_accuracy 0.5394 val_accuracy 0.5279  train_f1 0.5290 val_f1 0.5367\n",
      "Epoch 719, train_loss: 0.68813 val_loss 0.69087  train_accuracy 0.5398 val_accuracy 0.5274  train_f1 0.5309 val_f1 0.5380\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-720-0.6909.hdf5\n",
      "Epoch 720, train_loss: 0.68812 val_loss 0.69088  train_accuracy 0.5401 val_accuracy 0.5275  train_f1 0.5312 val_f1 0.5376\n",
      "Epoch 721, train_loss: 0.68814 val_loss 0.69087  train_accuracy 0.5403 val_accuracy 0.5274  train_f1 0.5309 val_f1 0.5376\n",
      "Epoch 722, train_loss: 0.68813 val_loss 0.69088  train_accuracy 0.5399 val_accuracy 0.5273  train_f1 0.5309 val_f1 0.5383\n",
      "Epoch 723, train_loss: 0.68809 val_loss 0.69086  train_accuracy 0.5400 val_accuracy 0.5272  train_f1 0.5311 val_f1 0.5369\n",
      "Epoch 724, train_loss: 0.68811 val_loss 0.69087  train_accuracy 0.5401 val_accuracy 0.5275  train_f1 0.5296 val_f1 0.5365\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-725-0.6909.hdf5\n",
      "Epoch 725, train_loss: 0.68816 val_loss 0.69088  train_accuracy 0.5400 val_accuracy 0.5273  train_f1 0.5306 val_f1 0.5380\n",
      "Epoch 726, train_loss: 0.68818 val_loss 0.69088  train_accuracy 0.5397 val_accuracy 0.5275  train_f1 0.5312 val_f1 0.5384\n",
      "Epoch 727, train_loss: 0.68814 val_loss 0.69088  train_accuracy 0.5399 val_accuracy 0.5273  train_f1 0.5315 val_f1 0.5379\n",
      "Epoch 728, train_loss: 0.68821 val_loss 0.69088  train_accuracy 0.5398 val_accuracy 0.5278  train_f1 0.5299 val_f1 0.5379\n",
      "Epoch 729, train_loss: 0.68817 val_loss 0.69087  train_accuracy 0.5401 val_accuracy 0.5276  train_f1 0.5306 val_f1 0.5386\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-730-0.6909.hdf5\n",
      "Epoch 730, train_loss: 0.68814 val_loss 0.69087  train_accuracy 0.5399 val_accuracy 0.5276  train_f1 0.5314 val_f1 0.5386\n",
      "Epoch 731, train_loss: 0.68816 val_loss 0.69088  train_accuracy 0.5400 val_accuracy 0.5277  train_f1 0.5314 val_f1 0.5388\n",
      "Epoch 732, train_loss: 0.68812 val_loss 0.69087  train_accuracy 0.5401 val_accuracy 0.5280  train_f1 0.5309 val_f1 0.5387\n",
      "Epoch 733, train_loss: 0.68812 val_loss 0.69087  train_accuracy 0.5404 val_accuracy 0.5279  train_f1 0.5309 val_f1 0.5384\n",
      "Epoch 734, train_loss: 0.68810 val_loss 0.69086  train_accuracy 0.5401 val_accuracy 0.5278  train_f1 0.5314 val_f1 0.5389\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-735-0.6909.hdf5\n",
      "Epoch 735, train_loss: 0.68813 val_loss 0.69086  train_accuracy 0.5400 val_accuracy 0.5279  train_f1 0.5303 val_f1 0.5376\n",
      "Epoch 736, train_loss: 0.68814 val_loss 0.69088  train_accuracy 0.5398 val_accuracy 0.5278  train_f1 0.5303 val_f1 0.5397\n",
      "Epoch 737, train_loss: 0.68807 val_loss 0.69088  train_accuracy 0.5399 val_accuracy 0.5280  train_f1 0.5318 val_f1 0.5388\n",
      "Epoch 738, train_loss: 0.68809 val_loss 0.69088  train_accuracy 0.5401 val_accuracy 0.5278  train_f1 0.5302 val_f1 0.5386\n",
      "Epoch 739, train_loss: 0.68810 val_loss 0.69087  train_accuracy 0.5398 val_accuracy 0.5279  train_f1 0.5299 val_f1 0.5389\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-740-0.6909.hdf5\n",
      "Epoch 740, train_loss: 0.68811 val_loss 0.69087  train_accuracy 0.5403 val_accuracy 0.5277  train_f1 0.5321 val_f1 0.5385\n",
      "Epoch 741, train_loss: 0.68809 val_loss 0.69086  train_accuracy 0.5402 val_accuracy 0.5277  train_f1 0.5298 val_f1 0.5381\n",
      "Epoch 742, train_loss: 0.68813 val_loss 0.69088  train_accuracy 0.5401 val_accuracy 0.5276  train_f1 0.5312 val_f1 0.5392\n",
      "Epoch 743, train_loss: 0.68815 val_loss 0.69088  train_accuracy 0.5398 val_accuracy 0.5277  train_f1 0.5314 val_f1 0.5385\n",
      "Epoch 744, train_loss: 0.68812 val_loss 0.69086  train_accuracy 0.5401 val_accuracy 0.5277  train_f1 0.5307 val_f1 0.5383\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-745-0.6909.hdf5\n",
      "Epoch 745, train_loss: 0.68813 val_loss 0.69087  train_accuracy 0.5403 val_accuracy 0.5277  train_f1 0.5317 val_f1 0.5390\n",
      "Epoch 746, train_loss: 0.68814 val_loss 0.69087  train_accuracy 0.5400 val_accuracy 0.5276  train_f1 0.5301 val_f1 0.5382\n",
      "Epoch 747, train_loss: 0.68805 val_loss 0.69086  train_accuracy 0.5402 val_accuracy 0.5279  train_f1 0.5318 val_f1 0.5390\n",
      "Epoch 748, train_loss: 0.68811 val_loss 0.69086  train_accuracy 0.5402 val_accuracy 0.5277  train_f1 0.5318 val_f1 0.5382\n",
      "Epoch 749, train_loss: 0.68805 val_loss 0.69086  train_accuracy 0.5403 val_accuracy 0.5274  train_f1 0.5308 val_f1 0.5372\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-750-0.6909.hdf5\n",
      "Epoch 750, train_loss: 0.68806 val_loss 0.69087  train_accuracy 0.5400 val_accuracy 0.5275  train_f1 0.5302 val_f1 0.5386\n",
      "Epoch 751, train_loss: 0.68805 val_loss 0.69086  train_accuracy 0.5401 val_accuracy 0.5280  train_f1 0.5319 val_f1 0.5394\n",
      "Epoch 752, train_loss: 0.68814 val_loss 0.69088  train_accuracy 0.5401 val_accuracy 0.5281  train_f1 0.5302 val_f1 0.5385\n",
      "Epoch 753, train_loss: 0.68808 val_loss 0.69088  train_accuracy 0.5401 val_accuracy 0.5282  train_f1 0.5308 val_f1 0.5404\n",
      "Epoch 754, train_loss: 0.68806 val_loss 0.69088  train_accuracy 0.5404 val_accuracy 0.5280  train_f1 0.5321 val_f1 0.5398\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-755-0.6909.hdf5\n",
      "Epoch 755, train_loss: 0.68805 val_loss 0.69088  train_accuracy 0.5404 val_accuracy 0.5280  train_f1 0.5307 val_f1 0.5397\n",
      "Epoch 756, train_loss: 0.68806 val_loss 0.69088  train_accuracy 0.5402 val_accuracy 0.5277  train_f1 0.5322 val_f1 0.5396\n",
      "Epoch 757, train_loss: 0.68806 val_loss 0.69087  train_accuracy 0.5403 val_accuracy 0.5280  train_f1 0.5314 val_f1 0.5389\n",
      "Epoch 758, train_loss: 0.68799 val_loss 0.69086  train_accuracy 0.5405 val_accuracy 0.5282  train_f1 0.5312 val_f1 0.5392\n",
      "Epoch 759, train_loss: 0.68808 val_loss 0.69086  train_accuracy 0.5404 val_accuracy 0.5277  train_f1 0.5316 val_f1 0.5392\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-760-0.6909.hdf5\n",
      "Epoch 760, train_loss: 0.68801 val_loss 0.69087  train_accuracy 0.5405 val_accuracy 0.5274  train_f1 0.5314 val_f1 0.5385\n",
      "Epoch 761, train_loss: 0.68812 val_loss 0.69087  train_accuracy 0.5402 val_accuracy 0.5277  train_f1 0.5314 val_f1 0.5397\n",
      "Epoch 762, train_loss: 0.68805 val_loss 0.69085  train_accuracy 0.5404 val_accuracy 0.5278  train_f1 0.5318 val_f1 0.5384\n",
      "Epoch 763, train_loss: 0.68801 val_loss 0.69086  train_accuracy 0.5404 val_accuracy 0.5280  train_f1 0.5314 val_f1 0.5399\n",
      "Epoch 764, train_loss: 0.68802 val_loss 0.69085  train_accuracy 0.5403 val_accuracy 0.5279  train_f1 0.5300 val_f1 0.5377\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-765-0.6909.hdf5\n",
      "Epoch 765, train_loss: 0.68806 val_loss 0.69086  train_accuracy 0.5402 val_accuracy 0.5283  train_f1 0.5320 val_f1 0.5404\n",
      "Epoch 766, train_loss: 0.68800 val_loss 0.69085  train_accuracy 0.5401 val_accuracy 0.5282  train_f1 0.5311 val_f1 0.5387\n",
      "Epoch 767, train_loss: 0.68803 val_loss 0.69086  train_accuracy 0.5396 val_accuracy 0.5283  train_f1 0.5290 val_f1 0.5403\n",
      "Epoch 768, train_loss: 0.68807 val_loss 0.69087  train_accuracy 0.5402 val_accuracy 0.5281  train_f1 0.5328 val_f1 0.5407\n",
      "Epoch 769, train_loss: 0.68803 val_loss 0.69086  train_accuracy 0.5405 val_accuracy 0.5282  train_f1 0.5304 val_f1 0.5385\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-770-0.6909.hdf5\n",
      "Epoch 770, train_loss: 0.68796 val_loss 0.69086  train_accuracy 0.5405 val_accuracy 0.5283  train_f1 0.5320 val_f1 0.5397\n",
      "Epoch 771, train_loss: 0.68808 val_loss 0.69087  train_accuracy 0.5403 val_accuracy 0.5283  train_f1 0.5305 val_f1 0.5395\n",
      "Epoch 772, train_loss: 0.68801 val_loss 0.69087  train_accuracy 0.5403 val_accuracy 0.5283  train_f1 0.5314 val_f1 0.5393\n",
      "Epoch 773, train_loss: 0.68804 val_loss 0.69088  train_accuracy 0.5402 val_accuracy 0.5276  train_f1 0.5320 val_f1 0.5394\n",
      "Epoch 774, train_loss: 0.68800 val_loss 0.69088  train_accuracy 0.5403 val_accuracy 0.5278  train_f1 0.5317 val_f1 0.5395\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-775-0.6909.hdf5\n",
      "Epoch 775, train_loss: 0.68803 val_loss 0.69088  train_accuracy 0.5406 val_accuracy 0.5277  train_f1 0.5320 val_f1 0.5386\n",
      "Epoch 776, train_loss: 0.68802 val_loss 0.69089  train_accuracy 0.5402 val_accuracy 0.5279  train_f1 0.5311 val_f1 0.5394\n",
      "Epoch 777, train_loss: 0.68798 val_loss 0.69089  train_accuracy 0.5405 val_accuracy 0.5279  train_f1 0.5313 val_f1 0.5393\n",
      "Epoch 778, train_loss: 0.68800 val_loss 0.69089  train_accuracy 0.5404 val_accuracy 0.5282  train_f1 0.5315 val_f1 0.5399\n",
      "Epoch 779, train_loss: 0.68802 val_loss 0.69089  train_accuracy 0.5404 val_accuracy 0.5280  train_f1 0.5313 val_f1 0.5398\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-780-0.6909.hdf5\n",
      "Epoch 780, train_loss: 0.68799 val_loss 0.69089  train_accuracy 0.5402 val_accuracy 0.5276  train_f1 0.5317 val_f1 0.5395\n",
      "Epoch 781, train_loss: 0.68807 val_loss 0.69089  train_accuracy 0.5401 val_accuracy 0.5278  train_f1 0.5319 val_f1 0.5397\n",
      "Epoch 782, train_loss: 0.68796 val_loss 0.69088  train_accuracy 0.5403 val_accuracy 0.5275  train_f1 0.5325 val_f1 0.5392\n",
      "Epoch 783, train_loss: 0.68801 val_loss 0.69088  train_accuracy 0.5401 val_accuracy 0.5277  train_f1 0.5299 val_f1 0.5389\n",
      "Epoch 784, train_loss: 0.68802 val_loss 0.69089  train_accuracy 0.5403 val_accuracy 0.5276  train_f1 0.5319 val_f1 0.5403\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-785-0.6909.hdf5\n",
      "Epoch 785, train_loss: 0.68791 val_loss 0.69088  train_accuracy 0.5407 val_accuracy 0.5279  train_f1 0.5330 val_f1 0.5404\n",
      "Epoch 786, train_loss: 0.68798 val_loss 0.69089  train_accuracy 0.5407 val_accuracy 0.5274  train_f1 0.5314 val_f1 0.5390\n",
      "Epoch 787, train_loss: 0.68801 val_loss 0.69090  train_accuracy 0.5405 val_accuracy 0.5275  train_f1 0.5319 val_f1 0.5401\n",
      "Epoch 788, train_loss: 0.68799 val_loss 0.69089  train_accuracy 0.5406 val_accuracy 0.5279  train_f1 0.5316 val_f1 0.5399\n",
      "Epoch 789, train_loss: 0.68796 val_loss 0.69089  train_accuracy 0.5406 val_accuracy 0.5281  train_f1 0.5312 val_f1 0.5406\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-790-0.6909.hdf5\n",
      "Epoch 790, train_loss: 0.68791 val_loss 0.69089  train_accuracy 0.5408 val_accuracy 0.5283  train_f1 0.5330 val_f1 0.5412\n",
      "Epoch 791, train_loss: 0.68799 val_loss 0.69089  train_accuracy 0.5406 val_accuracy 0.5281  train_f1 0.5321 val_f1 0.5404\n",
      "Epoch 792, train_loss: 0.68794 val_loss 0.69088  train_accuracy 0.5405 val_accuracy 0.5281  train_f1 0.5311 val_f1 0.5395\n",
      "Epoch 793, train_loss: 0.68795 val_loss 0.69089  train_accuracy 0.5405 val_accuracy 0.5281  train_f1 0.5319 val_f1 0.5405\n",
      "Epoch 794, train_loss: 0.68794 val_loss 0.69089  train_accuracy 0.5408 val_accuracy 0.5276  train_f1 0.5314 val_f1 0.5398\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-795-0.6909.hdf5\n",
      "Epoch 795, train_loss: 0.68790 val_loss 0.69089  train_accuracy 0.5405 val_accuracy 0.5276  train_f1 0.5313 val_f1 0.5399\n",
      "Epoch 796, train_loss: 0.68797 val_loss 0.69088  train_accuracy 0.5405 val_accuracy 0.5280  train_f1 0.5323 val_f1 0.5406\n",
      "Epoch 797, train_loss: 0.68790 val_loss 0.69088  train_accuracy 0.5404 val_accuracy 0.5279  train_f1 0.5311 val_f1 0.5397\n",
      "Epoch 798, train_loss: 0.68793 val_loss 0.69088  train_accuracy 0.5406 val_accuracy 0.5282  train_f1 0.5321 val_f1 0.5420\n",
      "Epoch 799, train_loss: 0.68794 val_loss 0.69086  train_accuracy 0.5404 val_accuracy 0.5278  train_f1 0.5319 val_f1 0.5401\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-800-0.6909.hdf5\n",
      "Epoch 800, train_loss: 0.68798 val_loss 0.69088  train_accuracy 0.5405 val_accuracy 0.5281  train_f1 0.5323 val_f1 0.5409\n",
      "Epoch 801, train_loss: 0.68790 val_loss 0.69087  train_accuracy 0.5407 val_accuracy 0.5280  train_f1 0.5316 val_f1 0.5408\n",
      "Epoch 802, train_loss: 0.68799 val_loss 0.69088  train_accuracy 0.5403 val_accuracy 0.5281  train_f1 0.5326 val_f1 0.5412\n",
      "Epoch 803, train_loss: 0.68793 val_loss 0.69088  train_accuracy 0.5408 val_accuracy 0.5278  train_f1 0.5323 val_f1 0.5408\n",
      "Epoch 804, train_loss: 0.68794 val_loss 0.69089  train_accuracy 0.5406 val_accuracy 0.5276  train_f1 0.5316 val_f1 0.5412\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-805-0.6909.hdf5\n",
      "Epoch 805, train_loss: 0.68788 val_loss 0.69088  train_accuracy 0.5407 val_accuracy 0.5280  train_f1 0.5337 val_f1 0.5407\n",
      "Epoch 806, train_loss: 0.68791 val_loss 0.69088  train_accuracy 0.5407 val_accuracy 0.5279  train_f1 0.5317 val_f1 0.5409\n",
      "Epoch 807, train_loss: 0.68791 val_loss 0.69088  train_accuracy 0.5408 val_accuracy 0.5282  train_f1 0.5320 val_f1 0.5410\n",
      "Epoch 808, train_loss: 0.68790 val_loss 0.69088  train_accuracy 0.5405 val_accuracy 0.5283  train_f1 0.5315 val_f1 0.5416\n",
      "Epoch 809, train_loss: 0.68795 val_loss 0.69088  train_accuracy 0.5408 val_accuracy 0.5284  train_f1 0.5327 val_f1 0.5416\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-810-0.6909.hdf5\n",
      "Epoch 810, train_loss: 0.68787 val_loss 0.69089  train_accuracy 0.5408 val_accuracy 0.5281  train_f1 0.5331 val_f1 0.5409\n",
      "Epoch 811, train_loss: 0.68788 val_loss 0.69088  train_accuracy 0.5409 val_accuracy 0.5282  train_f1 0.5313 val_f1 0.5406\n",
      "Epoch 812, train_loss: 0.68789 val_loss 0.69088  train_accuracy 0.5407 val_accuracy 0.5280  train_f1 0.5322 val_f1 0.5408\n",
      "Epoch 813, train_loss: 0.68786 val_loss 0.69088  train_accuracy 0.5407 val_accuracy 0.5278  train_f1 0.5314 val_f1 0.5399\n",
      "Epoch 814, train_loss: 0.68792 val_loss 0.69088  train_accuracy 0.5407 val_accuracy 0.5281  train_f1 0.5320 val_f1 0.5405\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-815-0.6909.hdf5\n",
      "Epoch 815, train_loss: 0.68790 val_loss 0.69087  train_accuracy 0.5410 val_accuracy 0.5282  train_f1 0.5331 val_f1 0.5401\n",
      "Epoch 816, train_loss: 0.68788 val_loss 0.69087  train_accuracy 0.5410 val_accuracy 0.5280  train_f1 0.5319 val_f1 0.5398\n",
      "Epoch 817, train_loss: 0.68793 val_loss 0.69087  train_accuracy 0.5404 val_accuracy 0.5284  train_f1 0.5311 val_f1 0.5400\n",
      "Epoch 818, train_loss: 0.68786 val_loss 0.69088  train_accuracy 0.5406 val_accuracy 0.5283  train_f1 0.5308 val_f1 0.5407\n",
      "Epoch 819, train_loss: 0.68789 val_loss 0.69087  train_accuracy 0.5408 val_accuracy 0.5282  train_f1 0.5336 val_f1 0.5412\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-820-0.6909.hdf5\n",
      "Epoch 820, train_loss: 0.68790 val_loss 0.69088  train_accuracy 0.5405 val_accuracy 0.5285  train_f1 0.5310 val_f1 0.5402\n",
      "Epoch 821, train_loss: 0.68785 val_loss 0.69089  train_accuracy 0.5408 val_accuracy 0.5284  train_f1 0.5324 val_f1 0.5416\n",
      "Epoch 822, train_loss: 0.68790 val_loss 0.69090  train_accuracy 0.5405 val_accuracy 0.5284  train_f1 0.5304 val_f1 0.5414\n",
      "Epoch 823, train_loss: 0.68789 val_loss 0.69088  train_accuracy 0.5409 val_accuracy 0.5285  train_f1 0.5330 val_f1 0.5416\n",
      "Epoch 824, train_loss: 0.68791 val_loss 0.69088  train_accuracy 0.5409 val_accuracy 0.5287  train_f1 0.5324 val_f1 0.5419\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-825-0.6909.hdf5\n",
      "Epoch 825, train_loss: 0.68788 val_loss 0.69088  train_accuracy 0.5410 val_accuracy 0.5286  train_f1 0.5325 val_f1 0.5414\n",
      "Epoch 826, train_loss: 0.68788 val_loss 0.69089  train_accuracy 0.5410 val_accuracy 0.5289  train_f1 0.5329 val_f1 0.5419\n",
      "Epoch 827, train_loss: 0.68786 val_loss 0.69090  train_accuracy 0.5408 val_accuracy 0.5289  train_f1 0.5316 val_f1 0.5420\n",
      "Epoch 828, train_loss: 0.68788 val_loss 0.69089  train_accuracy 0.5408 val_accuracy 0.5289  train_f1 0.5324 val_f1 0.5419\n",
      "Epoch 829, train_loss: 0.68788 val_loss 0.69089  train_accuracy 0.5406 val_accuracy 0.5288  train_f1 0.5312 val_f1 0.5426\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-830-0.6909.hdf5\n",
      "Epoch 830, train_loss: 0.68786 val_loss 0.69089  train_accuracy 0.5407 val_accuracy 0.5286  train_f1 0.5332 val_f1 0.5420\n",
      "Epoch 831, train_loss: 0.68785 val_loss 0.69090  train_accuracy 0.5411 val_accuracy 0.5286  train_f1 0.5317 val_f1 0.5418\n",
      "Epoch 832, train_loss: 0.68791 val_loss 0.69090  train_accuracy 0.5406 val_accuracy 0.5283  train_f1 0.5325 val_f1 0.5414\n",
      "Epoch 833, train_loss: 0.68785 val_loss 0.69090  train_accuracy 0.5410 val_accuracy 0.5286  train_f1 0.5320 val_f1 0.5417\n",
      "Epoch 834, train_loss: 0.68780 val_loss 0.69089  train_accuracy 0.5411 val_accuracy 0.5286  train_f1 0.5326 val_f1 0.5426\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-835-0.6909.hdf5\n",
      "Epoch 835, train_loss: 0.68785 val_loss 0.69090  train_accuracy 0.5409 val_accuracy 0.5283  train_f1 0.5315 val_f1 0.5412\n",
      "Epoch 836, train_loss: 0.68781 val_loss 0.69090  train_accuracy 0.5413 val_accuracy 0.5286  train_f1 0.5331 val_f1 0.5428\n",
      "Epoch 837, train_loss: 0.68783 val_loss 0.69089  train_accuracy 0.5407 val_accuracy 0.5284  train_f1 0.5327 val_f1 0.5422\n",
      "Epoch 838, train_loss: 0.68782 val_loss 0.69088  train_accuracy 0.5410 val_accuracy 0.5284  train_f1 0.5328 val_f1 0.5409\n",
      "Epoch 839, train_loss: 0.68783 val_loss 0.69088  train_accuracy 0.5412 val_accuracy 0.5288  train_f1 0.5315 val_f1 0.5414\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-840-0.6909.hdf5\n",
      "Epoch 840, train_loss: 0.68786 val_loss 0.69090  train_accuracy 0.5408 val_accuracy 0.5284  train_f1 0.5330 val_f1 0.5428\n",
      "Epoch 841, train_loss: 0.68782 val_loss 0.69089  train_accuracy 0.5407 val_accuracy 0.5288  train_f1 0.5321 val_f1 0.5425\n",
      "Epoch 842, train_loss: 0.68782 val_loss 0.69089  train_accuracy 0.5409 val_accuracy 0.5284  train_f1 0.5317 val_f1 0.5419\n",
      "Epoch 843, train_loss: 0.68787 val_loss 0.69090  train_accuracy 0.5410 val_accuracy 0.5287  train_f1 0.5324 val_f1 0.5433\n",
      "Epoch 844, train_loss: 0.68781 val_loss 0.69090  train_accuracy 0.5408 val_accuracy 0.5287  train_f1 0.5330 val_f1 0.5427\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-845-0.6909.hdf5\n",
      "Epoch 845, train_loss: 0.68781 val_loss 0.69090  train_accuracy 0.5409 val_accuracy 0.5286  train_f1 0.5317 val_f1 0.5427\n",
      "Epoch 846, train_loss: 0.68781 val_loss 0.69089  train_accuracy 0.5405 val_accuracy 0.5286  train_f1 0.5325 val_f1 0.5427\n",
      "Epoch 847, train_loss: 0.68782 val_loss 0.69090  train_accuracy 0.5409 val_accuracy 0.5287  train_f1 0.5325 val_f1 0.5425\n",
      "Epoch 848, train_loss: 0.68781 val_loss 0.69089  train_accuracy 0.5411 val_accuracy 0.5282  train_f1 0.5326 val_f1 0.5421\n",
      "Epoch 849, train_loss: 0.68781 val_loss 0.69089  train_accuracy 0.5408 val_accuracy 0.5280  train_f1 0.5325 val_f1 0.5417\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-850-0.6909.hdf5\n",
      "Epoch 850, train_loss: 0.68779 val_loss 0.69089  train_accuracy 0.5411 val_accuracy 0.5281  train_f1 0.5323 val_f1 0.5423\n",
      "Epoch 851, train_loss: 0.68781 val_loss 0.69089  train_accuracy 0.5405 val_accuracy 0.5284  train_f1 0.5327 val_f1 0.5423\n",
      "Epoch 852, train_loss: 0.68773 val_loss 0.69089  train_accuracy 0.5411 val_accuracy 0.5283  train_f1 0.5325 val_f1 0.5419\n",
      "Epoch 853, train_loss: 0.68778 val_loss 0.69089  train_accuracy 0.5412 val_accuracy 0.5283  train_f1 0.5327 val_f1 0.5426\n",
      "Epoch 854, train_loss: 0.68780 val_loss 0.69089  train_accuracy 0.5410 val_accuracy 0.5284  train_f1 0.5323 val_f1 0.5429\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-855-0.6909.hdf5\n",
      "Epoch 855, train_loss: 0.68775 val_loss 0.69089  train_accuracy 0.5412 val_accuracy 0.5280  train_f1 0.5320 val_f1 0.5426\n",
      "Epoch 856, train_loss: 0.68776 val_loss 0.69090  train_accuracy 0.5414 val_accuracy 0.5281  train_f1 0.5335 val_f1 0.5435\n",
      "Epoch 857, train_loss: 0.68782 val_loss 0.69090  train_accuracy 0.5408 val_accuracy 0.5280  train_f1 0.5322 val_f1 0.5422\n",
      "Epoch 858, train_loss: 0.68780 val_loss 0.69091  train_accuracy 0.5409 val_accuracy 0.5283  train_f1 0.5326 val_f1 0.5447\n",
      "Epoch 859, train_loss: 0.68778 val_loss 0.69091  train_accuracy 0.5411 val_accuracy 0.5285  train_f1 0.5331 val_f1 0.5442\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-860-0.6909.hdf5\n",
      "Epoch 860, train_loss: 0.68781 val_loss 0.69092  train_accuracy 0.5409 val_accuracy 0.5284  train_f1 0.5335 val_f1 0.5447\n",
      "Epoch 861, train_loss: 0.68780 val_loss 0.69091  train_accuracy 0.5410 val_accuracy 0.5283  train_f1 0.5329 val_f1 0.5428\n",
      "Epoch 862, train_loss: 0.68772 val_loss 0.69092  train_accuracy 0.5410 val_accuracy 0.5284  train_f1 0.5328 val_f1 0.5437\n",
      "Epoch 863, train_loss: 0.68780 val_loss 0.69091  train_accuracy 0.5411 val_accuracy 0.5287  train_f1 0.5330 val_f1 0.5439\n",
      "Epoch 864, train_loss: 0.68773 val_loss 0.69091  train_accuracy 0.5414 val_accuracy 0.5286  train_f1 0.5331 val_f1 0.5445\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-865-0.6909.hdf5\n",
      "Epoch 865, train_loss: 0.68780 val_loss 0.69091  train_accuracy 0.5408 val_accuracy 0.5284  train_f1 0.5333 val_f1 0.5429\n",
      "Epoch 866, train_loss: 0.68775 val_loss 0.69090  train_accuracy 0.5412 val_accuracy 0.5283  train_f1 0.5320 val_f1 0.5436\n",
      "Epoch 867, train_loss: 0.68775 val_loss 0.69091  train_accuracy 0.5415 val_accuracy 0.5283  train_f1 0.5336 val_f1 0.5437\n",
      "Epoch 868, train_loss: 0.68776 val_loss 0.69091  train_accuracy 0.5410 val_accuracy 0.5283  train_f1 0.5330 val_f1 0.5434\n",
      "Epoch 869, train_loss: 0.68777 val_loss 0.69091  train_accuracy 0.5413 val_accuracy 0.5282  train_f1 0.5334 val_f1 0.5433\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-870-0.6909.hdf5\n",
      "Epoch 870, train_loss: 0.68773 val_loss 0.69091  train_accuracy 0.5412 val_accuracy 0.5284  train_f1 0.5334 val_f1 0.5428\n",
      "Epoch 871, train_loss: 0.68779 val_loss 0.69091  train_accuracy 0.5414 val_accuracy 0.5286  train_f1 0.5331 val_f1 0.5425\n",
      "Epoch 872, train_loss: 0.68778 val_loss 0.69092  train_accuracy 0.5409 val_accuracy 0.5284  train_f1 0.5311 val_f1 0.5431\n",
      "Epoch 873, train_loss: 0.68775 val_loss 0.69091  train_accuracy 0.5410 val_accuracy 0.5284  train_f1 0.5335 val_f1 0.5430\n",
      "Epoch 874, train_loss: 0.68779 val_loss 0.69091  train_accuracy 0.5408 val_accuracy 0.5284  train_f1 0.5327 val_f1 0.5429\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-875-0.6909.hdf5\n",
      "Epoch 875, train_loss: 0.68774 val_loss 0.69090  train_accuracy 0.5413 val_accuracy 0.5282  train_f1 0.5325 val_f1 0.5418\n",
      "Epoch 876, train_loss: 0.68775 val_loss 0.69091  train_accuracy 0.5415 val_accuracy 0.5283  train_f1 0.5335 val_f1 0.5427\n",
      "Epoch 877, train_loss: 0.68767 val_loss 0.69090  train_accuracy 0.5413 val_accuracy 0.5285  train_f1 0.5321 val_f1 0.5421\n",
      "Epoch 878, train_loss: 0.68766 val_loss 0.69090  train_accuracy 0.5413 val_accuracy 0.5281  train_f1 0.5334 val_f1 0.5432\n",
      "Epoch 879, train_loss: 0.68775 val_loss 0.69090  train_accuracy 0.5412 val_accuracy 0.5280  train_f1 0.5339 val_f1 0.5429\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-880-0.6909.hdf5\n",
      "Epoch 880, train_loss: 0.68774 val_loss 0.69092  train_accuracy 0.5414 val_accuracy 0.5285  train_f1 0.5335 val_f1 0.5436\n",
      "Epoch 881, train_loss: 0.68774 val_loss 0.69091  train_accuracy 0.5409 val_accuracy 0.5285  train_f1 0.5339 val_f1 0.5440\n",
      "Epoch 882, train_loss: 0.68773 val_loss 0.69091  train_accuracy 0.5411 val_accuracy 0.5289  train_f1 0.5337 val_f1 0.5434\n",
      "Epoch 883, train_loss: 0.68772 val_loss 0.69091  train_accuracy 0.5412 val_accuracy 0.5287  train_f1 0.5327 val_f1 0.5437\n",
      "Epoch 884, train_loss: 0.68771 val_loss 0.69090  train_accuracy 0.5412 val_accuracy 0.5286  train_f1 0.5344 val_f1 0.5432\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-885-0.6909.hdf5\n",
      "Epoch 885, train_loss: 0.68766 val_loss 0.69091  train_accuracy 0.5416 val_accuracy 0.5287  train_f1 0.5336 val_f1 0.5430\n",
      "Epoch 886, train_loss: 0.68771 val_loss 0.69091  train_accuracy 0.5414 val_accuracy 0.5283  train_f1 0.5332 val_f1 0.5430\n",
      "Epoch 887, train_loss: 0.68765 val_loss 0.69091  train_accuracy 0.5416 val_accuracy 0.5287  train_f1 0.5342 val_f1 0.5435\n",
      "Epoch 888, train_loss: 0.68771 val_loss 0.69091  train_accuracy 0.5409 val_accuracy 0.5286  train_f1 0.5320 val_f1 0.5430\n",
      "Epoch 889, train_loss: 0.68777 val_loss 0.69092  train_accuracy 0.5408 val_accuracy 0.5288  train_f1 0.5322 val_f1 0.5439\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-890-0.6909.hdf5\n",
      "Epoch 890, train_loss: 0.68764 val_loss 0.69093  train_accuracy 0.5415 val_accuracy 0.5290  train_f1 0.5341 val_f1 0.5439\n",
      "Epoch 891, train_loss: 0.68767 val_loss 0.69093  train_accuracy 0.5413 val_accuracy 0.5288  train_f1 0.5322 val_f1 0.5430\n",
      "Epoch 892, train_loss: 0.68771 val_loss 0.69093  train_accuracy 0.5412 val_accuracy 0.5290  train_f1 0.5324 val_f1 0.5439\n",
      "Epoch 893, train_loss: 0.68768 val_loss 0.69093  train_accuracy 0.5413 val_accuracy 0.5288  train_f1 0.5332 val_f1 0.5441\n",
      "Epoch 894, train_loss: 0.68762 val_loss 0.69092  train_accuracy 0.5413 val_accuracy 0.5287  train_f1 0.5334 val_f1 0.5433\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-895-0.6909.hdf5\n",
      "Epoch 895, train_loss: 0.68765 val_loss 0.69093  train_accuracy 0.5413 val_accuracy 0.5285  train_f1 0.5325 val_f1 0.5441\n",
      "Epoch 896, train_loss: 0.68772 val_loss 0.69093  train_accuracy 0.5409 val_accuracy 0.5283  train_f1 0.5330 val_f1 0.5439\n",
      "Epoch 897, train_loss: 0.68768 val_loss 0.69093  train_accuracy 0.5413 val_accuracy 0.5284  train_f1 0.5334 val_f1 0.5433\n",
      "Epoch 898, train_loss: 0.68764 val_loss 0.69092  train_accuracy 0.5414 val_accuracy 0.5280  train_f1 0.5322 val_f1 0.5426\n",
      "Epoch 899, train_loss: 0.68767 val_loss 0.69093  train_accuracy 0.5413 val_accuracy 0.5281  train_f1 0.5334 val_f1 0.5442\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-900-0.6909.hdf5\n",
      "Epoch 900, train_loss: 0.68768 val_loss 0.69093  train_accuracy 0.5416 val_accuracy 0.5279  train_f1 0.5334 val_f1 0.5430\n",
      "Epoch 901, train_loss: 0.68766 val_loss 0.69093  train_accuracy 0.5413 val_accuracy 0.5280  train_f1 0.5337 val_f1 0.5441\n",
      "Epoch 902, train_loss: 0.68764 val_loss 0.69093  train_accuracy 0.5414 val_accuracy 0.5278  train_f1 0.5332 val_f1 0.5437\n",
      "Epoch 903, train_loss: 0.68771 val_loss 0.69093  train_accuracy 0.5413 val_accuracy 0.5281  train_f1 0.5343 val_f1 0.5441\n",
      "Epoch 904, train_loss: 0.68764 val_loss 0.69094  train_accuracy 0.5413 val_accuracy 0.5281  train_f1 0.5327 val_f1 0.5434\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-905-0.6909.hdf5\n",
      "Epoch 905, train_loss: 0.68768 val_loss 0.69093  train_accuracy 0.5415 val_accuracy 0.5280  train_f1 0.5330 val_f1 0.5437\n",
      "Epoch 906, train_loss: 0.68770 val_loss 0.69094  train_accuracy 0.5415 val_accuracy 0.5280  train_f1 0.5334 val_f1 0.5437\n",
      "Epoch 907, train_loss: 0.68763 val_loss 0.69094  train_accuracy 0.5415 val_accuracy 0.5283  train_f1 0.5334 val_f1 0.5449\n",
      "Epoch 908, train_loss: 0.68765 val_loss 0.69094  train_accuracy 0.5415 val_accuracy 0.5284  train_f1 0.5343 val_f1 0.5450\n",
      "Epoch 909, train_loss: 0.68765 val_loss 0.69094  train_accuracy 0.5413 val_accuracy 0.5284  train_f1 0.5332 val_f1 0.5444\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-910-0.6909.hdf5\n",
      "Epoch 910, train_loss: 0.68764 val_loss 0.69093  train_accuracy 0.5414 val_accuracy 0.5283  train_f1 0.5340 val_f1 0.5442\n",
      "Epoch 911, train_loss: 0.68768 val_loss 0.69094  train_accuracy 0.5413 val_accuracy 0.5285  train_f1 0.5335 val_f1 0.5440\n",
      "Epoch 912, train_loss: 0.68763 val_loss 0.69093  train_accuracy 0.5415 val_accuracy 0.5286  train_f1 0.5328 val_f1 0.5447\n",
      "Epoch 913, train_loss: 0.68765 val_loss 0.69092  train_accuracy 0.5415 val_accuracy 0.5286  train_f1 0.5337 val_f1 0.5442\n",
      "Epoch 914, train_loss: 0.68762 val_loss 0.69092  train_accuracy 0.5413 val_accuracy 0.5287  train_f1 0.5325 val_f1 0.5442\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-915-0.6909.hdf5\n",
      "Epoch 915, train_loss: 0.68773 val_loss 0.69093  train_accuracy 0.5414 val_accuracy 0.5285  train_f1 0.5333 val_f1 0.5448\n",
      "Epoch 916, train_loss: 0.68762 val_loss 0.69093  train_accuracy 0.5414 val_accuracy 0.5285  train_f1 0.5332 val_f1 0.5447\n",
      "Epoch 917, train_loss: 0.68760 val_loss 0.69094  train_accuracy 0.5415 val_accuracy 0.5282  train_f1 0.5346 val_f1 0.5450\n",
      "Epoch 918, train_loss: 0.68760 val_loss 0.69095  train_accuracy 0.5415 val_accuracy 0.5284  train_f1 0.5336 val_f1 0.5450\n",
      "Epoch 919, train_loss: 0.68761 val_loss 0.69093  train_accuracy 0.5416 val_accuracy 0.5282  train_f1 0.5338 val_f1 0.5437\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-920-0.6909.hdf5\n",
      "Epoch 920, train_loss: 0.68761 val_loss 0.69093  train_accuracy 0.5417 val_accuracy 0.5280  train_f1 0.5330 val_f1 0.5431\n",
      "Epoch 921, train_loss: 0.68761 val_loss 0.69094  train_accuracy 0.5412 val_accuracy 0.5280  train_f1 0.5335 val_f1 0.5443\n",
      "Epoch 922, train_loss: 0.68763 val_loss 0.69093  train_accuracy 0.5414 val_accuracy 0.5282  train_f1 0.5330 val_f1 0.5439\n",
      "Epoch 923, train_loss: 0.68766 val_loss 0.69094  train_accuracy 0.5413 val_accuracy 0.5281  train_f1 0.5335 val_f1 0.5436\n",
      "Epoch 924, train_loss: 0.68766 val_loss 0.69095  train_accuracy 0.5414 val_accuracy 0.5283  train_f1 0.5333 val_f1 0.5443\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-925-0.6910.hdf5\n",
      "Epoch 925, train_loss: 0.68761 val_loss 0.69094  train_accuracy 0.5415 val_accuracy 0.5282  train_f1 0.5334 val_f1 0.5443\n",
      "Epoch 926, train_loss: 0.68762 val_loss 0.69094  train_accuracy 0.5415 val_accuracy 0.5285  train_f1 0.5339 val_f1 0.5450\n",
      "Epoch 927, train_loss: 0.68753 val_loss 0.69094  train_accuracy 0.5418 val_accuracy 0.5283  train_f1 0.5340 val_f1 0.5445\n",
      "Epoch 928, train_loss: 0.68760 val_loss 0.69094  train_accuracy 0.5414 val_accuracy 0.5283  train_f1 0.5328 val_f1 0.5441\n",
      "Epoch 929, train_loss: 0.68760 val_loss 0.69095  train_accuracy 0.5416 val_accuracy 0.5282  train_f1 0.5343 val_f1 0.5445\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-930-0.6910.hdf5\n",
      "Epoch 930, train_loss: 0.68763 val_loss 0.69095  train_accuracy 0.5417 val_accuracy 0.5285  train_f1 0.5330 val_f1 0.5443\n",
      "Epoch 931, train_loss: 0.68756 val_loss 0.69094  train_accuracy 0.5414 val_accuracy 0.5283  train_f1 0.5331 val_f1 0.5444\n",
      "Epoch 932, train_loss: 0.68755 val_loss 0.69093  train_accuracy 0.5418 val_accuracy 0.5282  train_f1 0.5340 val_f1 0.5435\n",
      "Epoch 933, train_loss: 0.68759 val_loss 0.69094  train_accuracy 0.5413 val_accuracy 0.5284  train_f1 0.5328 val_f1 0.5451\n",
      "Epoch 934, train_loss: 0.68760 val_loss 0.69094  train_accuracy 0.5416 val_accuracy 0.5281  train_f1 0.5347 val_f1 0.5452\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-935-0.6909.hdf5\n",
      "Epoch 935, train_loss: 0.68757 val_loss 0.69094  train_accuracy 0.5416 val_accuracy 0.5280  train_f1 0.5336 val_f1 0.5435\n",
      "Epoch 936, train_loss: 0.68762 val_loss 0.69095  train_accuracy 0.5415 val_accuracy 0.5281  train_f1 0.5332 val_f1 0.5453\n",
      "Epoch 937, train_loss: 0.68757 val_loss 0.69095  train_accuracy 0.5417 val_accuracy 0.5283  train_f1 0.5350 val_f1 0.5455\n",
      "Epoch 938, train_loss: 0.68765 val_loss 0.69095  train_accuracy 0.5415 val_accuracy 0.5281  train_f1 0.5341 val_f1 0.5449\n",
      "Epoch 939, train_loss: 0.68761 val_loss 0.69095  train_accuracy 0.5413 val_accuracy 0.5285  train_f1 0.5332 val_f1 0.5444\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-940-0.6910.hdf5\n",
      "Epoch 940, train_loss: 0.68757 val_loss 0.69094  train_accuracy 0.5416 val_accuracy 0.5285  train_f1 0.5340 val_f1 0.5442\n",
      "Epoch 941, train_loss: 0.68751 val_loss 0.69094  train_accuracy 0.5417 val_accuracy 0.5285  train_f1 0.5338 val_f1 0.5448\n",
      "Epoch 942, train_loss: 0.68761 val_loss 0.69094  train_accuracy 0.5418 val_accuracy 0.5287  train_f1 0.5349 val_f1 0.5454\n",
      "Epoch 943, train_loss: 0.68754 val_loss 0.69093  train_accuracy 0.5415 val_accuracy 0.5282  train_f1 0.5334 val_f1 0.5438\n",
      "Epoch 944, train_loss: 0.68749 val_loss 0.69093  train_accuracy 0.5419 val_accuracy 0.5283  train_f1 0.5343 val_f1 0.5445\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-945-0.6909.hdf5\n",
      "Epoch 945, train_loss: 0.68761 val_loss 0.69094  train_accuracy 0.5416 val_accuracy 0.5281  train_f1 0.5337 val_f1 0.5438\n",
      "Epoch 946, train_loss: 0.68748 val_loss 0.69095  train_accuracy 0.5416 val_accuracy 0.5282  train_f1 0.5339 val_f1 0.5451\n",
      "Epoch 947, train_loss: 0.68756 val_loss 0.69095  train_accuracy 0.5414 val_accuracy 0.5281  train_f1 0.5341 val_f1 0.5444\n",
      "Epoch 948, train_loss: 0.68752 val_loss 0.69093  train_accuracy 0.5416 val_accuracy 0.5282  train_f1 0.5334 val_f1 0.5432\n",
      "Epoch 949, train_loss: 0.68754 val_loss 0.69094  train_accuracy 0.5418 val_accuracy 0.5284  train_f1 0.5334 val_f1 0.5434\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-950-0.6909.hdf5\n",
      "Epoch 950, train_loss: 0.68752 val_loss 0.69095  train_accuracy 0.5421 val_accuracy 0.5284  train_f1 0.5335 val_f1 0.5440\n",
      "Epoch 951, train_loss: 0.68753 val_loss 0.69095  train_accuracy 0.5416 val_accuracy 0.5284  train_f1 0.5335 val_f1 0.5433\n",
      "Epoch 952, train_loss: 0.68756 val_loss 0.69095  train_accuracy 0.5418 val_accuracy 0.5281  train_f1 0.5337 val_f1 0.5437\n",
      "Epoch 953, train_loss: 0.68757 val_loss 0.69095  train_accuracy 0.5418 val_accuracy 0.5282  train_f1 0.5341 val_f1 0.5441\n",
      "Epoch 954, train_loss: 0.68753 val_loss 0.69094  train_accuracy 0.5420 val_accuracy 0.5281  train_f1 0.5345 val_f1 0.5429\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-955-0.6909.hdf5\n",
      "Epoch 955, train_loss: 0.68752 val_loss 0.69095  train_accuracy 0.5420 val_accuracy 0.5281  train_f1 0.5339 val_f1 0.5443\n",
      "Epoch 956, train_loss: 0.68755 val_loss 0.69095  train_accuracy 0.5416 val_accuracy 0.5282  train_f1 0.5340 val_f1 0.5434\n",
      "Epoch 957, train_loss: 0.68757 val_loss 0.69094  train_accuracy 0.5417 val_accuracy 0.5281  train_f1 0.5328 val_f1 0.5437\n",
      "Epoch 958, train_loss: 0.68754 val_loss 0.69094  train_accuracy 0.5419 val_accuracy 0.5283  train_f1 0.5338 val_f1 0.5442\n",
      "Epoch 959, train_loss: 0.68750 val_loss 0.69094  train_accuracy 0.5419 val_accuracy 0.5287  train_f1 0.5342 val_f1 0.5445\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-960-0.6909.hdf5\n",
      "Epoch 960, train_loss: 0.68745 val_loss 0.69094  train_accuracy 0.5419 val_accuracy 0.5285  train_f1 0.5326 val_f1 0.5441\n",
      "Epoch 961, train_loss: 0.68750 val_loss 0.69094  train_accuracy 0.5418 val_accuracy 0.5286  train_f1 0.5341 val_f1 0.5450\n",
      "Epoch 962, train_loss: 0.68753 val_loss 0.69095  train_accuracy 0.5418 val_accuracy 0.5285  train_f1 0.5344 val_f1 0.5453\n",
      "Epoch 963, train_loss: 0.68755 val_loss 0.69095  train_accuracy 0.5415 val_accuracy 0.5283  train_f1 0.5338 val_f1 0.5447\n",
      "Epoch 964, train_loss: 0.68746 val_loss 0.69094  train_accuracy 0.5423 val_accuracy 0.5284  train_f1 0.5344 val_f1 0.5450\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-965-0.6909.hdf5\n",
      "Epoch 965, train_loss: 0.68744 val_loss 0.69094  train_accuracy 0.5418 val_accuracy 0.5282  train_f1 0.5337 val_f1 0.5447\n",
      "Epoch 966, train_loss: 0.68749 val_loss 0.69095  train_accuracy 0.5417 val_accuracy 0.5284  train_f1 0.5337 val_f1 0.5448\n",
      "Epoch 967, train_loss: 0.68753 val_loss 0.69095  train_accuracy 0.5415 val_accuracy 0.5283  train_f1 0.5336 val_f1 0.5446\n",
      "Epoch 968, train_loss: 0.68752 val_loss 0.69094  train_accuracy 0.5419 val_accuracy 0.5283  train_f1 0.5344 val_f1 0.5445\n",
      "Epoch 969, train_loss: 0.68749 val_loss 0.69095  train_accuracy 0.5418 val_accuracy 0.5283  train_f1 0.5342 val_f1 0.5448\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-970-0.6909.hdf5\n",
      "Epoch 970, train_loss: 0.68746 val_loss 0.69095  train_accuracy 0.5420 val_accuracy 0.5286  train_f1 0.5346 val_f1 0.5448\n",
      "Epoch 971, train_loss: 0.68747 val_loss 0.69095  train_accuracy 0.5416 val_accuracy 0.5285  train_f1 0.5342 val_f1 0.5449\n",
      "Epoch 972, train_loss: 0.68743 val_loss 0.69094  train_accuracy 0.5422 val_accuracy 0.5284  train_f1 0.5334 val_f1 0.5442\n",
      "Epoch 973, train_loss: 0.68747 val_loss 0.69096  train_accuracy 0.5418 val_accuracy 0.5285  train_f1 0.5344 val_f1 0.5456\n",
      "Epoch 974, train_loss: 0.68751 val_loss 0.69097  train_accuracy 0.5420 val_accuracy 0.5284  train_f1 0.5349 val_f1 0.5448\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-975-0.6910.hdf5\n",
      "Epoch 975, train_loss: 0.68743 val_loss 0.69095  train_accuracy 0.5419 val_accuracy 0.5284  train_f1 0.5341 val_f1 0.5446\n",
      "Epoch 976, train_loss: 0.68748 val_loss 0.69095  train_accuracy 0.5422 val_accuracy 0.5282  train_f1 0.5341 val_f1 0.5447\n",
      "Epoch 977, train_loss: 0.68744 val_loss 0.69095  train_accuracy 0.5418 val_accuracy 0.5285  train_f1 0.5343 val_f1 0.5460\n",
      "Epoch 978, train_loss: 0.68747 val_loss 0.69096  train_accuracy 0.5417 val_accuracy 0.5286  train_f1 0.5349 val_f1 0.5456\n",
      "Epoch 979, train_loss: 0.68743 val_loss 0.69094  train_accuracy 0.5421 val_accuracy 0.5290  train_f1 0.5345 val_f1 0.5454\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-980-0.6909.hdf5\n",
      "Epoch 980, train_loss: 0.68747 val_loss 0.69095  train_accuracy 0.5424 val_accuracy 0.5286  train_f1 0.5344 val_f1 0.5454\n",
      "Epoch 981, train_loss: 0.68738 val_loss 0.69096  train_accuracy 0.5419 val_accuracy 0.5286  train_f1 0.5340 val_f1 0.5466\n",
      "Epoch 982, train_loss: 0.68742 val_loss 0.69095  train_accuracy 0.5421 val_accuracy 0.5286  train_f1 0.5346 val_f1 0.5459\n",
      "Epoch 983, train_loss: 0.68750 val_loss 0.69096  train_accuracy 0.5420 val_accuracy 0.5288  train_f1 0.5352 val_f1 0.5465\n",
      "Epoch 984, train_loss: 0.68747 val_loss 0.69096  train_accuracy 0.5418 val_accuracy 0.5288  train_f1 0.5342 val_f1 0.5457\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-985-0.6910.hdf5\n",
      "Epoch 985, train_loss: 0.68747 val_loss 0.69095  train_accuracy 0.5419 val_accuracy 0.5286  train_f1 0.5347 val_f1 0.5463\n",
      "Epoch 986, train_loss: 0.68745 val_loss 0.69096  train_accuracy 0.5421 val_accuracy 0.5284  train_f1 0.5350 val_f1 0.5457\n",
      "Epoch 987, train_loss: 0.68747 val_loss 0.69095  train_accuracy 0.5421 val_accuracy 0.5286  train_f1 0.5344 val_f1 0.5457\n",
      "Epoch 988, train_loss: 0.68740 val_loss 0.69095  train_accuracy 0.5423 val_accuracy 0.5284  train_f1 0.5348 val_f1 0.5453\n",
      "Epoch 989, train_loss: 0.68748 val_loss 0.69097  train_accuracy 0.5417 val_accuracy 0.5289  train_f1 0.5343 val_f1 0.5470\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-990-0.6910.hdf5\n",
      "Epoch 990, train_loss: 0.68739 val_loss 0.69097  train_accuracy 0.5422 val_accuracy 0.5287  train_f1 0.5357 val_f1 0.5461\n",
      "Epoch 991, train_loss: 0.68737 val_loss 0.69096  train_accuracy 0.5423 val_accuracy 0.5289  train_f1 0.5340 val_f1 0.5461\n",
      "Epoch 992, train_loss: 0.68747 val_loss 0.69098  train_accuracy 0.5419 val_accuracy 0.5289  train_f1 0.5350 val_f1 0.5473\n",
      "Epoch 993, train_loss: 0.68745 val_loss 0.69096  train_accuracy 0.5419 val_accuracy 0.5287  train_f1 0.5355 val_f1 0.5458\n",
      "Epoch 994, train_loss: 0.68745 val_loss 0.69097  train_accuracy 0.5416 val_accuracy 0.5288  train_f1 0.5337 val_f1 0.5466\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-995-0.6910.hdf5\n",
      "Epoch 995, train_loss: 0.68740 val_loss 0.69096  train_accuracy 0.5421 val_accuracy 0.5283  train_f1 0.5352 val_f1 0.5452\n",
      "Epoch 996, train_loss: 0.68744 val_loss 0.69096  train_accuracy 0.5418 val_accuracy 0.5283  train_f1 0.5333 val_f1 0.5449\n",
      "Epoch 997, train_loss: 0.68747 val_loss 0.69097  train_accuracy 0.5420 val_accuracy 0.5285  train_f1 0.5352 val_f1 0.5465\n",
      "Epoch 998, train_loss: 0.68740 val_loss 0.69095  train_accuracy 0.5422 val_accuracy 0.5280  train_f1 0.5339 val_f1 0.5441\n",
      "Epoch 999, train_loss: 0.68741 val_loss 0.69096  train_accuracy 0.5418 val_accuracy 0.5283  train_f1 0.5348 val_f1 0.5459\n",
      "saved to  ./output/MLPModel01_20170425_2130_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0.5-1000-0.6910.hdf5\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 1000\n",
    "hist = model.fit(\n",
    "    X_train.as_matrix(), Y_train, \n",
    "    validation_data=(X_dev.as_matrix(),Y_dev), \n",
    "    max_epochs=max_epochs,\n",
    "    es_patience=1000, \n",
    "    es_min_delta=1e-5,\n",
    "    batch_size=2*1024*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.plot(model.progress_callback.train_losses[-100:], label='train_loss')\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "plt.plot(model.progress_callback.validation_losses[-500:], label='validation_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#plt.plot(model.progress_callback.train_f1s, label='train_f1')\n",
    "#plt.plot(model.progress_callback.validation_f1s, label='validation_f1')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train_pred = model.predict(X_train.as_matrix(), batch_size=1024)\n",
    "# Y_train_pred_class = utils.prediction_to_category2(Y_train_pred)\n",
    "\n",
    "Y_dev_pred = model.predict(X_dev.as_matrix(), batch_size=1024)\n",
    "# Y_dev_pred_class = utils.prediction_to_category2(Y_dev_pred)\n",
    "\n",
    "Y_test_pred = model.predict(X_test.as_matrix(), batch_size=1024)\n",
    "# Y_test_pred_class = utils.prediction_to_category2(Y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random forest model\n",
    "# Y_test_pred = rf_model.predict(X_test)\n",
    "# Y_test_pred_class = utils.prediction_to_category2(Y_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adaboost model\n",
    "# Y_test_pred = adb_model.predict(X_test)\n",
    "# Y_test_pred_class = utils.prediction_to_category2(Y_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gaussian naive bayes model\n",
    "# Y_test_pred = gnb_model.predict(X_test)\n",
    "# Y_test_pred_class = utils.prediction_to_category2(Y_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#performance_report(\"train\", prices_train, lookahead, Y_train, Y_train_pred_class, cum_return_plot=True, heatmap=True, histogram=True)\n",
    "performance_report(\"dev\",  prices_dev,  lookahead, Y_dev, Y_dev_pred, cum_return_plot=True, heatmap=True, histogram=True)\n",
    "#performance_report(\"test\",  prices_test,  lookahead, Y_test, Y_test_pred, cum_return_plot=True, heatmap=True, histogram=True)\n",
    "\n",
    "#train_curve = precision_recall_curve(Y_train, Y_train_pred)\n",
    "#test_curve = precision_recall_curve(Y_test, Y_test_pred)\n",
    "\n",
    "# plt.plot(train_curve[0], train_curve[1], label='train')\n",
    "# plt.plot(test_curve[0], test_curve[1], label='test')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# sns.heatmap(confusion_matrix(Y_test, Y_test_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout = 0  , lookahead = 1\n",
    "\n",
    "| width | layers    | epochs | train | test |\n",
    "| -----: | ---- :     | ---:    | ---:   | ---:  |\n",
    "| 1     | 1         |  100   | .57   | .58  |\n",
    "| 1     | 100       |  100   | .58   | .59  |\n",
    "| 1     | 100       |  500   | .57   | .59  |\n",
    "| 1     | 100 x 100 |  100 | .58   | .59  |\n",
    "| 10    | 1         |  100 |  .57  | .59  |\n",
    "| 10    | 10        |  100 |  .57  | .58  |\n",
    "| 10    | 100       |  100 |  .58 | .58   |\n",
    "| 10    | 100 x 100 |  100 |  .60 | .58   |\n",
    "| 10    | 100 x 100 x 100 |  100 |  .62 | .57   |\n",
    "| 10    | 100 x 100 x 100 x 100 |  100 |  .63 | .56   |\n",
    "| 20    | 1 |  100 |  .56 | .58  |\n",
    "| 20    | 100 |  100 | .58  | .58  |\n",
    "| 40    | 100 |  500 x 500 x 500 | .60  | .61  |\n",
    "\n",
    "Dropout = 0.1, lookahead = 1 \n",
    "\n",
    "| width | layers    | epochs | train | test |\n",
    "| -----: | ---- :     | ---:    | ---:   | ---:  |\n",
    "| 10    | 100 x 100 x 100 x 100 |  100 | .60  | .58  |\n",
    "| 10    | 100 x 100 x 100 x 100 |  200 | .62  | .58  |\n",
    "\n",
    "\n",
    "Dropout = 0.2, lookahead = 1 \n",
    "\n",
    "| width | layers    | epochs | train | test |\n",
    "| -----: | ---- :     | ---:    | ---:   | ---:  |\n",
    "| 10    | 100 x 100 x 100 x 100 |  100 | .59 | .59  |\n",
    "| 10    | 100 x 100 x 100 x 100 |  200 | .60 | .58  |\n",
    "\n",
    "Dropout = 0.5, lookeahead = 1\n",
    "\n",
    "| width | layers    | epochs | train | test |\n",
    "| -----: | ---- :     | ---:    | ---:   | ---:  |\n",
    "| 60    | 500 x 500 x 500 x 500 |  200 | .64 | .62|\n",
    "| 60    | 100 x 100 x 100 x 100 |  200 | .64 | .65\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "- .681 / .57 100^3 / LA:1, W:60\n",
    "- .679 / .569 100^4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print( len(Y_test) )\n",
    "print( len(Y_test_pred_class) )\n",
    "print( len(prices_test))\n",
    "print( len(utils.future_return(prices_test, 1) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idea: look at mean future return on *perfect* 1min prediction. (or on last minute direction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sqrt(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i_s = []\n",
    "returns = (prices_test-prices_test.mean()).pct_change()\n",
    "mean_rets = []\n",
    "for a in np.arange(1,15,.1):\n",
    "    i = int(a*a)\n",
    "    test_returns = utils.future_return(prices_test, i).fillna(0).values\n",
    "    test_returns = test_returns - test_returns.mean()\n",
    "\n",
    "    # just use sign of latest return as predictor\n",
    "    idx = np.sign(returns)\n",
    "\n",
    "    mean_ret = (test_returns * idx).mean() * 1e4\n",
    "    mean_rets.append(mean_ret)\n",
    "    i_s.append(i) #*24))\n",
    "plt.plot(i_s, mean_rets,'.-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i_s = []\n",
    "mean_rets = []\n",
    "for a in np.arange(1,15,.1):\n",
    "    i = int(a*a)\n",
    "    test_returns = utils.future_return(prices_test, i).fillna(0).values\n",
    "    test_returns = test_returns - test_returns.mean()\n",
    "\n",
    "    idx = np.zeros(len(test_returns))\n",
    "    idx[Y_test_pred_class == 0] = -1\n",
    "    idx[Y_test_pred_class == 1] = 1\n",
    "\n",
    "    mean_ret = (test_returns * idx).mean() * 1e4\n",
    "    mean_rets.append(mean_ret)\n",
    "    i_s.append(i) #*24))\n",
    "plt.plot(i_s, mean_rets,'.-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ser = pd.Series(mean_rets, index = pd.DatetimeIndex(np.array(i_s)* 60e9))\n",
    "ser.plot()\n",
    "#plt.plot(i_s, mean_rets,'.-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prices_test.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_trade = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "18000 / (60*24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# load test dataset\n",
    "X_test, Y_test, prices_test = datasets.load(datasets.filename(dataset_name, lookahead, window, sym, 2012))\n",
    "Y_test_pred = model.predict(X_test.as_matrix(), batch_size=1024)\n",
    "\n",
    "performance_report(\"test\",  prices_test,  lookahead, Y_test, Y_test_pred, cum_return_plot=True)\n",
    "\n",
    "\n",
    "\n",
    "# TODO: demean fut_return.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random forest model\n",
    "# load test dataset\n",
    "X_test, Y_test, prices_test = datasets.load(datasets.filename(dataset_name, lookahead, window, sym, 2014))\n",
    "#X_test, Y_test, prices_test, fut_return_test = datasets.prepare_dataset2(df=datasets.random_ohlc(300000),lookahead=1, window=25)\n",
    "\n",
    "performance_report(\"RF model\",  prices_test,  lookahead, Y_test, rf_model.predict(X_test), cum_return_plot=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adaboost model\n",
    "# load test dataset\n",
    "X_test, Y_test, prices_test, fut_return_test = datasets.load(datasets.filename(dataset_name, lookahead, window, sym, 2014))\n",
    "#X_test, Y_test, prices_test, fut_return_test = datasets.prepare_dataset2(df=datasets.random_ohlc(300000),lookahead=1, window=25)\n",
    "Y_test_pred = adb_model.predict(X_test)\n",
    "Y_test_pred_class = utils.prediction_to_category2(Y_test_pred)\n",
    "performance_report(\"test\",  prices_test,  lookahead, Y_test, Y_test_pred_class)\n",
    "(fut_return_test.ix[Y_test_pred_class[:]]+1).cumprod().plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(fut_return_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
