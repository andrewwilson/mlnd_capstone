{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 580 (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "from model01 import MLPModel01\n",
    "from metrics import performance_report\n",
    "import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_recall_curve, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_categories = 2 # implicit in prepare_data (maybe parameterise)\n",
    "lookahead = 1\n",
    "window = 25\n",
    "sym = 'EURUSD'\n",
    "dataset_name = 'DS3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds_name = sym + '_' + dataset_name + '_20092014' \n",
    "X_train, Y_train, prices_train = \\\n",
    "    datasets.load(datasets.filename(dataset_name, lookahead, window, sym, 20092014))\n",
    "\n",
    "X_dev, Y_dev, prices_dev = \\\n",
    "    datasets.load(datasets.filename(dataset_name, lookahead, window, sym, 2015))\n",
    "\n",
    "# sample 50k records from 2015 as dev set\n",
    "dev_idx = np.random.choice(len(X_dev), 50000, replace=False)\n",
    "X_dev, Y_dev, prices_dev = \\\n",
    "    X_dev.ix[dev_idx], Y_dev.ix[dev_idx], prices_dev.ix[dev_idx]\n",
    "\n",
    "#X_test, Y_test, prices_test = \\\n",
    "#    datasets.load(datasets.filename(dataset_name, lookahead, window, sym, 2016))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EURUSD_DS3_20092014\n",
      "train (2101596, 99)\n",
      "dev (50000, 99)\n",
      "n_features: 99\n"
     ]
    }
   ],
   "source": [
    "print(train_ds_name)\n",
    "print(\"train\", X_train.shape)\n",
    "print(\"dev\", X_dev.shape)\n",
    "#print(\"test\", X_test.shape)\n",
    "n_features = X_train.shape[1]\n",
    "print (\"n_features:\", n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import SGDClassifier\n",
    "# lin_model = SGDClassifier(loss='hinge', penalty='l2', n_iter=200, n_jobs=10)\n",
    "# lin_model.fit(X_train.as_matrix(), Y_train)\n",
    "\n",
    "# print( \"train f1\", f1_score(Y_train, lin_model.predict(X_train), average='weighted') )\n",
    "# print( \"test f1\", f1_score(Y_test, lin_model.predict(X_test), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# lr_model = LogisticRegression(n_jobs=10)\n",
    "# lr_model.fit(X_train.as_matrix(), Y_train)\n",
    "\n",
    "# print( \"train f1\", f1_score(Y_train, lr_model.predict(X_train), average='weighted') )\n",
    "# print( \"test f1\", f1_score(Y_test, lr_model.predict(X_test), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# gnb_model = GaussianNB()\n",
    "# gnb_model.fit(X_train.as_matrix(), Y_train)\n",
    "\n",
    "# print( \"train f1\", f1_score(Y_train, gnb_model.predict(X_train), average='weighted') )\n",
    "# print( \"dev f1\", f1_score(Y_dev, gnb_model.predict(X_dev), average='weighted'))\n",
    "# print( \"test f1\", f1_score(Y_test, gnb_model.predict(X_test), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.externals import joblib\n",
    "# joblib.dump(gnb_model, './output/gnb_{}.pkl'.format(train_ds_name)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# adb_model = AdaBoostClassifier()\n",
    "# adb_model.fit(X_train.as_matrix(), Y_train)\n",
    "\n",
    "# print( \"train f1\", f1_score(Y_train, adb_model.predict(X_train), average='weighted') )\n",
    "# print( \"dev f1\", f1_score(Y_dev, adb_model.predict(X_dev), average='weighted'))\n",
    "# print( \"test f1\", f1_score(Y_test, adb_model.predict(X_test), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# rf_model = RandomForestClassifier(max_depth=5, n_jobs=12)\n",
    "# rf_model.fit(X_train.as_matrix(), Y_train)\n",
    "\n",
    "# print( \"train f1\", f1_score(Y_train, rf_model.predict(X_train), average='weighted') )\n",
    "# print( \"dev f1\", f1_score(Y_dev, rf_model.predict(X_dev), average='weighted'))\n",
    "# print( \"test f1\", f1_score(Y_test, rf_model.predict(X_test), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_widths = [500,500] \n",
    "\n",
    "dropout = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# todo: try 100 x 4 layers\n",
    "# 500 x 3 and 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compilation took: 0.1 seconds\n",
      "Model id:  MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                  (None, 500)           50000       dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 500)           250500      dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             501         dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 301,001\n",
      "Trainable params: 301,001\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = MLPModel01(train_ds_name, lookahead, n_features, n_categories, layer_widths, dropout)\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss: 0.87681 val_loss 0.80133  train_accuracy 0.5055 val_accuracy 0.4987  train_f1 0.3979 val_f1 0.6513\n",
      "Epoch 1, train_loss: 0.76334 val_loss 0.72816  train_accuracy 0.5055 val_accuracy 0.5074  train_f1 0.4213 val_f1 0.6023\n",
      "Epoch 2, train_loss: 0.72584 val_loss 0.71537  train_accuracy 0.5093 val_accuracy 0.5072  train_f1 0.4223 val_f1 0.5771\n",
      "Epoch 3, train_loss: 0.71190 val_loss 0.70977  train_accuracy 0.5125 val_accuracy 0.5088  train_f1 0.4622 val_f1 0.5706\n",
      "Epoch 4, train_loss: 0.70583 val_loss 0.70545  train_accuracy 0.5136 val_accuracy 0.5120  train_f1 0.4824 val_f1 0.5380\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-005-0.7055.hdf5\n",
      "Epoch 5, train_loss: 0.70241 val_loss 0.70344  train_accuracy 0.5172 val_accuracy 0.5121  train_f1 0.4896 val_f1 0.5204\n",
      "Epoch 6, train_loss: 0.70044 val_loss 0.70207  train_accuracy 0.5176 val_accuracy 0.5130  train_f1 0.4990 val_f1 0.4968\n",
      "Epoch 7, train_loss: 0.69916 val_loss 0.70133  train_accuracy 0.5197 val_accuracy 0.5136  train_f1 0.5017 val_f1 0.4884\n",
      "Epoch 8, train_loss: 0.69811 val_loss 0.70071  train_accuracy 0.5202 val_accuracy 0.5142  train_f1 0.5036 val_f1 0.4858\n",
      "Epoch 9, train_loss: 0.69734 val_loss 0.70006  train_accuracy 0.5217 val_accuracy 0.5141  train_f1 0.5062 val_f1 0.4894\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-010-0.7001.hdf5\n",
      "Epoch 10, train_loss: 0.69660 val_loss 0.69968  train_accuracy 0.5223 val_accuracy 0.5135  train_f1 0.5066 val_f1 0.4952\n",
      "Epoch 11, train_loss: 0.69598 val_loss 0.69921  train_accuracy 0.5234 val_accuracy 0.5141  train_f1 0.5090 val_f1 0.5041\n",
      "Epoch 12, train_loss: 0.69540 val_loss 0.69892  train_accuracy 0.5244 val_accuracy 0.5147  train_f1 0.5084 val_f1 0.4962\n",
      "Epoch 13, train_loss: 0.69490 val_loss 0.69865  train_accuracy 0.5251 val_accuracy 0.5161  train_f1 0.5092 val_f1 0.4956\n",
      "Epoch 14, train_loss: 0.69444 val_loss 0.69823  train_accuracy 0.5261 val_accuracy 0.5159  train_f1 0.5105 val_f1 0.4989\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-015-0.6982.hdf5\n",
      "Epoch 15, train_loss: 0.69400 val_loss 0.69820  train_accuracy 0.5270 val_accuracy 0.5165  train_f1 0.5111 val_f1 0.5056\n",
      "Epoch 16, train_loss: 0.69358 val_loss 0.69790  train_accuracy 0.5280 val_accuracy 0.5160  train_f1 0.5133 val_f1 0.4830\n",
      "Epoch 17, train_loss: 0.69323 val_loss 0.69764  train_accuracy 0.5288 val_accuracy 0.5165  train_f1 0.5085 val_f1 0.5043\n",
      "Epoch 18, train_loss: 0.69283 val_loss 0.69766  train_accuracy 0.5294 val_accuracy 0.5161  train_f1 0.5159 val_f1 0.5162\n",
      "Epoch 19, train_loss: 0.69249 val_loss 0.69732  train_accuracy 0.5303 val_accuracy 0.5168  train_f1 0.5148 val_f1 0.4885\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-020-0.6973.hdf5\n",
      "Epoch 20, train_loss: 0.69214 val_loss 0.69719  train_accuracy 0.5311 val_accuracy 0.5160  train_f1 0.5141 val_f1 0.5032\n",
      "Epoch 21, train_loss: 0.69182 val_loss 0.69700  train_accuracy 0.5316 val_accuracy 0.5176  train_f1 0.5171 val_f1 0.4901\n",
      "Epoch 22, train_loss: 0.69154 val_loss 0.69682  train_accuracy 0.5325 val_accuracy 0.5170  train_f1 0.5135 val_f1 0.5008\n",
      "Epoch 23, train_loss: 0.69122 val_loss 0.69684  train_accuracy 0.5332 val_accuracy 0.5176  train_f1 0.5178 val_f1 0.5023\n",
      "Epoch 24, train_loss: 0.69096 val_loss 0.69661  train_accuracy 0.5339 val_accuracy 0.5176  train_f1 0.5167 val_f1 0.5067\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-025-0.6966.hdf5\n",
      "Epoch 25, train_loss: 0.69069 val_loss 0.69661  train_accuracy 0.5345 val_accuracy 0.5187  train_f1 0.5229 val_f1 0.4858\n",
      "Epoch 26, train_loss: 0.69044 val_loss 0.69648  train_accuracy 0.5351 val_accuracy 0.5181  train_f1 0.5162 val_f1 0.5165\n",
      "Epoch 27, train_loss: 0.69015 val_loss 0.69638  train_accuracy 0.5357 val_accuracy 0.5185  train_f1 0.5198 val_f1 0.5021\n",
      "Epoch 28, train_loss: 0.68990 val_loss 0.69632  train_accuracy 0.5365 val_accuracy 0.5181  train_f1 0.5207 val_f1 0.5009\n",
      "Epoch 29, train_loss: 0.68965 val_loss 0.69623  train_accuracy 0.5371 val_accuracy 0.5190  train_f1 0.5217 val_f1 0.4999\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-030-0.6962.hdf5\n",
      "Epoch 30, train_loss: 0.68942 val_loss 0.69614  train_accuracy 0.5377 val_accuracy 0.5191  train_f1 0.5218 val_f1 0.5071\n",
      "Epoch 31, train_loss: 0.68919 val_loss 0.69610  train_accuracy 0.5384 val_accuracy 0.5192  train_f1 0.5230 val_f1 0.5064\n",
      "Epoch 32, train_loss: 0.68898 val_loss 0.69614  train_accuracy 0.5389 val_accuracy 0.5183  train_f1 0.5225 val_f1 0.5161\n",
      "Epoch 33, train_loss: 0.68879 val_loss 0.69614  train_accuracy 0.5395 val_accuracy 0.5198  train_f1 0.5271 val_f1 0.4820\n",
      "Epoch 34, train_loss: 0.68861 val_loss 0.69601  train_accuracy 0.5398 val_accuracy 0.5192  train_f1 0.5232 val_f1 0.5140\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-035-0.6960.hdf5\n",
      "Epoch 35, train_loss: 0.68839 val_loss 0.69587  train_accuracy 0.5407 val_accuracy 0.5192  train_f1 0.5233 val_f1 0.5017\n",
      "Epoch 36, train_loss: 0.68816 val_loss 0.69588  train_accuracy 0.5413 val_accuracy 0.5186  train_f1 0.5251 val_f1 0.5124\n",
      "Epoch 37, train_loss: 0.68800 val_loss 0.69600  train_accuracy 0.5418 val_accuracy 0.5187  train_f1 0.5255 val_f1 0.5332\n",
      "Epoch 38, train_loss: 0.68786 val_loss 0.69575  train_accuracy 0.5421 val_accuracy 0.5194  train_f1 0.5267 val_f1 0.5151\n",
      "Epoch 39, train_loss: 0.68764 val_loss 0.69576  train_accuracy 0.5426 val_accuracy 0.5190  train_f1 0.5320 val_f1 0.4848\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-040-0.6958.hdf5\n",
      "Epoch 40, train_loss: 0.68743 val_loss 0.69572  train_accuracy 0.5430 val_accuracy 0.5197  train_f1 0.5258 val_f1 0.5092\n",
      "Epoch 41, train_loss: 0.68725 val_loss 0.69557  train_accuracy 0.5437 val_accuracy 0.5199  train_f1 0.5262 val_f1 0.5132\n",
      "Epoch 42, train_loss: 0.68706 val_loss 0.69575  train_accuracy 0.5442 val_accuracy 0.5201  train_f1 0.5296 val_f1 0.5167\n",
      "Epoch 43, train_loss: 0.68693 val_loss 0.69565  train_accuracy 0.5446 val_accuracy 0.5207  train_f1 0.5298 val_f1 0.5218\n",
      "Epoch 44, train_loss: 0.68674 val_loss 0.69576  train_accuracy 0.5452 val_accuracy 0.5206  train_f1 0.5310 val_f1 0.5123\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-045-0.6958.hdf5\n",
      "Epoch 45, train_loss: 0.68655 val_loss 0.69553  train_accuracy 0.5456 val_accuracy 0.5206  train_f1 0.5321 val_f1 0.4930\n",
      "Epoch 46, train_loss: 0.68643 val_loss 0.69558  train_accuracy 0.5460 val_accuracy 0.5199  train_f1 0.5313 val_f1 0.5006\n",
      "Epoch 47, train_loss: 0.68621 val_loss 0.69560  train_accuracy 0.5465 val_accuracy 0.5200  train_f1 0.5307 val_f1 0.5103\n",
      "\n",
      "Epoch 00047: reducing learning rate to 0.000200000009499.\n",
      "Epoch 48, train_loss: 0.68603 val_loss 0.69551  train_accuracy 0.5471 val_accuracy 0.5194  train_f1 0.5325 val_f1 0.5036\n",
      "Epoch 49, train_loss: 0.68599 val_loss 0.69551  train_accuracy 0.5470 val_accuracy 0.5196  train_f1 0.5314 val_f1 0.5027\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-050-0.6955.hdf5\n",
      "Epoch 50, train_loss: 0.68595 val_loss 0.69552  train_accuracy 0.5473 val_accuracy 0.5195  train_f1 0.5304 val_f1 0.5112\n",
      "Epoch 51, train_loss: 0.68591 val_loss 0.69551  train_accuracy 0.5473 val_accuracy 0.5191  train_f1 0.5306 val_f1 0.5143\n",
      "Epoch 52, train_loss: 0.68588 val_loss 0.69548  train_accuracy 0.5474 val_accuracy 0.5199  train_f1 0.5329 val_f1 0.5116\n",
      "Epoch 53, train_loss: 0.68585 val_loss 0.69550  train_accuracy 0.5476 val_accuracy 0.5196  train_f1 0.5323 val_f1 0.5117\n",
      "Epoch 54, train_loss: 0.68581 val_loss 0.69548  train_accuracy 0.5475 val_accuracy 0.5195  train_f1 0.5342 val_f1 0.5032\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-055-0.6955.hdf5\n",
      "\n",
      "Epoch 00054: reducing learning rate to 5e-05.\n",
      "Epoch 55, train_loss: 0.68576 val_loss 0.69550  train_accuracy 0.5477 val_accuracy 0.5196  train_f1 0.5310 val_f1 0.5092\n",
      "Epoch 56, train_loss: 0.68575 val_loss 0.69549  train_accuracy 0.5478 val_accuracy 0.5199  train_f1 0.5340 val_f1 0.5069\n",
      "Epoch 57, train_loss: 0.68574 val_loss 0.69548  train_accuracy 0.5478 val_accuracy 0.5199  train_f1 0.5324 val_f1 0.5075\n",
      "Epoch 58, train_loss: 0.68573 val_loss 0.69549  train_accuracy 0.5478 val_accuracy 0.5202  train_f1 0.5324 val_f1 0.5079\n",
      "Epoch 59, train_loss: 0.68572 val_loss 0.69547  train_accuracy 0.5478 val_accuracy 0.5200  train_f1 0.5327 val_f1 0.5079\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-060-0.6955.hdf5\n",
      "Epoch 60, train_loss: 0.68572 val_loss 0.69547  train_accuracy 0.5478 val_accuracy 0.5198  train_f1 0.5338 val_f1 0.5096\n",
      "Epoch 61, train_loss: 0.68571 val_loss 0.69548  train_accuracy 0.5478 val_accuracy 0.5201  train_f1 0.5338 val_f1 0.5081\n",
      "Epoch 62, train_loss: 0.68570 val_loss 0.69548  train_accuracy 0.5479 val_accuracy 0.5199  train_f1 0.5323 val_f1 0.5081\n",
      "Epoch 63, train_loss: 0.68569 val_loss 0.69548  train_accuracy 0.5479 val_accuracy 0.5204  train_f1 0.5338 val_f1 0.5087\n",
      "Epoch 64, train_loss: 0.68568 val_loss 0.69548  train_accuracy 0.5479 val_accuracy 0.5204  train_f1 0.5323 val_f1 0.5084\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-065-0.6955.hdf5\n",
      "Epoch 65, train_loss: 0.68567 val_loss 0.69548  train_accuracy 0.5480 val_accuracy 0.5205  train_f1 0.5340 val_f1 0.5086\n",
      "Epoch 66, train_loss: 0.68566 val_loss 0.69548  train_accuracy 0.5480 val_accuracy 0.5203  train_f1 0.5324 val_f1 0.5093\n",
      "Epoch 67, train_loss: 0.68565 val_loss 0.69547  train_accuracy 0.5479 val_accuracy 0.5197  train_f1 0.5356 val_f1 0.5070\n",
      "Epoch 68, train_loss: 0.68564 val_loss 0.69548  train_accuracy 0.5481 val_accuracy 0.5201  train_f1 0.5298 val_f1 0.5073\n",
      "Epoch 69, train_loss: 0.68563 val_loss 0.69549  train_accuracy 0.5480 val_accuracy 0.5201  train_f1 0.5349 val_f1 0.5101\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-070-0.6955.hdf5\n",
      "Epoch 70, train_loss: 0.68562 val_loss 0.69549  train_accuracy 0.5481 val_accuracy 0.5202  train_f1 0.5327 val_f1 0.5061\n",
      "Epoch 71, train_loss: 0.68561 val_loss 0.69549  train_accuracy 0.5481 val_accuracy 0.5202  train_f1 0.5329 val_f1 0.5100\n",
      "Epoch 72, train_loss: 0.68560 val_loss 0.69547  train_accuracy 0.5481 val_accuracy 0.5200  train_f1 0.5337 val_f1 0.5074\n",
      "Epoch 73, train_loss: 0.68559 val_loss 0.69549  train_accuracy 0.5482 val_accuracy 0.5204  train_f1 0.5332 val_f1 0.5099\n",
      "Epoch 74, train_loss: 0.68558 val_loss 0.69549  train_accuracy 0.5482 val_accuracy 0.5203  train_f1 0.5346 val_f1 0.5086\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-075-0.6955.hdf5\n",
      "Epoch 75, train_loss: 0.68557 val_loss 0.69548  train_accuracy 0.5483 val_accuracy 0.5204  train_f1 0.5328 val_f1 0.5090\n",
      "Epoch 76, train_loss: 0.68556 val_loss 0.69548  train_accuracy 0.5482 val_accuracy 0.5205  train_f1 0.5360 val_f1 0.5087\n",
      "Epoch 77, train_loss: 0.68555 val_loss 0.69549  train_accuracy 0.5484 val_accuracy 0.5200  train_f1 0.5315 val_f1 0.5074\n",
      "Epoch 78, train_loss: 0.68554 val_loss 0.69549  train_accuracy 0.5483 val_accuracy 0.5202  train_f1 0.5348 val_f1 0.5079\n",
      "Epoch 79, train_loss: 0.68553 val_loss 0.69547  train_accuracy 0.5484 val_accuracy 0.5201  train_f1 0.5311 val_f1 0.5077\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-080-0.6955.hdf5\n",
      "Epoch 80, train_loss: 0.68552 val_loss 0.69547  train_accuracy 0.5484 val_accuracy 0.5200  train_f1 0.5334 val_f1 0.5089\n",
      "Epoch 81, train_loss: 0.68551 val_loss 0.69548  train_accuracy 0.5485 val_accuracy 0.5202  train_f1 0.5326 val_f1 0.5087\n",
      "Epoch 82, train_loss: 0.68550 val_loss 0.69548  train_accuracy 0.5484 val_accuracy 0.5202  train_f1 0.5353 val_f1 0.5082\n",
      "Epoch 83, train_loss: 0.68549 val_loss 0.69548  train_accuracy 0.5485 val_accuracy 0.5204  train_f1 0.5329 val_f1 0.5084\n",
      "Epoch 84, train_loss: 0.68548 val_loss 0.69548  train_accuracy 0.5485 val_accuracy 0.5203  train_f1 0.5329 val_f1 0.5083\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-085-0.6955.hdf5\n",
      "Epoch 85, train_loss: 0.68547 val_loss 0.69549  train_accuracy 0.5486 val_accuracy 0.5201  train_f1 0.5327 val_f1 0.5089\n",
      "Epoch 86, train_loss: 0.68546 val_loss 0.69547  train_accuracy 0.5485 val_accuracy 0.5202  train_f1 0.5357 val_f1 0.5065\n",
      "Epoch 87, train_loss: 0.68545 val_loss 0.69548  train_accuracy 0.5486 val_accuracy 0.5204  train_f1 0.5314 val_f1 0.5090\n",
      "Epoch 88, train_loss: 0.68544 val_loss 0.69548  train_accuracy 0.5487 val_accuracy 0.5204  train_f1 0.5338 val_f1 0.5086\n",
      "Epoch 89, train_loss: 0.68543 val_loss 0.69549  train_accuracy 0.5487 val_accuracy 0.5204  train_f1 0.5331 val_f1 0.5083\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-090-0.6955.hdf5\n",
      "Epoch 90, train_loss: 0.68542 val_loss 0.69547  train_accuracy 0.5488 val_accuracy 0.5204  train_f1 0.5350 val_f1 0.5072\n",
      "Epoch 91, train_loss: 0.68541 val_loss 0.69546  train_accuracy 0.5488 val_accuracy 0.5207  train_f1 0.5317 val_f1 0.5104\n",
      "Epoch 92, train_loss: 0.68540 val_loss 0.69547  train_accuracy 0.5487 val_accuracy 0.5204  train_f1 0.5371 val_f1 0.5079\n",
      "Epoch 93, train_loss: 0.68539 val_loss 0.69548  train_accuracy 0.5489 val_accuracy 0.5207  train_f1 0.5310 val_f1 0.5095\n",
      "Epoch 94, train_loss: 0.68537 val_loss 0.69546  train_accuracy 0.5488 val_accuracy 0.5206  train_f1 0.5347 val_f1 0.5089\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-095-0.6955.hdf5\n",
      "Epoch 95, train_loss: 0.68536 val_loss 0.69546  train_accuracy 0.5487 val_accuracy 0.5203  train_f1 0.5348 val_f1 0.5087\n",
      "Epoch 96, train_loss: 0.68535 val_loss 0.69547  train_accuracy 0.5489 val_accuracy 0.5202  train_f1 0.5335 val_f1 0.5088\n",
      "Epoch 97, train_loss: 0.68534 val_loss 0.69547  train_accuracy 0.5489 val_accuracy 0.5206  train_f1 0.5325 val_f1 0.5105\n",
      "Epoch 98, train_loss: 0.68533 val_loss 0.69544  train_accuracy 0.5489 val_accuracy 0.5206  train_f1 0.5370 val_f1 0.5079\n",
      "Epoch 99, train_loss: 0.68532 val_loss 0.69547  train_accuracy 0.5490 val_accuracy 0.5207  train_f1 0.5326 val_f1 0.5095\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-100-0.6955.hdf5\n",
      "Epoch 100, train_loss: 0.68531 val_loss 0.69548  train_accuracy 0.5490 val_accuracy 0.5209  train_f1 0.5348 val_f1 0.5095\n",
      "Epoch 101, train_loss: 0.68530 val_loss 0.69548  train_accuracy 0.5490 val_accuracy 0.5208  train_f1 0.5339 val_f1 0.5103\n",
      "Epoch 102, train_loss: 0.68529 val_loss 0.69545  train_accuracy 0.5490 val_accuracy 0.5205  train_f1 0.5361 val_f1 0.5080\n",
      "Epoch 103, train_loss: 0.68528 val_loss 0.69547  train_accuracy 0.5491 val_accuracy 0.5204  train_f1 0.5323 val_f1 0.5073\n",
      "Epoch 104, train_loss: 0.68526 val_loss 0.69548  train_accuracy 0.5491 val_accuracy 0.5201  train_f1 0.5326 val_f1 0.5099\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-105-0.6955.hdf5\n",
      "Epoch 105, train_loss: 0.68525 val_loss 0.69546  train_accuracy 0.5491 val_accuracy 0.5199  train_f1 0.5350 val_f1 0.5088\n",
      "Epoch 106, train_loss: 0.68524 val_loss 0.69543  train_accuracy 0.5492 val_accuracy 0.5201  train_f1 0.5327 val_f1 0.5101\n",
      "Epoch 107, train_loss: 0.68523 val_loss 0.69545  train_accuracy 0.5492 val_accuracy 0.5207  train_f1 0.5359 val_f1 0.5090\n",
      "Epoch 108, train_loss: 0.68522 val_loss 0.69546  train_accuracy 0.5493 val_accuracy 0.5207  train_f1 0.5339 val_f1 0.5097\n",
      "Epoch 109, train_loss: 0.68521 val_loss 0.69546  train_accuracy 0.5494 val_accuracy 0.5204  train_f1 0.5326 val_f1 0.5103\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-110-0.6955.hdf5\n",
      "Epoch 110, train_loss: 0.68520 val_loss 0.69545  train_accuracy 0.5493 val_accuracy 0.5206  train_f1 0.5381 val_f1 0.5062\n",
      "Epoch 111, train_loss: 0.68519 val_loss 0.69545  train_accuracy 0.5494 val_accuracy 0.5205  train_f1 0.5310 val_f1 0.5105\n",
      "Epoch 112, train_loss: 0.68517 val_loss 0.69545  train_accuracy 0.5494 val_accuracy 0.5203  train_f1 0.5372 val_f1 0.5070\n",
      "Epoch 113, train_loss: 0.68517 val_loss 0.69548  train_accuracy 0.5495 val_accuracy 0.5204  train_f1 0.5302 val_f1 0.5120\n",
      "Epoch 114, train_loss: 0.68515 val_loss 0.69543  train_accuracy 0.5494 val_accuracy 0.5206  train_f1 0.5386 val_f1 0.5051\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-115-0.6954.hdf5\n",
      "Epoch 115, train_loss: 0.68514 val_loss 0.69546  train_accuracy 0.5495 val_accuracy 0.5203  train_f1 0.5316 val_f1 0.5100\n",
      "Epoch 116, train_loss: 0.68513 val_loss 0.69546  train_accuracy 0.5495 val_accuracy 0.5206  train_f1 0.5342 val_f1 0.5105\n",
      "Epoch 117, train_loss: 0.68511 val_loss 0.69544  train_accuracy 0.5496 val_accuracy 0.5211  train_f1 0.5370 val_f1 0.5082\n",
      "Epoch 118, train_loss: 0.68510 val_loss 0.69547  train_accuracy 0.5497 val_accuracy 0.5203  train_f1 0.5342 val_f1 0.5099\n",
      "Epoch 119, train_loss: 0.68509 val_loss 0.69544  train_accuracy 0.5497 val_accuracy 0.5203  train_f1 0.5343 val_f1 0.5099\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-120-0.6954.hdf5\n",
      "Epoch 120, train_loss: 0.68508 val_loss 0.69543  train_accuracy 0.5497 val_accuracy 0.5204  train_f1 0.5378 val_f1 0.5077\n",
      "Epoch 121, train_loss: 0.68507 val_loss 0.69549  train_accuracy 0.5498 val_accuracy 0.5200  train_f1 0.5321 val_f1 0.5125\n",
      "Epoch 122, train_loss: 0.68506 val_loss 0.69546  train_accuracy 0.5498 val_accuracy 0.5210  train_f1 0.5392 val_f1 0.5051\n",
      "Epoch 123, train_loss: 0.68505 val_loss 0.69549  train_accuracy 0.5498 val_accuracy 0.5204  train_f1 0.5316 val_f1 0.5144\n",
      "Epoch 124, train_loss: 0.68504 val_loss 0.69546  train_accuracy 0.5497 val_accuracy 0.5209  train_f1 0.5380 val_f1 0.5044\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-125-0.6955.hdf5\n",
      "Epoch 125, train_loss: 0.68502 val_loss 0.69550  train_accuracy 0.5499 val_accuracy 0.5203  train_f1 0.5334 val_f1 0.5122\n",
      "Epoch 126, train_loss: 0.68500 val_loss 0.69548  train_accuracy 0.5500 val_accuracy 0.5207  train_f1 0.5357 val_f1 0.5069\n",
      "Epoch 127, train_loss: 0.68500 val_loss 0.69547  train_accuracy 0.5500 val_accuracy 0.5201  train_f1 0.5350 val_f1 0.5109\n",
      "Epoch 128, train_loss: 0.68498 val_loss 0.69546  train_accuracy 0.5501 val_accuracy 0.5204  train_f1 0.5343 val_f1 0.5084\n",
      "Epoch 129, train_loss: 0.68497 val_loss 0.69546  train_accuracy 0.5501 val_accuracy 0.5205  train_f1 0.5357 val_f1 0.5117\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-130-0.6955.hdf5\n",
      "Epoch 130, train_loss: 0.68495 val_loss 0.69546  train_accuracy 0.5502 val_accuracy 0.5205  train_f1 0.5371 val_f1 0.5071\n",
      "Epoch 131, train_loss: 0.68494 val_loss 0.69547  train_accuracy 0.5501 val_accuracy 0.5205  train_f1 0.5348 val_f1 0.5096\n",
      "Epoch 132, train_loss: 0.68493 val_loss 0.69545  train_accuracy 0.5502 val_accuracy 0.5206  train_f1 0.5352 val_f1 0.5095\n",
      "Epoch 133, train_loss: 0.68492 val_loss 0.69545  train_accuracy 0.5502 val_accuracy 0.5206  train_f1 0.5371 val_f1 0.5109\n",
      "Epoch 134, train_loss: 0.68491 val_loss 0.69546  train_accuracy 0.5503 val_accuracy 0.5208  train_f1 0.5365 val_f1 0.5085\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-135-0.6955.hdf5\n",
      "Epoch 135, train_loss: 0.68489 val_loss 0.69548  train_accuracy 0.5503 val_accuracy 0.5204  train_f1 0.5364 val_f1 0.5085\n",
      "Epoch 136, train_loss: 0.68488 val_loss 0.69547  train_accuracy 0.5503 val_accuracy 0.5207  train_f1 0.5349 val_f1 0.5092\n",
      "Epoch 137, train_loss: 0.68487 val_loss 0.69547  train_accuracy 0.5503 val_accuracy 0.5212  train_f1 0.5355 val_f1 0.5094\n",
      "Epoch 138, train_loss: 0.68485 val_loss 0.69546  train_accuracy 0.5503 val_accuracy 0.5209  train_f1 0.5356 val_f1 0.5093\n",
      "Epoch 139, train_loss: 0.68484 val_loss 0.69546  train_accuracy 0.5504 val_accuracy 0.5209  train_f1 0.5346 val_f1 0.5127\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-140-0.6955.hdf5\n",
      "Epoch 140, train_loss: 0.68483 val_loss 0.69543  train_accuracy 0.5504 val_accuracy 0.5212  train_f1 0.5371 val_f1 0.5066\n",
      "Epoch 141, train_loss: 0.68482 val_loss 0.69545  train_accuracy 0.5504 val_accuracy 0.5208  train_f1 0.5364 val_f1 0.5092\n",
      "Epoch 142, train_loss: 0.68480 val_loss 0.69545  train_accuracy 0.5504 val_accuracy 0.5205  train_f1 0.5336 val_f1 0.5120\n",
      "Epoch 143, train_loss: 0.68479 val_loss 0.69545  train_accuracy 0.5505 val_accuracy 0.5214  train_f1 0.5386 val_f1 0.5075\n",
      "Epoch 144, train_loss: 0.68478 val_loss 0.69547  train_accuracy 0.5506 val_accuracy 0.5207  train_f1 0.5343 val_f1 0.5120\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-145-0.6955.hdf5\n",
      "Epoch 145, train_loss: 0.68476 val_loss 0.69545  train_accuracy 0.5507 val_accuracy 0.5207  train_f1 0.5373 val_f1 0.5060\n",
      "Epoch 146, train_loss: 0.68475 val_loss 0.69549  train_accuracy 0.5506 val_accuracy 0.5208  train_f1 0.5348 val_f1 0.5127\n",
      "Epoch 147, train_loss: 0.68474 val_loss 0.69546  train_accuracy 0.5506 val_accuracy 0.5210  train_f1 0.5375 val_f1 0.5061\n",
      "Epoch 148, train_loss: 0.68473 val_loss 0.69547  train_accuracy 0.5507 val_accuracy 0.5208  train_f1 0.5354 val_f1 0.5096\n",
      "Epoch 149, train_loss: 0.68471 val_loss 0.69546  train_accuracy 0.5507 val_accuracy 0.5208  train_f1 0.5336 val_f1 0.5110\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-150-0.6955.hdf5\n",
      "Epoch 150, train_loss: 0.68470 val_loss 0.69545  train_accuracy 0.5508 val_accuracy 0.5211  train_f1 0.5371 val_f1 0.5090\n",
      "Epoch 151, train_loss: 0.68469 val_loss 0.69544  train_accuracy 0.5508 val_accuracy 0.5212  train_f1 0.5364 val_f1 0.5085\n",
      "Epoch 152, train_loss: 0.68467 val_loss 0.69547  train_accuracy 0.5508 val_accuracy 0.5211  train_f1 0.5359 val_f1 0.5124\n",
      "Epoch 153, train_loss: 0.68466 val_loss 0.69545  train_accuracy 0.5509 val_accuracy 0.5211  train_f1 0.5374 val_f1 0.5099\n",
      "Epoch 154, train_loss: 0.68465 val_loss 0.69545  train_accuracy 0.5509 val_accuracy 0.5207  train_f1 0.5374 val_f1 0.5075\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-155-0.6954.hdf5\n",
      "Epoch 155, train_loss: 0.68463 val_loss 0.69547  train_accuracy 0.5509 val_accuracy 0.5209  train_f1 0.5353 val_f1 0.5119\n",
      "Epoch 156, train_loss: 0.68462 val_loss 0.69543  train_accuracy 0.5510 val_accuracy 0.5207  train_f1 0.5382 val_f1 0.5052\n",
      "Epoch 157, train_loss: 0.68461 val_loss 0.69545  train_accuracy 0.5511 val_accuracy 0.5210  train_f1 0.5356 val_f1 0.5105\n",
      "Epoch 158, train_loss: 0.68459 val_loss 0.69546  train_accuracy 0.5510 val_accuracy 0.5210  train_f1 0.5349 val_f1 0.5125\n",
      "Epoch 159, train_loss: 0.68458 val_loss 0.69544  train_accuracy 0.5511 val_accuracy 0.5210  train_f1 0.5397 val_f1 0.5081\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-160-0.6954.hdf5\n",
      "Epoch 160, train_loss: 0.68457 val_loss 0.69544  train_accuracy 0.5511 val_accuracy 0.5210  train_f1 0.5373 val_f1 0.5066\n",
      "Epoch 161, train_loss: 0.68455 val_loss 0.69547  train_accuracy 0.5511 val_accuracy 0.5204  train_f1 0.5329 val_f1 0.5119\n",
      "Epoch 162, train_loss: 0.68454 val_loss 0.69543  train_accuracy 0.5512 val_accuracy 0.5210  train_f1 0.5365 val_f1 0.5092\n",
      "Epoch 163, train_loss: 0.68452 val_loss 0.69546  train_accuracy 0.5512 val_accuracy 0.5210  train_f1 0.5371 val_f1 0.5109\n",
      "Epoch 164, train_loss: 0.68451 val_loss 0.69544  train_accuracy 0.5513 val_accuracy 0.5212  train_f1 0.5373 val_f1 0.5085\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-165-0.6954.hdf5\n",
      "Epoch 165, train_loss: 0.68450 val_loss 0.69547  train_accuracy 0.5512 val_accuracy 0.5208  train_f1 0.5352 val_f1 0.5117\n",
      "Epoch 166, train_loss: 0.68448 val_loss 0.69544  train_accuracy 0.5513 val_accuracy 0.5210  train_f1 0.5358 val_f1 0.5094\n",
      "Epoch 167, train_loss: 0.68447 val_loss 0.69544  train_accuracy 0.5514 val_accuracy 0.5215  train_f1 0.5375 val_f1 0.5106\n",
      "Epoch 168, train_loss: 0.68445 val_loss 0.69545  train_accuracy 0.5514 val_accuracy 0.5212  train_f1 0.5374 val_f1 0.5103\n",
      "Epoch 169, train_loss: 0.68444 val_loss 0.69546  train_accuracy 0.5514 val_accuracy 0.5209  train_f1 0.5357 val_f1 0.5102\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-170-0.6955.hdf5\n",
      "Epoch 170, train_loss: 0.68442 val_loss 0.69547  train_accuracy 0.5515 val_accuracy 0.5205  train_f1 0.5391 val_f1 0.5074\n",
      "Epoch 171, train_loss: 0.68441 val_loss 0.69546  train_accuracy 0.5514 val_accuracy 0.5206  train_f1 0.5374 val_f1 0.5071\n",
      "Epoch 172, train_loss: 0.68440 val_loss 0.69548  train_accuracy 0.5515 val_accuracy 0.5207  train_f1 0.5339 val_f1 0.5142\n",
      "Epoch 173, train_loss: 0.68438 val_loss 0.69546  train_accuracy 0.5515 val_accuracy 0.5213  train_f1 0.5376 val_f1 0.5084\n",
      "Epoch 174, train_loss: 0.68437 val_loss 0.69546  train_accuracy 0.5516 val_accuracy 0.5210  train_f1 0.5374 val_f1 0.5128\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-175-0.6955.hdf5\n",
      "Epoch 175, train_loss: 0.68435 val_loss 0.69547  train_accuracy 0.5517 val_accuracy 0.5209  train_f1 0.5367 val_f1 0.5119\n",
      "Epoch 176, train_loss: 0.68434 val_loss 0.69545  train_accuracy 0.5517 val_accuracy 0.5210  train_f1 0.5389 val_f1 0.5072\n",
      "Epoch 177, train_loss: 0.68433 val_loss 0.69544  train_accuracy 0.5518 val_accuracy 0.5212  train_f1 0.5378 val_f1 0.5081\n",
      "Epoch 178, train_loss: 0.68431 val_loss 0.69545  train_accuracy 0.5518 val_accuracy 0.5213  train_f1 0.5359 val_f1 0.5133\n",
      "Epoch 179, train_loss: 0.68430 val_loss 0.69545  train_accuracy 0.5519 val_accuracy 0.5216  train_f1 0.5375 val_f1 0.5104\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-180-0.6955.hdf5\n",
      "Epoch 180, train_loss: 0.68428 val_loss 0.69547  train_accuracy 0.5518 val_accuracy 0.5216  train_f1 0.5373 val_f1 0.5116\n",
      "Epoch 181, train_loss: 0.68427 val_loss 0.69545  train_accuracy 0.5519 val_accuracy 0.5212  train_f1 0.5373 val_f1 0.5098\n",
      "Epoch 182, train_loss: 0.68425 val_loss 0.69546  train_accuracy 0.5519 val_accuracy 0.5214  train_f1 0.5367 val_f1 0.5107\n",
      "Epoch 183, train_loss: 0.68424 val_loss 0.69545  train_accuracy 0.5520 val_accuracy 0.5212  train_f1 0.5371 val_f1 0.5136\n",
      "Epoch 184, train_loss: 0.68423 val_loss 0.69545  train_accuracy 0.5521 val_accuracy 0.5211  train_f1 0.5395 val_f1 0.5080\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-185-0.6955.hdf5\n",
      "Epoch 185, train_loss: 0.68421 val_loss 0.69546  train_accuracy 0.5520 val_accuracy 0.5211  train_f1 0.5397 val_f1 0.5086\n",
      "Epoch 186, train_loss: 0.68420 val_loss 0.69545  train_accuracy 0.5521 val_accuracy 0.5211  train_f1 0.5382 val_f1 0.5064\n",
      "Epoch 187, train_loss: 0.68418 val_loss 0.69549  train_accuracy 0.5521 val_accuracy 0.5208  train_f1 0.5343 val_f1 0.5146\n",
      "Epoch 188, train_loss: 0.68417 val_loss 0.69544  train_accuracy 0.5521 val_accuracy 0.5212  train_f1 0.5390 val_f1 0.5063\n",
      "Epoch 189, train_loss: 0.68415 val_loss 0.69546  train_accuracy 0.5522 val_accuracy 0.5215  train_f1 0.5386 val_f1 0.5122\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-190-0.6955.hdf5\n",
      "Epoch 190, train_loss: 0.68413 val_loss 0.69548  train_accuracy 0.5522 val_accuracy 0.5215  train_f1 0.5367 val_f1 0.5122\n",
      "Epoch 191, train_loss: 0.68412 val_loss 0.69547  train_accuracy 0.5524 val_accuracy 0.5211  train_f1 0.5373 val_f1 0.5099\n",
      "Epoch 192, train_loss: 0.68410 val_loss 0.69545  train_accuracy 0.5524 val_accuracy 0.5213  train_f1 0.5387 val_f1 0.5099\n",
      "Epoch 193, train_loss: 0.68409 val_loss 0.69546  train_accuracy 0.5524 val_accuracy 0.5213  train_f1 0.5380 val_f1 0.5121\n",
      "Epoch 194, train_loss: 0.68407 val_loss 0.69547  train_accuracy 0.5525 val_accuracy 0.5212  train_f1 0.5374 val_f1 0.5093\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-195-0.6955.hdf5\n",
      "Epoch 195, train_loss: 0.68406 val_loss 0.69548  train_accuracy 0.5524 val_accuracy 0.5211  train_f1 0.5389 val_f1 0.5090\n",
      "Epoch 196, train_loss: 0.68404 val_loss 0.69547  train_accuracy 0.5525 val_accuracy 0.5209  train_f1 0.5371 val_f1 0.5108\n",
      "Epoch 197, train_loss: 0.68403 val_loss 0.69551  train_accuracy 0.5526 val_accuracy 0.5210  train_f1 0.5359 val_f1 0.5153\n",
      "Epoch 198, train_loss: 0.68402 val_loss 0.69548  train_accuracy 0.5527 val_accuracy 0.5211  train_f1 0.5402 val_f1 0.5049\n",
      "Epoch 199, train_loss: 0.68400 val_loss 0.69550  train_accuracy 0.5525 val_accuracy 0.5210  train_f1 0.5374 val_f1 0.5111\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-200-0.6955.hdf5\n",
      "Epoch 200, train_loss: 0.68399 val_loss 0.69551  train_accuracy 0.5526 val_accuracy 0.5210  train_f1 0.5361 val_f1 0.5124\n",
      "Epoch 201, train_loss: 0.68397 val_loss 0.69548  train_accuracy 0.5527 val_accuracy 0.5208  train_f1 0.5379 val_f1 0.5113\n",
      "Epoch 202, train_loss: 0.68395 val_loss 0.69548  train_accuracy 0.5528 val_accuracy 0.5210  train_f1 0.5393 val_f1 0.5094\n",
      "Epoch 203, train_loss: 0.68393 val_loss 0.69549  train_accuracy 0.5528 val_accuracy 0.5212  train_f1 0.5393 val_f1 0.5090\n",
      "Epoch 204, train_loss: 0.68392 val_loss 0.69549  train_accuracy 0.5529 val_accuracy 0.5215  train_f1 0.5379 val_f1 0.5091\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-205-0.6955.hdf5\n",
      "Epoch 205, train_loss: 0.68391 val_loss 0.69552  train_accuracy 0.5528 val_accuracy 0.5212  train_f1 0.5364 val_f1 0.5151\n",
      "Epoch 206, train_loss: 0.68389 val_loss 0.69547  train_accuracy 0.5529 val_accuracy 0.5213  train_f1 0.5389 val_f1 0.5112\n",
      "Epoch 207, train_loss: 0.68387 val_loss 0.69548  train_accuracy 0.5530 val_accuracy 0.5208  train_f1 0.5398 val_f1 0.5093\n",
      "Epoch 208, train_loss: 0.68386 val_loss 0.69550  train_accuracy 0.5531 val_accuracy 0.5210  train_f1 0.5374 val_f1 0.5110\n",
      "Epoch 209, train_loss: 0.68384 val_loss 0.69549  train_accuracy 0.5531 val_accuracy 0.5210  train_f1 0.5386 val_f1 0.5117\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-210-0.6955.hdf5\n",
      "Epoch 210, train_loss: 0.68383 val_loss 0.69550  train_accuracy 0.5533 val_accuracy 0.5211  train_f1 0.5402 val_f1 0.5075\n",
      "Epoch 211, train_loss: 0.68381 val_loss 0.69550  train_accuracy 0.5533 val_accuracy 0.5216  train_f1 0.5407 val_f1 0.5054\n",
      "Epoch 212, train_loss: 0.68380 val_loss 0.69553  train_accuracy 0.5532 val_accuracy 0.5211  train_f1 0.5377 val_f1 0.5117\n",
      "Epoch 213, train_loss: 0.68378 val_loss 0.69551  train_accuracy 0.5531 val_accuracy 0.5213  train_f1 0.5364 val_f1 0.5128\n",
      "Epoch 214, train_loss: 0.68376 val_loss 0.69552  train_accuracy 0.5532 val_accuracy 0.5207  train_f1 0.5380 val_f1 0.5131\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-215-0.6955.hdf5\n",
      "Epoch 215, train_loss: 0.68375 val_loss 0.69551  train_accuracy 0.5534 val_accuracy 0.5209  train_f1 0.5402 val_f1 0.5096\n",
      "Epoch 216, train_loss: 0.68373 val_loss 0.69549  train_accuracy 0.5535 val_accuracy 0.5214  train_f1 0.5398 val_f1 0.5090\n",
      "Epoch 217, train_loss: 0.68372 val_loss 0.69551  train_accuracy 0.5535 val_accuracy 0.5210  train_f1 0.5370 val_f1 0.5141\n",
      "Epoch 218, train_loss: 0.68370 val_loss 0.69553  train_accuracy 0.5535 val_accuracy 0.5209  train_f1 0.5391 val_f1 0.5087\n",
      "Epoch 219, train_loss: 0.68368 val_loss 0.69551  train_accuracy 0.5535 val_accuracy 0.5206  train_f1 0.5393 val_f1 0.5096\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-220-0.6955.hdf5\n",
      "Epoch 220, train_loss: 0.68367 val_loss 0.69552  train_accuracy 0.5536 val_accuracy 0.5212  train_f1 0.5399 val_f1 0.5119\n",
      "Epoch 221, train_loss: 0.68365 val_loss 0.69549  train_accuracy 0.5536 val_accuracy 0.5208  train_f1 0.5390 val_f1 0.5085\n",
      "Epoch 222, train_loss: 0.68363 val_loss 0.69551  train_accuracy 0.5537 val_accuracy 0.5213  train_f1 0.5384 val_f1 0.5142\n",
      "Epoch 223, train_loss: 0.68362 val_loss 0.69550  train_accuracy 0.5538 val_accuracy 0.5209  train_f1 0.5395 val_f1 0.5069\n",
      "Epoch 224, train_loss: 0.68360 val_loss 0.69552  train_accuracy 0.5538 val_accuracy 0.5211  train_f1 0.5395 val_f1 0.5069\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-225-0.6955.hdf5\n",
      "Epoch 225, train_loss: 0.68359 val_loss 0.69550  train_accuracy 0.5537 val_accuracy 0.5215  train_f1 0.5377 val_f1 0.5122\n",
      "Epoch 226, train_loss: 0.68357 val_loss 0.69552  train_accuracy 0.5538 val_accuracy 0.5210  train_f1 0.5386 val_f1 0.5086\n",
      "Epoch 227, train_loss: 0.68355 val_loss 0.69551  train_accuracy 0.5539 val_accuracy 0.5216  train_f1 0.5389 val_f1 0.5113\n",
      "Epoch 228, train_loss: 0.68354 val_loss 0.69550  train_accuracy 0.5540 val_accuracy 0.5214  train_f1 0.5411 val_f1 0.5035\n",
      "Epoch 229, train_loss: 0.68353 val_loss 0.69549  train_accuracy 0.5541 val_accuracy 0.5218  train_f1 0.5406 val_f1 0.5123\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-230-0.6955.hdf5\n",
      "Epoch 230, train_loss: 0.68350 val_loss 0.69554  train_accuracy 0.5540 val_accuracy 0.5214  train_f1 0.5388 val_f1 0.5123\n",
      "Epoch 231, train_loss: 0.68349 val_loss 0.69552  train_accuracy 0.5541 val_accuracy 0.5212  train_f1 0.5406 val_f1 0.5080\n",
      "Epoch 232, train_loss: 0.68347 val_loss 0.69551  train_accuracy 0.5541 val_accuracy 0.5216  train_f1 0.5406 val_f1 0.5071\n",
      "Epoch 233, train_loss: 0.68346 val_loss 0.69552  train_accuracy 0.5541 val_accuracy 0.5210  train_f1 0.5386 val_f1 0.5130\n",
      "Epoch 234, train_loss: 0.68344 val_loss 0.69554  train_accuracy 0.5542 val_accuracy 0.5217  train_f1 0.5396 val_f1 0.5119\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-235-0.6955.hdf5\n",
      "Epoch 235, train_loss: 0.68343 val_loss 0.69555  train_accuracy 0.5543 val_accuracy 0.5217  train_f1 0.5385 val_f1 0.5148\n",
      "Epoch 236, train_loss: 0.68340 val_loss 0.69557  train_accuracy 0.5543 val_accuracy 0.5219  train_f1 0.5383 val_f1 0.5153\n",
      "Epoch 237, train_loss: 0.68339 val_loss 0.69552  train_accuracy 0.5543 val_accuracy 0.5211  train_f1 0.5432 val_f1 0.5021\n",
      "Epoch 238, train_loss: 0.68338 val_loss 0.69554  train_accuracy 0.5544 val_accuracy 0.5214  train_f1 0.5406 val_f1 0.5117\n",
      "Epoch 239, train_loss: 0.68335 val_loss 0.69556  train_accuracy 0.5545 val_accuracy 0.5213  train_f1 0.5375 val_f1 0.5152\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-240-0.6956.hdf5\n",
      "Epoch 240, train_loss: 0.68334 val_loss 0.69556  train_accuracy 0.5546 val_accuracy 0.5214  train_f1 0.5399 val_f1 0.5149\n",
      "Epoch 241, train_loss: 0.68332 val_loss 0.69553  train_accuracy 0.5545 val_accuracy 0.5217  train_f1 0.5410 val_f1 0.5111\n",
      "Epoch 242, train_loss: 0.68330 val_loss 0.69553  train_accuracy 0.5546 val_accuracy 0.5215  train_f1 0.5401 val_f1 0.5123\n",
      "Epoch 243, train_loss: 0.68329 val_loss 0.69551  train_accuracy 0.5546 val_accuracy 0.5213  train_f1 0.5409 val_f1 0.5140\n",
      "Epoch 244, train_loss: 0.68327 val_loss 0.69555  train_accuracy 0.5546 val_accuracy 0.5220  train_f1 0.5425 val_f1 0.5063\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-245-0.6956.hdf5\n",
      "Epoch 245, train_loss: 0.68325 val_loss 0.69551  train_accuracy 0.5547 val_accuracy 0.5216  train_f1 0.5406 val_f1 0.5129\n",
      "Epoch 246, train_loss: 0.68323 val_loss 0.69552  train_accuracy 0.5547 val_accuracy 0.5216  train_f1 0.5410 val_f1 0.5108\n",
      "Epoch 247, train_loss: 0.68322 val_loss 0.69557  train_accuracy 0.5547 val_accuracy 0.5219  train_f1 0.5394 val_f1 0.5136\n",
      "Epoch 248, train_loss: 0.68320 val_loss 0.69553  train_accuracy 0.5549 val_accuracy 0.5222  train_f1 0.5403 val_f1 0.5097\n",
      "Epoch 249, train_loss: 0.68319 val_loss 0.69553  train_accuracy 0.5549 val_accuracy 0.5222  train_f1 0.5404 val_f1 0.5099\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-250-0.6955.hdf5\n",
      "Epoch 250, train_loss: 0.68317 val_loss 0.69556  train_accuracy 0.5549 val_accuracy 0.5225  train_f1 0.5380 val_f1 0.5130\n",
      "Epoch 251, train_loss: 0.68315 val_loss 0.69557  train_accuracy 0.5550 val_accuracy 0.5220  train_f1 0.5399 val_f1 0.5179\n",
      "Epoch 252, train_loss: 0.68315 val_loss 0.69558  train_accuracy 0.5550 val_accuracy 0.5221  train_f1 0.5411 val_f1 0.5159\n",
      "Epoch 253, train_loss: 0.68313 val_loss 0.69554  train_accuracy 0.5552 val_accuracy 0.5211  train_f1 0.5447 val_f1 0.5015\n",
      "Epoch 254, train_loss: 0.68311 val_loss 0.69554  train_accuracy 0.5552 val_accuracy 0.5216  train_f1 0.5427 val_f1 0.5018\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-255-0.6955.hdf5\n",
      "Epoch 255, train_loss: 0.68309 val_loss 0.69554  train_accuracy 0.5552 val_accuracy 0.5220  train_f1 0.5385 val_f1 0.5128\n",
      "Epoch 256, train_loss: 0.68306 val_loss 0.69555  train_accuracy 0.5553 val_accuracy 0.5218  train_f1 0.5403 val_f1 0.5126\n",
      "Epoch 257, train_loss: 0.68304 val_loss 0.69556  train_accuracy 0.5554 val_accuracy 0.5221  train_f1 0.5416 val_f1 0.5111\n",
      "Epoch 258, train_loss: 0.68303 val_loss 0.69560  train_accuracy 0.5553 val_accuracy 0.5224  train_f1 0.5404 val_f1 0.5128\n",
      "Epoch 259, train_loss: 0.68301 val_loss 0.69559  train_accuracy 0.5554 val_accuracy 0.5214  train_f1 0.5384 val_f1 0.5173\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-260-0.6956.hdf5\n",
      "Epoch 260, train_loss: 0.68299 val_loss 0.69558  train_accuracy 0.5555 val_accuracy 0.5218  train_f1 0.5404 val_f1 0.5148\n",
      "Epoch 261, train_loss: 0.68297 val_loss 0.69558  train_accuracy 0.5556 val_accuracy 0.5220  train_f1 0.5426 val_f1 0.5102\n",
      "Epoch 262, train_loss: 0.68295 val_loss 0.69557  train_accuracy 0.5555 val_accuracy 0.5221  train_f1 0.5416 val_f1 0.5125\n",
      "Epoch 263, train_loss: 0.68294 val_loss 0.69557  train_accuracy 0.5557 val_accuracy 0.5218  train_f1 0.5416 val_f1 0.5119\n",
      "Epoch 264, train_loss: 0.68292 val_loss 0.69556  train_accuracy 0.5557 val_accuracy 0.5210  train_f1 0.5429 val_f1 0.5046\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-265-0.6956.hdf5\n",
      "Epoch 265, train_loss: 0.68290 val_loss 0.69557  train_accuracy 0.5557 val_accuracy 0.5219  train_f1 0.5414 val_f1 0.5106\n",
      "Epoch 266, train_loss: 0.68288 val_loss 0.69556  train_accuracy 0.5558 val_accuracy 0.5222  train_f1 0.5419 val_f1 0.5107\n",
      "Epoch 267, train_loss: 0.68286 val_loss 0.69558  train_accuracy 0.5559 val_accuracy 0.5217  train_f1 0.5418 val_f1 0.5092\n",
      "Epoch 268, train_loss: 0.68284 val_loss 0.69559  train_accuracy 0.5559 val_accuracy 0.5218  train_f1 0.5406 val_f1 0.5117\n",
      "Epoch 269, train_loss: 0.68282 val_loss 0.69557  train_accuracy 0.5561 val_accuracy 0.5217  train_f1 0.5418 val_f1 0.5099\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-270-0.6956.hdf5\n",
      "Epoch 270, train_loss: 0.68281 val_loss 0.69558  train_accuracy 0.5561 val_accuracy 0.5219  train_f1 0.5420 val_f1 0.5128\n",
      "Epoch 271, train_loss: 0.68279 val_loss 0.69554  train_accuracy 0.5561 val_accuracy 0.5220  train_f1 0.5428 val_f1 0.5108\n",
      "Epoch 272, train_loss: 0.68277 val_loss 0.69558  train_accuracy 0.5562 val_accuracy 0.5213  train_f1 0.5435 val_f1 0.5045\n",
      "Epoch 273, train_loss: 0.68277 val_loss 0.69565  train_accuracy 0.5561 val_accuracy 0.5216  train_f1 0.5397 val_f1 0.5117\n",
      "Epoch 274, train_loss: 0.68273 val_loss 0.69560  train_accuracy 0.5562 val_accuracy 0.5214  train_f1 0.5411 val_f1 0.5065\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-275-0.6956.hdf5\n",
      "Epoch 275, train_loss: 0.68272 val_loss 0.69563  train_accuracy 0.5562 val_accuracy 0.5220  train_f1 0.5398 val_f1 0.5150\n",
      "Epoch 276, train_loss: 0.68269 val_loss 0.69558  train_accuracy 0.5562 val_accuracy 0.5218  train_f1 0.5408 val_f1 0.5114\n",
      "Epoch 277, train_loss: 0.68268 val_loss 0.69560  train_accuracy 0.5564 val_accuracy 0.5213  train_f1 0.5432 val_f1 0.5102\n",
      "Epoch 278, train_loss: 0.68266 val_loss 0.69562  train_accuracy 0.5565 val_accuracy 0.5212  train_f1 0.5422 val_f1 0.5118\n",
      "Epoch 279, train_loss: 0.68264 val_loss 0.69561  train_accuracy 0.5565 val_accuracy 0.5212  train_f1 0.5420 val_f1 0.5104\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-280-0.6956.hdf5\n",
      "Epoch 280, train_loss: 0.68262 val_loss 0.69561  train_accuracy 0.5566 val_accuracy 0.5216  train_f1 0.5419 val_f1 0.5132\n",
      "Epoch 281, train_loss: 0.68260 val_loss 0.69560  train_accuracy 0.5566 val_accuracy 0.5220  train_f1 0.5420 val_f1 0.5136\n",
      "Epoch 282, train_loss: 0.68258 val_loss 0.69559  train_accuracy 0.5566 val_accuracy 0.5210  train_f1 0.5434 val_f1 0.5058\n",
      "Epoch 283, train_loss: 0.68256 val_loss 0.69561  train_accuracy 0.5566 val_accuracy 0.5217  train_f1 0.5423 val_f1 0.5122\n",
      "Epoch 284, train_loss: 0.68255 val_loss 0.69563  train_accuracy 0.5567 val_accuracy 0.5218  train_f1 0.5433 val_f1 0.5145\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-285-0.6956.hdf5\n",
      "Epoch 285, train_loss: 0.68254 val_loss 0.69558  train_accuracy 0.5567 val_accuracy 0.5216  train_f1 0.5447 val_f1 0.5078\n",
      "Epoch 286, train_loss: 0.68251 val_loss 0.69561  train_accuracy 0.5569 val_accuracy 0.5220  train_f1 0.5438 val_f1 0.5111\n",
      "Epoch 287, train_loss: 0.68249 val_loss 0.69564  train_accuracy 0.5569 val_accuracy 0.5220  train_f1 0.5432 val_f1 0.5095\n",
      "Epoch 288, train_loss: 0.68247 val_loss 0.69563  train_accuracy 0.5570 val_accuracy 0.5218  train_f1 0.5436 val_f1 0.5088\n",
      "Epoch 289, train_loss: 0.68245 val_loss 0.69561  train_accuracy 0.5570 val_accuracy 0.5213  train_f1 0.5437 val_f1 0.5110\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-290-0.6956.hdf5\n",
      "Epoch 290, train_loss: 0.68243 val_loss 0.69566  train_accuracy 0.5570 val_accuracy 0.5216  train_f1 0.5425 val_f1 0.5132\n",
      "Epoch 291, train_loss: 0.68241 val_loss 0.69562  train_accuracy 0.5571 val_accuracy 0.5209  train_f1 0.5439 val_f1 0.5075\n",
      "Epoch 292, train_loss: 0.68239 val_loss 0.69564  train_accuracy 0.5571 val_accuracy 0.5213  train_f1 0.5427 val_f1 0.5133\n",
      "Epoch 293, train_loss: 0.68237 val_loss 0.69560  train_accuracy 0.5573 val_accuracy 0.5214  train_f1 0.5452 val_f1 0.5076\n",
      "Epoch 294, train_loss: 0.68235 val_loss 0.69563  train_accuracy 0.5573 val_accuracy 0.5215  train_f1 0.5437 val_f1 0.5099\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-295-0.6956.hdf5\n",
      "Epoch 295, train_loss: 0.68234 val_loss 0.69566  train_accuracy 0.5573 val_accuracy 0.5218  train_f1 0.5428 val_f1 0.5075\n",
      "Epoch 296, train_loss: 0.68233 val_loss 0.69568  train_accuracy 0.5572 val_accuracy 0.5216  train_f1 0.5420 val_f1 0.5144\n",
      "Epoch 297, train_loss: 0.68230 val_loss 0.69574  train_accuracy 0.5574 val_accuracy 0.5218  train_f1 0.5419 val_f1 0.5228\n",
      "Epoch 298, train_loss: 0.68228 val_loss 0.69565  train_accuracy 0.5576 val_accuracy 0.5210  train_f1 0.5442 val_f1 0.5088\n",
      "Epoch 299, train_loss: 0.68225 val_loss 0.69568  train_accuracy 0.5575 val_accuracy 0.5220  train_f1 0.5437 val_f1 0.5105\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-300-0.6957.hdf5\n",
      "Epoch 300, train_loss: 0.68223 val_loss 0.69566  train_accuracy 0.5576 val_accuracy 0.5215  train_f1 0.5431 val_f1 0.5106\n",
      "Epoch 301, train_loss: 0.68221 val_loss 0.69570  train_accuracy 0.5577 val_accuracy 0.5214  train_f1 0.5438 val_f1 0.5138\n",
      "Epoch 302, train_loss: 0.68219 val_loss 0.69568  train_accuracy 0.5578 val_accuracy 0.5211  train_f1 0.5445 val_f1 0.5089\n",
      "Epoch 303, train_loss: 0.68217 val_loss 0.69568  train_accuracy 0.5577 val_accuracy 0.5218  train_f1 0.5430 val_f1 0.5131\n",
      "Epoch 304, train_loss: 0.68215 val_loss 0.69563  train_accuracy 0.5579 val_accuracy 0.5214  train_f1 0.5444 val_f1 0.5061\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-305-0.6956.hdf5\n",
      "Epoch 305, train_loss: 0.68212 val_loss 0.69570  train_accuracy 0.5579 val_accuracy 0.5216  train_f1 0.5436 val_f1 0.5156\n",
      "Epoch 306, train_loss: 0.68211 val_loss 0.69567  train_accuracy 0.5579 val_accuracy 0.5212  train_f1 0.5441 val_f1 0.5105\n",
      "Epoch 307, train_loss: 0.68209 val_loss 0.69566  train_accuracy 0.5579 val_accuracy 0.5218  train_f1 0.5437 val_f1 0.5123\n",
      "Epoch 308, train_loss: 0.68207 val_loss 0.69566  train_accuracy 0.5579 val_accuracy 0.5219  train_f1 0.5447 val_f1 0.5067\n",
      "Epoch 309, train_loss: 0.68205 val_loss 0.69564  train_accuracy 0.5580 val_accuracy 0.5217  train_f1 0.5449 val_f1 0.5042\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-310-0.6956.hdf5\n",
      "Epoch 310, train_loss: 0.68204 val_loss 0.69569  train_accuracy 0.5581 val_accuracy 0.5215  train_f1 0.5423 val_f1 0.5139\n",
      "Epoch 311, train_loss: 0.68201 val_loss 0.69568  train_accuracy 0.5582 val_accuracy 0.5216  train_f1 0.5449 val_f1 0.5141\n",
      "Epoch 312, train_loss: 0.68199 val_loss 0.69572  train_accuracy 0.5582 val_accuracy 0.5218  train_f1 0.5445 val_f1 0.5119\n",
      "Epoch 313, train_loss: 0.68197 val_loss 0.69568  train_accuracy 0.5584 val_accuracy 0.5220  train_f1 0.5451 val_f1 0.5078\n",
      "Epoch 314, train_loss: 0.68195 val_loss 0.69570  train_accuracy 0.5583 val_accuracy 0.5222  train_f1 0.5464 val_f1 0.5026\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-315-0.6957.hdf5\n",
      "Epoch 315, train_loss: 0.68194 val_loss 0.69569  train_accuracy 0.5584 val_accuracy 0.5218  train_f1 0.5439 val_f1 0.5052\n",
      "Epoch 316, train_loss: 0.68193 val_loss 0.69566  train_accuracy 0.5583 val_accuracy 0.5226  train_f1 0.5445 val_f1 0.5131\n",
      "Epoch 317, train_loss: 0.68189 val_loss 0.69575  train_accuracy 0.5584 val_accuracy 0.5219  train_f1 0.5424 val_f1 0.5148\n",
      "Epoch 318, train_loss: 0.68186 val_loss 0.69575  train_accuracy 0.5585 val_accuracy 0.5221  train_f1 0.5441 val_f1 0.5123\n",
      "Epoch 319, train_loss: 0.68185 val_loss 0.69574  train_accuracy 0.5586 val_accuracy 0.5219  train_f1 0.5458 val_f1 0.5109\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-320-0.6957.hdf5\n",
      "Epoch 320, train_loss: 0.68182 val_loss 0.69575  train_accuracy 0.5587 val_accuracy 0.5213  train_f1 0.5430 val_f1 0.5148\n",
      "Epoch 321, train_loss: 0.68180 val_loss 0.69576  train_accuracy 0.5587 val_accuracy 0.5217  train_f1 0.5447 val_f1 0.5162\n",
      "Epoch 322, train_loss: 0.68178 val_loss 0.69577  train_accuracy 0.5588 val_accuracy 0.5218  train_f1 0.5457 val_f1 0.5132\n",
      "Epoch 323, train_loss: 0.68176 val_loss 0.69576  train_accuracy 0.5588 val_accuracy 0.5225  train_f1 0.5450 val_f1 0.5154\n",
      "Epoch 324, train_loss: 0.68174 val_loss 0.69574  train_accuracy 0.5590 val_accuracy 0.5212  train_f1 0.5465 val_f1 0.5052\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-325-0.6957.hdf5\n",
      "Epoch 325, train_loss: 0.68172 val_loss 0.69572  train_accuracy 0.5590 val_accuracy 0.5219  train_f1 0.5469 val_f1 0.5070\n",
      "Epoch 326, train_loss: 0.68170 val_loss 0.69573  train_accuracy 0.5589 val_accuracy 0.5216  train_f1 0.5458 val_f1 0.5097\n",
      "Epoch 327, train_loss: 0.68169 val_loss 0.69573  train_accuracy 0.5591 val_accuracy 0.5220  train_f1 0.5468 val_f1 0.5118\n",
      "Epoch 328, train_loss: 0.68166 val_loss 0.69575  train_accuracy 0.5591 val_accuracy 0.5228  train_f1 0.5468 val_f1 0.5118\n",
      "Epoch 329, train_loss: 0.68164 val_loss 0.69576  train_accuracy 0.5592 val_accuracy 0.5223  train_f1 0.5475 val_f1 0.5042\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-330-0.6958.hdf5\n",
      "Epoch 330, train_loss: 0.68161 val_loss 0.69577  train_accuracy 0.5593 val_accuracy 0.5222  train_f1 0.5457 val_f1 0.5115\n",
      "Epoch 331, train_loss: 0.68159 val_loss 0.69580  train_accuracy 0.5593 val_accuracy 0.5216  train_f1 0.5453 val_f1 0.5183\n",
      "Epoch 332, train_loss: 0.68157 val_loss 0.69578  train_accuracy 0.5593 val_accuracy 0.5221  train_f1 0.5466 val_f1 0.5152\n",
      "Epoch 333, train_loss: 0.68154 val_loss 0.69581  train_accuracy 0.5594 val_accuracy 0.5224  train_f1 0.5452 val_f1 0.5134\n",
      "Epoch 334, train_loss: 0.68152 val_loss 0.69579  train_accuracy 0.5595 val_accuracy 0.5217  train_f1 0.5448 val_f1 0.5141\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-335-0.6958.hdf5\n",
      "Epoch 335, train_loss: 0.68150 val_loss 0.69580  train_accuracy 0.5597 val_accuracy 0.5221  train_f1 0.5469 val_f1 0.5122\n",
      "Epoch 336, train_loss: 0.68148 val_loss 0.69579  train_accuracy 0.5597 val_accuracy 0.5222  train_f1 0.5462 val_f1 0.5098\n",
      "Epoch 337, train_loss: 0.68146 val_loss 0.69580  train_accuracy 0.5597 val_accuracy 0.5220  train_f1 0.5446 val_f1 0.5179\n",
      "Epoch 338, train_loss: 0.68143 val_loss 0.69582  train_accuracy 0.5597 val_accuracy 0.5226  train_f1 0.5471 val_f1 0.5096\n",
      "Epoch 339, train_loss: 0.68141 val_loss 0.69583  train_accuracy 0.5598 val_accuracy 0.5220  train_f1 0.5441 val_f1 0.5157\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-340-0.6958.hdf5\n",
      "Epoch 340, train_loss: 0.68139 val_loss 0.69579  train_accuracy 0.5599 val_accuracy 0.5222  train_f1 0.5457 val_f1 0.5147\n",
      "Epoch 341, train_loss: 0.68137 val_loss 0.69580  train_accuracy 0.5600 val_accuracy 0.5227  train_f1 0.5454 val_f1 0.5134\n",
      "Epoch 342, train_loss: 0.68134 val_loss 0.69580  train_accuracy 0.5600 val_accuracy 0.5217  train_f1 0.5459 val_f1 0.5157\n",
      "Epoch 343, train_loss: 0.68133 val_loss 0.69583  train_accuracy 0.5601 val_accuracy 0.5221  train_f1 0.5467 val_f1 0.5151\n",
      "Epoch 344, train_loss: 0.68130 val_loss 0.69579  train_accuracy 0.5602 val_accuracy 0.5224  train_f1 0.5470 val_f1 0.5156\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-345-0.6958.hdf5\n",
      "Epoch 345, train_loss: 0.68128 val_loss 0.69584  train_accuracy 0.5601 val_accuracy 0.5225  train_f1 0.5467 val_f1 0.5173\n",
      "Epoch 346, train_loss: 0.68126 val_loss 0.69585  train_accuracy 0.5602 val_accuracy 0.5219  train_f1 0.5451 val_f1 0.5161\n",
      "Epoch 347, train_loss: 0.68124 val_loss 0.69586  train_accuracy 0.5603 val_accuracy 0.5220  train_f1 0.5464 val_f1 0.5176\n",
      "Epoch 348, train_loss: 0.68121 val_loss 0.69582  train_accuracy 0.5604 val_accuracy 0.5222  train_f1 0.5481 val_f1 0.5122\n",
      "Epoch 349, train_loss: 0.68119 val_loss 0.69588  train_accuracy 0.5605 val_accuracy 0.5226  train_f1 0.5465 val_f1 0.5174\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-350-0.6959.hdf5\n",
      "Epoch 350, train_loss: 0.68117 val_loss 0.69581  train_accuracy 0.5605 val_accuracy 0.5223  train_f1 0.5476 val_f1 0.5138\n",
      "Epoch 351, train_loss: 0.68115 val_loss 0.69591  train_accuracy 0.5606 val_accuracy 0.5225  train_f1 0.5466 val_f1 0.5137\n",
      "Epoch 352, train_loss: 0.68112 val_loss 0.69588  train_accuracy 0.5607 val_accuracy 0.5223  train_f1 0.5466 val_f1 0.5134\n",
      "Epoch 353, train_loss: 0.68110 val_loss 0.69582  train_accuracy 0.5607 val_accuracy 0.5225  train_f1 0.5474 val_f1 0.5109\n",
      "Epoch 354, train_loss: 0.68109 val_loss 0.69584  train_accuracy 0.5607 val_accuracy 0.5228  train_f1 0.5474 val_f1 0.5080\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-355-0.6958.hdf5\n",
      "Epoch 355, train_loss: 0.68105 val_loss 0.69587  train_accuracy 0.5608 val_accuracy 0.5220  train_f1 0.5467 val_f1 0.5132\n",
      "Epoch 356, train_loss: 0.68103 val_loss 0.69589  train_accuracy 0.5610 val_accuracy 0.5227  train_f1 0.5477 val_f1 0.5031\n",
      "Epoch 357, train_loss: 0.68101 val_loss 0.69590  train_accuracy 0.5609 val_accuracy 0.5225  train_f1 0.5468 val_f1 0.5105\n",
      "Epoch 358, train_loss: 0.68098 val_loss 0.69589  train_accuracy 0.5611 val_accuracy 0.5217  train_f1 0.5467 val_f1 0.5159\n",
      "Epoch 359, train_loss: 0.68096 val_loss 0.69585  train_accuracy 0.5612 val_accuracy 0.5219  train_f1 0.5483 val_f1 0.5097\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-360-0.6958.hdf5\n",
      "Epoch 360, train_loss: 0.68094 val_loss 0.69588  train_accuracy 0.5611 val_accuracy 0.5228  train_f1 0.5481 val_f1 0.5096\n",
      "Epoch 361, train_loss: 0.68091 val_loss 0.69589  train_accuracy 0.5612 val_accuracy 0.5226  train_f1 0.5477 val_f1 0.5074\n",
      "Epoch 362, train_loss: 0.68089 val_loss 0.69589  train_accuracy 0.5612 val_accuracy 0.5221  train_f1 0.5475 val_f1 0.5134\n",
      "Epoch 363, train_loss: 0.68087 val_loss 0.69587  train_accuracy 0.5613 val_accuracy 0.5231  train_f1 0.5505 val_f1 0.5053\n",
      "Epoch 364, train_loss: 0.68084 val_loss 0.69593  train_accuracy 0.5615 val_accuracy 0.5218  train_f1 0.5477 val_f1 0.5041\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-365-0.6959.hdf5\n",
      "Epoch 365, train_loss: 0.68082 val_loss 0.69589  train_accuracy 0.5615 val_accuracy 0.5225  train_f1 0.5479 val_f1 0.5070\n",
      "Epoch 366, train_loss: 0.68080 val_loss 0.69592  train_accuracy 0.5616 val_accuracy 0.5229  train_f1 0.5479 val_f1 0.5074\n",
      "Epoch 367, train_loss: 0.68077 val_loss 0.69595  train_accuracy 0.5617 val_accuracy 0.5223  train_f1 0.5481 val_f1 0.5158\n",
      "Epoch 368, train_loss: 0.68074 val_loss 0.69596  train_accuracy 0.5617 val_accuracy 0.5218  train_f1 0.5483 val_f1 0.5150\n",
      "Epoch 369, train_loss: 0.68072 val_loss 0.69594  train_accuracy 0.5618 val_accuracy 0.5218  train_f1 0.5494 val_f1 0.5159\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-370-0.6959.hdf5\n",
      "Epoch 370, train_loss: 0.68070 val_loss 0.69603  train_accuracy 0.5618 val_accuracy 0.5216  train_f1 0.5474 val_f1 0.5218\n",
      "Epoch 371, train_loss: 0.68069 val_loss 0.69598  train_accuracy 0.5619 val_accuracy 0.5218  train_f1 0.5486 val_f1 0.5231\n",
      "Epoch 372, train_loss: 0.68066 val_loss 0.69598  train_accuracy 0.5619 val_accuracy 0.5217  train_f1 0.5493 val_f1 0.5191\n",
      "Epoch 373, train_loss: 0.68063 val_loss 0.69602  train_accuracy 0.5620 val_accuracy 0.5220  train_f1 0.5466 val_f1 0.5192\n",
      "Epoch 374, train_loss: 0.68061 val_loss 0.69597  train_accuracy 0.5621 val_accuracy 0.5217  train_f1 0.5491 val_f1 0.5183\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-375-0.6960.hdf5\n",
      "Epoch 375, train_loss: 0.68057 val_loss 0.69602  train_accuracy 0.5622 val_accuracy 0.5220  train_f1 0.5478 val_f1 0.5158\n",
      "Epoch 376, train_loss: 0.68055 val_loss 0.69598  train_accuracy 0.5622 val_accuracy 0.5220  train_f1 0.5497 val_f1 0.5090\n",
      "Epoch 377, train_loss: 0.68053 val_loss 0.69601  train_accuracy 0.5624 val_accuracy 0.5227  train_f1 0.5494 val_f1 0.5073\n",
      "Epoch 378, train_loss: 0.68050 val_loss 0.69602  train_accuracy 0.5624 val_accuracy 0.5216  train_f1 0.5488 val_f1 0.5146\n",
      "Epoch 379, train_loss: 0.68048 val_loss 0.69602  train_accuracy 0.5625 val_accuracy 0.5217  train_f1 0.5493 val_f1 0.5127\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-380-0.6960.hdf5\n",
      "Epoch 380, train_loss: 0.68046 val_loss 0.69606  train_accuracy 0.5625 val_accuracy 0.5216  train_f1 0.5482 val_f1 0.5128\n",
      "Epoch 381, train_loss: 0.68044 val_loss 0.69603  train_accuracy 0.5625 val_accuracy 0.5218  train_f1 0.5510 val_f1 0.5156\n",
      "Epoch 382, train_loss: 0.68040 val_loss 0.69605  train_accuracy 0.5627 val_accuracy 0.5224  train_f1 0.5489 val_f1 0.5104\n",
      "Epoch 383, train_loss: 0.68038 val_loss 0.69604  train_accuracy 0.5627 val_accuracy 0.5220  train_f1 0.5483 val_f1 0.5123\n",
      "Epoch 384, train_loss: 0.68036 val_loss 0.69602  train_accuracy 0.5626 val_accuracy 0.5221  train_f1 0.5510 val_f1 0.5090\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-385-0.6960.hdf5\n",
      "Epoch 385, train_loss: 0.68033 val_loss 0.69607  train_accuracy 0.5628 val_accuracy 0.5220  train_f1 0.5487 val_f1 0.5123\n",
      "Epoch 386, train_loss: 0.68031 val_loss 0.69608  train_accuracy 0.5629 val_accuracy 0.5227  train_f1 0.5494 val_f1 0.5064\n",
      "Epoch 387, train_loss: 0.68029 val_loss 0.69605  train_accuracy 0.5628 val_accuracy 0.5219  train_f1 0.5497 val_f1 0.5085\n",
      "Epoch 388, train_loss: 0.68025 val_loss 0.69607  train_accuracy 0.5630 val_accuracy 0.5221  train_f1 0.5502 val_f1 0.5105\n",
      "Epoch 389, train_loss: 0.68022 val_loss 0.69609  train_accuracy 0.5630 val_accuracy 0.5229  train_f1 0.5501 val_f1 0.5040\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-390-0.6961.hdf5\n",
      "Epoch 390, train_loss: 0.68022 val_loss 0.69612  train_accuracy 0.5630 val_accuracy 0.5220  train_f1 0.5492 val_f1 0.5080\n",
      "Epoch 391, train_loss: 0.68018 val_loss 0.69609  train_accuracy 0.5631 val_accuracy 0.5221  train_f1 0.5487 val_f1 0.5101\n",
      "Epoch 392, train_loss: 0.68015 val_loss 0.69610  train_accuracy 0.5632 val_accuracy 0.5219  train_f1 0.5504 val_f1 0.5121\n",
      "Epoch 393, train_loss: 0.68013 val_loss 0.69613  train_accuracy 0.5633 val_accuracy 0.5224  train_f1 0.5510 val_f1 0.5100\n",
      "Epoch 394, train_loss: 0.68010 val_loss 0.69616  train_accuracy 0.5633 val_accuracy 0.5229  train_f1 0.5495 val_f1 0.5071\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-395-0.6962.hdf5\n",
      "Epoch 395, train_loss: 0.68008 val_loss 0.69610  train_accuracy 0.5634 val_accuracy 0.5224  train_f1 0.5506 val_f1 0.5079\n",
      "Epoch 396, train_loss: 0.68006 val_loss 0.69613  train_accuracy 0.5634 val_accuracy 0.5220  train_f1 0.5492 val_f1 0.5108\n",
      "Epoch 397, train_loss: 0.68003 val_loss 0.69613  train_accuracy 0.5635 val_accuracy 0.5223  train_f1 0.5499 val_f1 0.5091\n",
      "Epoch 398, train_loss: 0.68000 val_loss 0.69619  train_accuracy 0.5635 val_accuracy 0.5220  train_f1 0.5503 val_f1 0.5123\n",
      "Epoch 399, train_loss: 0.67997 val_loss 0.69614  train_accuracy 0.5636 val_accuracy 0.5227  train_f1 0.5498 val_f1 0.5079\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-400-0.6961.hdf5\n",
      "Epoch 400, train_loss: 0.67995 val_loss 0.69611  train_accuracy 0.5637 val_accuracy 0.5228  train_f1 0.5510 val_f1 0.5071\n",
      "Epoch 401, train_loss: 0.67993 val_loss 0.69618  train_accuracy 0.5639 val_accuracy 0.5226  train_f1 0.5511 val_f1 0.5068\n",
      "Epoch 402, train_loss: 0.67990 val_loss 0.69616  train_accuracy 0.5639 val_accuracy 0.5228  train_f1 0.5512 val_f1 0.5085\n",
      "Epoch 403, train_loss: 0.67987 val_loss 0.69620  train_accuracy 0.5639 val_accuracy 0.5225  train_f1 0.5496 val_f1 0.5093\n",
      "Epoch 404, train_loss: 0.67984 val_loss 0.69616  train_accuracy 0.5642 val_accuracy 0.5223  train_f1 0.5527 val_f1 0.5133\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-405-0.6962.hdf5\n",
      "Epoch 405, train_loss: 0.67982 val_loss 0.69625  train_accuracy 0.5640 val_accuracy 0.5227  train_f1 0.5512 val_f1 0.5068\n",
      "Epoch 406, train_loss: 0.67980 val_loss 0.69620  train_accuracy 0.5640 val_accuracy 0.5221  train_f1 0.5489 val_f1 0.5103\n",
      "Epoch 407, train_loss: 0.67978 val_loss 0.69624  train_accuracy 0.5642 val_accuracy 0.5221  train_f1 0.5526 val_f1 0.5180\n",
      "Epoch 408, train_loss: 0.67975 val_loss 0.69626  train_accuracy 0.5642 val_accuracy 0.5224  train_f1 0.5514 val_f1 0.5089\n",
      "Epoch 409, train_loss: 0.67971 val_loss 0.69620  train_accuracy 0.5644 val_accuracy 0.5227  train_f1 0.5511 val_f1 0.5080\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-410-0.6962.hdf5\n",
      "Epoch 410, train_loss: 0.67969 val_loss 0.69630  train_accuracy 0.5645 val_accuracy 0.5215  train_f1 0.5509 val_f1 0.5163\n",
      "Epoch 411, train_loss: 0.67967 val_loss 0.69632  train_accuracy 0.5644 val_accuracy 0.5209  train_f1 0.5509 val_f1 0.5242\n",
      "Epoch 412, train_loss: 0.67965 val_loss 0.69630  train_accuracy 0.5646 val_accuracy 0.5220  train_f1 0.5523 val_f1 0.5146\n",
      "Epoch 413, train_loss: 0.67961 val_loss 0.69628  train_accuracy 0.5646 val_accuracy 0.5219  train_f1 0.5502 val_f1 0.5155\n",
      "Epoch 414, train_loss: 0.67958 val_loss 0.69627  train_accuracy 0.5646 val_accuracy 0.5229  train_f1 0.5522 val_f1 0.5106\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-415-0.6963.hdf5\n",
      "Epoch 415, train_loss: 0.67955 val_loss 0.69630  train_accuracy 0.5647 val_accuracy 0.5228  train_f1 0.5510 val_f1 0.5141\n",
      "Epoch 416, train_loss: 0.67952 val_loss 0.69628  train_accuracy 0.5648 val_accuracy 0.5223  train_f1 0.5526 val_f1 0.5055\n",
      "Epoch 417, train_loss: 0.67950 val_loss 0.69632  train_accuracy 0.5648 val_accuracy 0.5219  train_f1 0.5512 val_f1 0.5103\n",
      "Epoch 418, train_loss: 0.67948 val_loss 0.69638  train_accuracy 0.5649 val_accuracy 0.5219  train_f1 0.5509 val_f1 0.5180\n",
      "Epoch 419, train_loss: 0.67945 val_loss 0.69632  train_accuracy 0.5648 val_accuracy 0.5209  train_f1 0.5525 val_f1 0.5172\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-420-0.6963.hdf5\n",
      "Epoch 420, train_loss: 0.67941 val_loss 0.69639  train_accuracy 0.5651 val_accuracy 0.5216  train_f1 0.5504 val_f1 0.5156\n",
      "Epoch 421, train_loss: 0.67939 val_loss 0.69632  train_accuracy 0.5650 val_accuracy 0.5225  train_f1 0.5515 val_f1 0.5063\n",
      "Epoch 422, train_loss: 0.67937 val_loss 0.69634  train_accuracy 0.5652 val_accuracy 0.5227  train_f1 0.5524 val_f1 0.5061\n",
      "Epoch 423, train_loss: 0.67934 val_loss 0.69637  train_accuracy 0.5652 val_accuracy 0.5219  train_f1 0.5519 val_f1 0.5128\n",
      "Epoch 424, train_loss: 0.67931 val_loss 0.69638  train_accuracy 0.5653 val_accuracy 0.5220  train_f1 0.5524 val_f1 0.5168\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-425-0.6964.hdf5\n",
      "Epoch 425, train_loss: 0.67928 val_loss 0.69636  train_accuracy 0.5654 val_accuracy 0.5223  train_f1 0.5532 val_f1 0.5131\n",
      "Epoch 426, train_loss: 0.67925 val_loss 0.69639  train_accuracy 0.5655 val_accuracy 0.5219  train_f1 0.5514 val_f1 0.5098\n",
      "Epoch 427, train_loss: 0.67922 val_loss 0.69637  train_accuracy 0.5655 val_accuracy 0.5222  train_f1 0.5523 val_f1 0.5092\n",
      "Epoch 428, train_loss: 0.67920 val_loss 0.69643  train_accuracy 0.5656 val_accuracy 0.5215  train_f1 0.5507 val_f1 0.5186\n",
      "Epoch 429, train_loss: 0.67917 val_loss 0.69642  train_accuracy 0.5658 val_accuracy 0.5229  train_f1 0.5537 val_f1 0.5079\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-430-0.6964.hdf5\n",
      "Epoch 430, train_loss: 0.67915 val_loss 0.69639  train_accuracy 0.5658 val_accuracy 0.5218  train_f1 0.5519 val_f1 0.5108\n",
      "Epoch 431, train_loss: 0.67911 val_loss 0.69648  train_accuracy 0.5658 val_accuracy 0.5220  train_f1 0.5531 val_f1 0.5184\n",
      "Epoch 432, train_loss: 0.67908 val_loss 0.69648  train_accuracy 0.5660 val_accuracy 0.5221  train_f1 0.5528 val_f1 0.5129\n",
      "Epoch 433, train_loss: 0.67906 val_loss 0.69644  train_accuracy 0.5660 val_accuracy 0.5222  train_f1 0.5523 val_f1 0.5127\n",
      "Epoch 434, train_loss: 0.67903 val_loss 0.69650  train_accuracy 0.5660 val_accuracy 0.5226  train_f1 0.5529 val_f1 0.5130\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-435-0.6965.hdf5\n",
      "Epoch 435, train_loss: 0.67900 val_loss 0.69652  train_accuracy 0.5661 val_accuracy 0.5213  train_f1 0.5519 val_f1 0.5210\n",
      "Epoch 436, train_loss: 0.67900 val_loss 0.69656  train_accuracy 0.5661 val_accuracy 0.5210  train_f1 0.5546 val_f1 0.5238\n",
      "Epoch 437, train_loss: 0.67895 val_loss 0.69647  train_accuracy 0.5661 val_accuracy 0.5221  train_f1 0.5533 val_f1 0.5134\n",
      "Epoch 438, train_loss: 0.67891 val_loss 0.69652  train_accuracy 0.5662 val_accuracy 0.5224  train_f1 0.5528 val_f1 0.5177\n",
      "Epoch 439, train_loss: 0.67888 val_loss 0.69653  train_accuracy 0.5664 val_accuracy 0.5221  train_f1 0.5537 val_f1 0.5044\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-440-0.6965.hdf5\n",
      "Epoch 440, train_loss: 0.67888 val_loss 0.69652  train_accuracy 0.5663 val_accuracy 0.5225  train_f1 0.5529 val_f1 0.5080\n",
      "Epoch 441, train_loss: 0.67883 val_loss 0.69656  train_accuracy 0.5663 val_accuracy 0.5216  train_f1 0.5525 val_f1 0.5184\n",
      "Epoch 442, train_loss: 0.67882 val_loss 0.69654  train_accuracy 0.5666 val_accuracy 0.5225  train_f1 0.5558 val_f1 0.5066\n",
      "Epoch 443, train_loss: 0.67879 val_loss 0.69661  train_accuracy 0.5665 val_accuracy 0.5228  train_f1 0.5525 val_f1 0.4983\n",
      "Epoch 444, train_loss: 0.67877 val_loss 0.69659  train_accuracy 0.5667 val_accuracy 0.5223  train_f1 0.5529 val_f1 0.5142\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-445-0.6966.hdf5\n",
      "Epoch 445, train_loss: 0.67875 val_loss 0.69674  train_accuracy 0.5666 val_accuracy 0.5203  train_f1 0.5542 val_f1 0.5265\n",
      "Epoch 446, train_loss: 0.67872 val_loss 0.69666  train_accuracy 0.5667 val_accuracy 0.5224  train_f1 0.5544 val_f1 0.5135\n",
      "Epoch 447, train_loss: 0.67867 val_loss 0.69657  train_accuracy 0.5668 val_accuracy 0.5224  train_f1 0.5540 val_f1 0.5087\n",
      "Epoch 448, train_loss: 0.67863 val_loss 0.69662  train_accuracy 0.5669 val_accuracy 0.5229  train_f1 0.5545 val_f1 0.4983\n",
      "Epoch 449, train_loss: 0.67862 val_loss 0.69657  train_accuracy 0.5669 val_accuracy 0.5223  train_f1 0.5528 val_f1 0.5098\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-450-0.6966.hdf5\n",
      "Epoch 450, train_loss: 0.67857 val_loss 0.69672  train_accuracy 0.5669 val_accuracy 0.5219  train_f1 0.5548 val_f1 0.5230\n",
      "Epoch 451, train_loss: 0.67857 val_loss 0.69665  train_accuracy 0.5671 val_accuracy 0.5219  train_f1 0.5546 val_f1 0.5136\n",
      "Epoch 452, train_loss: 0.67852 val_loss 0.69666  train_accuracy 0.5671 val_accuracy 0.5217  train_f1 0.5544 val_f1 0.5172\n",
      "Epoch 453, train_loss: 0.67849 val_loss 0.69669  train_accuracy 0.5672 val_accuracy 0.5226  train_f1 0.5544 val_f1 0.5104\n",
      "Epoch 454, train_loss: 0.67847 val_loss 0.69668  train_accuracy 0.5674 val_accuracy 0.5228  train_f1 0.5538 val_f1 0.5104\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-455-0.6967.hdf5\n",
      "Epoch 455, train_loss: 0.67843 val_loss 0.69670  train_accuracy 0.5673 val_accuracy 0.5218  train_f1 0.5546 val_f1 0.5127\n",
      "Epoch 456, train_loss: 0.67841 val_loss 0.69677  train_accuracy 0.5675 val_accuracy 0.5211  train_f1 0.5549 val_f1 0.5239\n",
      "Epoch 457, train_loss: 0.67838 val_loss 0.69685  train_accuracy 0.5675 val_accuracy 0.5212  train_f1 0.5545 val_f1 0.5267\n",
      "Epoch 458, train_loss: 0.67836 val_loss 0.69676  train_accuracy 0.5675 val_accuracy 0.5209  train_f1 0.5552 val_f1 0.5198\n",
      "Epoch 459, train_loss: 0.67831 val_loss 0.69671  train_accuracy 0.5676 val_accuracy 0.5232  train_f1 0.5552 val_f1 0.5030\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-460-0.6967.hdf5\n",
      "Epoch 460, train_loss: 0.67833 val_loss 0.69671  train_accuracy 0.5677 val_accuracy 0.5233  train_f1 0.5533 val_f1 0.5102\n",
      "Epoch 461, train_loss: 0.67826 val_loss 0.69675  train_accuracy 0.5676 val_accuracy 0.5213  train_f1 0.5541 val_f1 0.5186\n",
      "Epoch 462, train_loss: 0.67823 val_loss 0.69689  train_accuracy 0.5677 val_accuracy 0.5225  train_f1 0.5563 val_f1 0.5230\n",
      "Epoch 463, train_loss: 0.67820 val_loss 0.69675  train_accuracy 0.5678 val_accuracy 0.5223  train_f1 0.5569 val_f1 0.5103\n",
      "Epoch 464, train_loss: 0.67816 val_loss 0.69687  train_accuracy 0.5680 val_accuracy 0.5225  train_f1 0.5533 val_f1 0.5186\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-465-0.6969.hdf5\n",
      "Epoch 465, train_loss: 0.67813 val_loss 0.69683  train_accuracy 0.5681 val_accuracy 0.5213  train_f1 0.5565 val_f1 0.5167\n",
      "Epoch 466, train_loss: 0.67811 val_loss 0.69697  train_accuracy 0.5681 val_accuracy 0.5214  train_f1 0.5547 val_f1 0.5251\n",
      "Epoch 467, train_loss: 0.67810 val_loss 0.69685  train_accuracy 0.5681 val_accuracy 0.5217  train_f1 0.5570 val_f1 0.5179\n",
      "Epoch 468, train_loss: 0.67806 val_loss 0.69685  train_accuracy 0.5682 val_accuracy 0.5230  train_f1 0.5553 val_f1 0.5032\n",
      "Epoch 469, train_loss: 0.67803 val_loss 0.69683  train_accuracy 0.5682 val_accuracy 0.5225  train_f1 0.5550 val_f1 0.5091\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-470-0.6968.hdf5\n",
      "Epoch 470, train_loss: 0.67799 val_loss 0.69686  train_accuracy 0.5683 val_accuracy 0.5212  train_f1 0.5543 val_f1 0.5182\n",
      "Epoch 471, train_loss: 0.67796 val_loss 0.69697  train_accuracy 0.5685 val_accuracy 0.5214  train_f1 0.5576 val_f1 0.5256\n",
      "Epoch 472, train_loss: 0.67795 val_loss 0.69689  train_accuracy 0.5684 val_accuracy 0.5229  train_f1 0.5574 val_f1 0.5071\n",
      "Epoch 473, train_loss: 0.67791 val_loss 0.69690  train_accuracy 0.5686 val_accuracy 0.5227  train_f1 0.5552 val_f1 0.5026\n",
      "Epoch 474, train_loss: 0.67787 val_loss 0.69695  train_accuracy 0.5685 val_accuracy 0.5214  train_f1 0.5546 val_f1 0.5207\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-475-0.6970.hdf5\n",
      "Epoch 475, train_loss: 0.67785 val_loss 0.69695  train_accuracy 0.5687 val_accuracy 0.5229  train_f1 0.5567 val_f1 0.5159\n",
      "Epoch 476, train_loss: 0.67782 val_loss 0.69693  train_accuracy 0.5687 val_accuracy 0.5225  train_f1 0.5570 val_f1 0.5035\n",
      "Epoch 477, train_loss: 0.67778 val_loss 0.69697  train_accuracy 0.5687 val_accuracy 0.5227  train_f1 0.5544 val_f1 0.5121\n",
      "Epoch 478, train_loss: 0.67775 val_loss 0.69696  train_accuracy 0.5688 val_accuracy 0.5219  train_f1 0.5577 val_f1 0.5155\n",
      "Epoch 479, train_loss: 0.67771 val_loss 0.69698  train_accuracy 0.5690 val_accuracy 0.5221  train_f1 0.5552 val_f1 0.5148\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-480-0.6970.hdf5\n",
      "Epoch 480, train_loss: 0.67767 val_loss 0.69696  train_accuracy 0.5690 val_accuracy 0.5217  train_f1 0.5572 val_f1 0.5153\n",
      "Epoch 481, train_loss: 0.67765 val_loss 0.69704  train_accuracy 0.5691 val_accuracy 0.5225  train_f1 0.5563 val_f1 0.5127\n",
      "Epoch 482, train_loss: 0.67761 val_loss 0.69702  train_accuracy 0.5692 val_accuracy 0.5227  train_f1 0.5565 val_f1 0.5133\n",
      "Epoch 483, train_loss: 0.67758 val_loss 0.69706  train_accuracy 0.5694 val_accuracy 0.5230  train_f1 0.5570 val_f1 0.5157\n",
      "Epoch 484, train_loss: 0.67756 val_loss 0.69698  train_accuracy 0.5692 val_accuracy 0.5212  train_f1 0.5570 val_f1 0.5140\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-485-0.6970.hdf5\n",
      "Epoch 485, train_loss: 0.67754 val_loss 0.69725  train_accuracy 0.5693 val_accuracy 0.5207  train_f1 0.5570 val_f1 0.5281\n",
      "Epoch 486, train_loss: 0.67751 val_loss 0.69711  train_accuracy 0.5695 val_accuracy 0.5221  train_f1 0.5573 val_f1 0.5178\n",
      "Epoch 487, train_loss: 0.67747 val_loss 0.69703  train_accuracy 0.5695 val_accuracy 0.5217  train_f1 0.5581 val_f1 0.5079\n",
      "Epoch 488, train_loss: 0.67744 val_loss 0.69716  train_accuracy 0.5697 val_accuracy 0.5218  train_f1 0.5549 val_f1 0.5203\n",
      "Epoch 489, train_loss: 0.67741 val_loss 0.69709  train_accuracy 0.5696 val_accuracy 0.5215  train_f1 0.5574 val_f1 0.5165\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-490-0.6971.hdf5\n",
      "Epoch 490, train_loss: 0.67737 val_loss 0.69702  train_accuracy 0.5698 val_accuracy 0.5223  train_f1 0.5567 val_f1 0.5105\n",
      "Epoch 491, train_loss: 0.67734 val_loss 0.69715  train_accuracy 0.5699 val_accuracy 0.5220  train_f1 0.5578 val_f1 0.5129\n",
      "Epoch 492, train_loss: 0.67731 val_loss 0.69706  train_accuracy 0.5700 val_accuracy 0.5220  train_f1 0.5570 val_f1 0.5112\n",
      "Epoch 493, train_loss: 0.67728 val_loss 0.69728  train_accuracy 0.5701 val_accuracy 0.5211  train_f1 0.5573 val_f1 0.5256\n",
      "Epoch 494, train_loss: 0.67725 val_loss 0.69713  train_accuracy 0.5701 val_accuracy 0.5218  train_f1 0.5589 val_f1 0.5090\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-495-0.6971.hdf5\n",
      "Epoch 495, train_loss: 0.67722 val_loss 0.69717  train_accuracy 0.5702 val_accuracy 0.5235  train_f1 0.5577 val_f1 0.5014\n",
      "Epoch 496, train_loss: 0.67723 val_loss 0.69729  train_accuracy 0.5701 val_accuracy 0.5215  train_f1 0.5563 val_f1 0.5228\n",
      "Epoch 497, train_loss: 0.67715 val_loss 0.69724  train_accuracy 0.5704 val_accuracy 0.5213  train_f1 0.5592 val_f1 0.5208\n",
      "Epoch 498, train_loss: 0.67711 val_loss 0.69723  train_accuracy 0.5703 val_accuracy 0.5223  train_f1 0.5581 val_f1 0.5127\n",
      "Epoch 499, train_loss: 0.67708 val_loss 0.69723  train_accuracy 0.5704 val_accuracy 0.5225  train_f1 0.5579 val_f1 0.5117\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-500-0.6972.hdf5\n",
      "Epoch 500, train_loss: 0.67705 val_loss 0.69726  train_accuracy 0.5705 val_accuracy 0.5208  train_f1 0.5581 val_f1 0.5193\n",
      "Epoch 501, train_loss: 0.67703 val_loss 0.69742  train_accuracy 0.5707 val_accuracy 0.5208  train_f1 0.5583 val_f1 0.5247\n",
      "Epoch 502, train_loss: 0.67701 val_loss 0.69724  train_accuracy 0.5705 val_accuracy 0.5211  train_f1 0.5588 val_f1 0.5181\n",
      "Epoch 503, train_loss: 0.67698 val_loss 0.69727  train_accuracy 0.5708 val_accuracy 0.5229  train_f1 0.5613 val_f1 0.4945\n",
      "Epoch 504, train_loss: 0.67700 val_loss 0.69736  train_accuracy 0.5706 val_accuracy 0.5210  train_f1 0.5542 val_f1 0.5181\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-505-0.6974.hdf5\n",
      "Epoch 505, train_loss: 0.67691 val_loss 0.69726  train_accuracy 0.5710 val_accuracy 0.5217  train_f1 0.5601 val_f1 0.5126\n",
      "Epoch 506, train_loss: 0.67686 val_loss 0.69738  train_accuracy 0.5710 val_accuracy 0.5212  train_f1 0.5589 val_f1 0.5145\n",
      "Epoch 507, train_loss: 0.67682 val_loss 0.69737  train_accuracy 0.5710 val_accuracy 0.5221  train_f1 0.5573 val_f1 0.5138\n",
      "Epoch 508, train_loss: 0.67680 val_loss 0.69736  train_accuracy 0.5710 val_accuracy 0.5218  train_f1 0.5594 val_f1 0.5111\n",
      "Epoch 509, train_loss: 0.67676 val_loss 0.69732  train_accuracy 0.5712 val_accuracy 0.5213  train_f1 0.5579 val_f1 0.5140\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-510-0.6973.hdf5\n",
      "Epoch 510, train_loss: 0.67673 val_loss 0.69740  train_accuracy 0.5713 val_accuracy 0.5208  train_f1 0.5606 val_f1 0.5184\n",
      "Epoch 511, train_loss: 0.67671 val_loss 0.69740  train_accuracy 0.5713 val_accuracy 0.5213  train_f1 0.5588 val_f1 0.5147\n",
      "Epoch 512, train_loss: 0.67666 val_loss 0.69746  train_accuracy 0.5714 val_accuracy 0.5215  train_f1 0.5587 val_f1 0.5190\n",
      "Epoch 513, train_loss: 0.67664 val_loss 0.69742  train_accuracy 0.5716 val_accuracy 0.5217  train_f1 0.5611 val_f1 0.5091\n",
      "Epoch 514, train_loss: 0.67660 val_loss 0.69747  train_accuracy 0.5716 val_accuracy 0.5214  train_f1 0.5580 val_f1 0.5176\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-515-0.6975.hdf5\n",
      "Epoch 515, train_loss: 0.67657 val_loss 0.69742  train_accuracy 0.5717 val_accuracy 0.5227  train_f1 0.5606 val_f1 0.5086\n",
      "Epoch 516, train_loss: 0.67654 val_loss 0.69744  train_accuracy 0.5717 val_accuracy 0.5216  train_f1 0.5597 val_f1 0.5128\n",
      "Epoch 517, train_loss: 0.67650 val_loss 0.69756  train_accuracy 0.5719 val_accuracy 0.5214  train_f1 0.5598 val_f1 0.5122\n",
      "Epoch 518, train_loss: 0.67647 val_loss 0.69749  train_accuracy 0.5719 val_accuracy 0.5202  train_f1 0.5602 val_f1 0.5136\n",
      "Epoch 519, train_loss: 0.67644 val_loss 0.69756  train_accuracy 0.5719 val_accuracy 0.5210  train_f1 0.5597 val_f1 0.5232\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-520-0.6976.hdf5\n",
      "Epoch 520, train_loss: 0.67642 val_loss 0.69755  train_accuracy 0.5720 val_accuracy 0.5214  train_f1 0.5620 val_f1 0.5150\n",
      "Epoch 521, train_loss: 0.67638 val_loss 0.69753  train_accuracy 0.5722 val_accuracy 0.5224  train_f1 0.5607 val_f1 0.5059\n",
      "Epoch 522, train_loss: 0.67634 val_loss 0.69763  train_accuracy 0.5721 val_accuracy 0.5220  train_f1 0.5597 val_f1 0.5136\n",
      "Epoch 523, train_loss: 0.67631 val_loss 0.69762  train_accuracy 0.5721 val_accuracy 0.5198  train_f1 0.5588 val_f1 0.5223\n",
      "Epoch 524, train_loss: 0.67629 val_loss 0.69760  train_accuracy 0.5724 val_accuracy 0.5212  train_f1 0.5622 val_f1 0.5145\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-525-0.6976.hdf5\n",
      "Epoch 525, train_loss: 0.67624 val_loss 0.69769  train_accuracy 0.5724 val_accuracy 0.5203  train_f1 0.5597 val_f1 0.5242\n",
      "Epoch 526, train_loss: 0.67623 val_loss 0.69765  train_accuracy 0.5721 val_accuracy 0.5228  train_f1 0.5618 val_f1 0.5047\n",
      "Epoch 527, train_loss: 0.67618 val_loss 0.69760  train_accuracy 0.5725 val_accuracy 0.5220  train_f1 0.5599 val_f1 0.5100\n",
      "Epoch 528, train_loss: 0.67613 val_loss 0.69765  train_accuracy 0.5727 val_accuracy 0.5218  train_f1 0.5600 val_f1 0.5120\n",
      "Epoch 529, train_loss: 0.67611 val_loss 0.69775  train_accuracy 0.5727 val_accuracy 0.5212  train_f1 0.5598 val_f1 0.5146\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-530-0.6978.hdf5\n",
      "Epoch 530, train_loss: 0.67607 val_loss 0.69769  train_accuracy 0.5727 val_accuracy 0.5211  train_f1 0.5616 val_f1 0.5154\n",
      "Epoch 531, train_loss: 0.67603 val_loss 0.69771  train_accuracy 0.5728 val_accuracy 0.5212  train_f1 0.5614 val_f1 0.5130\n",
      "Epoch 532, train_loss: 0.67601 val_loss 0.69771  train_accuracy 0.5729 val_accuracy 0.5228  train_f1 0.5611 val_f1 0.4989\n",
      "Epoch 533, train_loss: 0.67601 val_loss 0.69782  train_accuracy 0.5729 val_accuracy 0.5203  train_f1 0.5586 val_f1 0.5228\n",
      "Epoch 534, train_loss: 0.67596 val_loss 0.69772  train_accuracy 0.5730 val_accuracy 0.5213  train_f1 0.5626 val_f1 0.5101\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-535-0.6977.hdf5\n",
      "Epoch 535, train_loss: 0.67593 val_loss 0.69775  train_accuracy 0.5731 val_accuracy 0.5215  train_f1 0.5609 val_f1 0.5117\n",
      "Epoch 536, train_loss: 0.67587 val_loss 0.69783  train_accuracy 0.5732 val_accuracy 0.5216  train_f1 0.5622 val_f1 0.5179\n",
      "Epoch 537, train_loss: 0.67584 val_loss 0.69784  train_accuracy 0.5732 val_accuracy 0.5207  train_f1 0.5623 val_f1 0.5140\n",
      "Epoch 538, train_loss: 0.67579 val_loss 0.69783  train_accuracy 0.5733 val_accuracy 0.5205  train_f1 0.5608 val_f1 0.5193\n",
      "Epoch 539, train_loss: 0.67577 val_loss 0.69783  train_accuracy 0.5733 val_accuracy 0.5220  train_f1 0.5618 val_f1 0.5069\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-540-0.6978.hdf5\n",
      "Epoch 540, train_loss: 0.67575 val_loss 0.69787  train_accuracy 0.5736 val_accuracy 0.5208  train_f1 0.5607 val_f1 0.5184\n",
      "Epoch 541, train_loss: 0.67570 val_loss 0.69794  train_accuracy 0.5736 val_accuracy 0.5205  train_f1 0.5616 val_f1 0.5150\n",
      "Epoch 542, train_loss: 0.67567 val_loss 0.69788  train_accuracy 0.5736 val_accuracy 0.5209  train_f1 0.5624 val_f1 0.5127\n",
      "Epoch 543, train_loss: 0.67563 val_loss 0.69798  train_accuracy 0.5737 val_accuracy 0.5202  train_f1 0.5615 val_f1 0.5211\n",
      "Epoch 544, train_loss: 0.67562 val_loss 0.69791  train_accuracy 0.5736 val_accuracy 0.5223  train_f1 0.5635 val_f1 0.5078\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-545-0.6979.hdf5\n",
      "Epoch 545, train_loss: 0.67560 val_loss 0.69800  train_accuracy 0.5738 val_accuracy 0.5208  train_f1 0.5609 val_f1 0.5124\n",
      "Epoch 546, train_loss: 0.67555 val_loss 0.69803  train_accuracy 0.5736 val_accuracy 0.5219  train_f1 0.5623 val_f1 0.5123\n",
      "Epoch 547, train_loss: 0.67550 val_loss 0.69796  train_accuracy 0.5740 val_accuracy 0.5215  train_f1 0.5603 val_f1 0.5142\n",
      "Epoch 548, train_loss: 0.67546 val_loss 0.69806  train_accuracy 0.5741 val_accuracy 0.5197  train_f1 0.5635 val_f1 0.5195\n",
      "Epoch 549, train_loss: 0.67545 val_loss 0.69803  train_accuracy 0.5741 val_accuracy 0.5220  train_f1 0.5633 val_f1 0.5049\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-550-0.6980.hdf5\n",
      "Epoch 550, train_loss: 0.67541 val_loss 0.69820  train_accuracy 0.5741 val_accuracy 0.5200  train_f1 0.5613 val_f1 0.5238\n",
      "Epoch 551, train_loss: 0.67536 val_loss 0.69801  train_accuracy 0.5742 val_accuracy 0.5210  train_f1 0.5636 val_f1 0.5104\n",
      "Epoch 552, train_loss: 0.67533 val_loss 0.69818  train_accuracy 0.5743 val_accuracy 0.5207  train_f1 0.5613 val_f1 0.5224\n",
      "Epoch 553, train_loss: 0.67530 val_loss 0.69809  train_accuracy 0.5744 val_accuracy 0.5207  train_f1 0.5647 val_f1 0.5139\n",
      "Epoch 554, train_loss: 0.67526 val_loss 0.69810  train_accuracy 0.5744 val_accuracy 0.5209  train_f1 0.5608 val_f1 0.5126\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-555-0.6981.hdf5\n",
      "Epoch 555, train_loss: 0.67522 val_loss 0.69813  train_accuracy 0.5747 val_accuracy 0.5201  train_f1 0.5636 val_f1 0.5143\n",
      "Epoch 556, train_loss: 0.67520 val_loss 0.69825  train_accuracy 0.5747 val_accuracy 0.5210  train_f1 0.5612 val_f1 0.5217\n",
      "Epoch 557, train_loss: 0.67520 val_loss 0.69817  train_accuracy 0.5745 val_accuracy 0.5206  train_f1 0.5654 val_f1 0.5176\n",
      "Epoch 558, train_loss: 0.67511 val_loss 0.69820  train_accuracy 0.5749 val_accuracy 0.5201  train_f1 0.5621 val_f1 0.5115\n",
      "Epoch 559, train_loss: 0.67507 val_loss 0.69832  train_accuracy 0.5748 val_accuracy 0.5200  train_f1 0.5630 val_f1 0.5231\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-560-0.6983.hdf5\n",
      "Epoch 560, train_loss: 0.67505 val_loss 0.69825  train_accuracy 0.5749 val_accuracy 0.5209  train_f1 0.5642 val_f1 0.5196\n",
      "Epoch 561, train_loss: 0.67500 val_loss 0.69828  train_accuracy 0.5750 val_accuracy 0.5213  train_f1 0.5637 val_f1 0.5111\n",
      "Epoch 562, train_loss: 0.67498 val_loss 0.69830  train_accuracy 0.5751 val_accuracy 0.5207  train_f1 0.5627 val_f1 0.5156\n",
      "Epoch 563, train_loss: 0.67493 val_loss 0.69824  train_accuracy 0.5752 val_accuracy 0.5216  train_f1 0.5642 val_f1 0.5069\n",
      "Epoch 564, train_loss: 0.67491 val_loss 0.69831  train_accuracy 0.5752 val_accuracy 0.5210  train_f1 0.5630 val_f1 0.5138\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-565-0.6983.hdf5\n",
      "Epoch 565, train_loss: 0.67487 val_loss 0.69836  train_accuracy 0.5754 val_accuracy 0.5209  train_f1 0.5632 val_f1 0.5175\n",
      "Epoch 566, train_loss: 0.67483 val_loss 0.69831  train_accuracy 0.5754 val_accuracy 0.5206  train_f1 0.5645 val_f1 0.5106\n",
      "Epoch 567, train_loss: 0.67480 val_loss 0.69833  train_accuracy 0.5755 val_accuracy 0.5205  train_f1 0.5637 val_f1 0.5125\n",
      "Epoch 568, train_loss: 0.67477 val_loss 0.69842  train_accuracy 0.5756 val_accuracy 0.5212  train_f1 0.5640 val_f1 0.5144\n",
      "Epoch 569, train_loss: 0.67473 val_loss 0.69844  train_accuracy 0.5754 val_accuracy 0.5206  train_f1 0.5632 val_f1 0.5217\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-570-0.6984.hdf5\n",
      "Epoch 570, train_loss: 0.67469 val_loss 0.69837  train_accuracy 0.5757 val_accuracy 0.5212  train_f1 0.5658 val_f1 0.5102\n",
      "Epoch 571, train_loss: 0.67467 val_loss 0.69854  train_accuracy 0.5757 val_accuracy 0.5208  train_f1 0.5616 val_f1 0.5249\n",
      "Epoch 572, train_loss: 0.67465 val_loss 0.69843  train_accuracy 0.5758 val_accuracy 0.5210  train_f1 0.5678 val_f1 0.5070\n",
      "Epoch 573, train_loss: 0.67459 val_loss 0.69851  train_accuracy 0.5758 val_accuracy 0.5214  train_f1 0.5629 val_f1 0.5202\n",
      "Epoch 574, train_loss: 0.67457 val_loss 0.69848  train_accuracy 0.5759 val_accuracy 0.5201  train_f1 0.5660 val_f1 0.5119\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-575-0.6985.hdf5\n",
      "Epoch 575, train_loss: 0.67452 val_loss 0.69863  train_accuracy 0.5761 val_accuracy 0.5209  train_f1 0.5638 val_f1 0.5268\n",
      "Epoch 576, train_loss: 0.67452 val_loss 0.69856  train_accuracy 0.5760 val_accuracy 0.5216  train_f1 0.5674 val_f1 0.4989\n",
      "Epoch 577, train_loss: 0.67456 val_loss 0.69880  train_accuracy 0.5758 val_accuracy 0.5208  train_f1 0.5597 val_f1 0.5319\n",
      "Epoch 578, train_loss: 0.67450 val_loss 0.69861  train_accuracy 0.5760 val_accuracy 0.5218  train_f1 0.5678 val_f1 0.4917\n",
      "Epoch 579, train_loss: 0.67452 val_loss 0.69872  train_accuracy 0.5759 val_accuracy 0.5205  train_f1 0.5597 val_f1 0.5262\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-580-0.6987.hdf5\n",
      "Epoch 580, train_loss: 0.67436 val_loss 0.69861  train_accuracy 0.5764 val_accuracy 0.5206  train_f1 0.5670 val_f1 0.5167\n",
      "Epoch 581, train_loss: 0.67431 val_loss 0.69862  train_accuracy 0.5765 val_accuracy 0.5205  train_f1 0.5651 val_f1 0.5169\n",
      "Epoch 582, train_loss: 0.67429 val_loss 0.69860  train_accuracy 0.5765 val_accuracy 0.5209  train_f1 0.5668 val_f1 0.5116\n",
      "Epoch 583, train_loss: 0.67425 val_loss 0.69880  train_accuracy 0.5765 val_accuracy 0.5200  train_f1 0.5651 val_f1 0.5150\n",
      "Epoch 584, train_loss: 0.67421 val_loss 0.69875  train_accuracy 0.5765 val_accuracy 0.5201  train_f1 0.5634 val_f1 0.5234\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-585-0.6988.hdf5\n",
      "Epoch 585, train_loss: 0.67418 val_loss 0.69869  train_accuracy 0.5767 val_accuracy 0.5208  train_f1 0.5679 val_f1 0.5125\n",
      "Epoch 586, train_loss: 0.67413 val_loss 0.69880  train_accuracy 0.5767 val_accuracy 0.5206  train_f1 0.5640 val_f1 0.5157\n",
      "Epoch 587, train_loss: 0.67409 val_loss 0.69874  train_accuracy 0.5768 val_accuracy 0.5203  train_f1 0.5667 val_f1 0.5123\n",
      "Epoch 588, train_loss: 0.67406 val_loss 0.69884  train_accuracy 0.5768 val_accuracy 0.5209  train_f1 0.5652 val_f1 0.5181\n",
      "Epoch 589, train_loss: 0.67403 val_loss 0.69879  train_accuracy 0.5771 val_accuracy 0.5196  train_f1 0.5670 val_f1 0.5186\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-590-0.6988.hdf5\n",
      "Epoch 590, train_loss: 0.67399 val_loss 0.69879  train_accuracy 0.5770 val_accuracy 0.5207  train_f1 0.5661 val_f1 0.5069\n",
      "Epoch 591, train_loss: 0.67398 val_loss 0.69885  train_accuracy 0.5771 val_accuracy 0.5199  train_f1 0.5640 val_f1 0.5205\n",
      "Epoch 592, train_loss: 0.67392 val_loss 0.69880  train_accuracy 0.5771 val_accuracy 0.5207  train_f1 0.5673 val_f1 0.5112\n",
      "Epoch 593, train_loss: 0.67388 val_loss 0.69884  train_accuracy 0.5772 val_accuracy 0.5203  train_f1 0.5663 val_f1 0.5152\n",
      "Epoch 594, train_loss: 0.67385 val_loss 0.69889  train_accuracy 0.5772 val_accuracy 0.5210  train_f1 0.5658 val_f1 0.5136\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-595-0.6989.hdf5\n",
      "Epoch 595, train_loss: 0.67381 val_loss 0.69892  train_accuracy 0.5774 val_accuracy 0.5205  train_f1 0.5662 val_f1 0.5171\n",
      "Epoch 596, train_loss: 0.67377 val_loss 0.69886  train_accuracy 0.5775 val_accuracy 0.5205  train_f1 0.5657 val_f1 0.5122\n",
      "Epoch 597, train_loss: 0.67374 val_loss 0.69897  train_accuracy 0.5774 val_accuracy 0.5206  train_f1 0.5670 val_f1 0.5153\n",
      "Epoch 598, train_loss: 0.67370 val_loss 0.69905  train_accuracy 0.5777 val_accuracy 0.5200  train_f1 0.5664 val_f1 0.5200\n",
      "Epoch 599, train_loss: 0.67367 val_loss 0.69897  train_accuracy 0.5775 val_accuracy 0.5207  train_f1 0.5663 val_f1 0.5141\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-600-0.6990.hdf5\n",
      "Epoch 600, train_loss: 0.67364 val_loss 0.69902  train_accuracy 0.5778 val_accuracy 0.5201  train_f1 0.5672 val_f1 0.5170\n",
      "Epoch 601, train_loss: 0.67360 val_loss 0.69901  train_accuracy 0.5778 val_accuracy 0.5211  train_f1 0.5672 val_f1 0.5130\n",
      "Epoch 602, train_loss: 0.67356 val_loss 0.69907  train_accuracy 0.5778 val_accuracy 0.5200  train_f1 0.5660 val_f1 0.5198\n",
      "Epoch 603, train_loss: 0.67353 val_loss 0.69903  train_accuracy 0.5780 val_accuracy 0.5210  train_f1 0.5686 val_f1 0.5099\n",
      "Epoch 604, train_loss: 0.67350 val_loss 0.69908  train_accuracy 0.5779 val_accuracy 0.5202  train_f1 0.5656 val_f1 0.5120\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-605-0.6991.hdf5\n",
      "Epoch 605, train_loss: 0.67345 val_loss 0.69908  train_accuracy 0.5781 val_accuracy 0.5201  train_f1 0.5679 val_f1 0.5126\n",
      "Epoch 606, train_loss: 0.67342 val_loss 0.69909  train_accuracy 0.5782 val_accuracy 0.5209  train_f1 0.5665 val_f1 0.5139\n",
      "Epoch 607, train_loss: 0.67339 val_loss 0.69909  train_accuracy 0.5781 val_accuracy 0.5194  train_f1 0.5663 val_f1 0.5186\n",
      "Epoch 608, train_loss: 0.67337 val_loss 0.69916  train_accuracy 0.5783 val_accuracy 0.5210  train_f1 0.5698 val_f1 0.5088\n",
      "Epoch 609, train_loss: 0.67333 val_loss 0.69913  train_accuracy 0.5783 val_accuracy 0.5209  train_f1 0.5665 val_f1 0.5124\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-610-0.6991.hdf5\n",
      "Epoch 610, train_loss: 0.67328 val_loss 0.69920  train_accuracy 0.5784 val_accuracy 0.5210  train_f1 0.5659 val_f1 0.5103\n",
      "Epoch 611, train_loss: 0.67324 val_loss 0.69930  train_accuracy 0.5785 val_accuracy 0.5204  train_f1 0.5682 val_f1 0.5168\n",
      "Epoch 612, train_loss: 0.67320 val_loss 0.69919  train_accuracy 0.5786 val_accuracy 0.5208  train_f1 0.5664 val_f1 0.5160\n",
      "Epoch 613, train_loss: 0.67317 val_loss 0.69929  train_accuracy 0.5787 val_accuracy 0.5212  train_f1 0.5692 val_f1 0.5143\n",
      "Epoch 614, train_loss: 0.67312 val_loss 0.69925  train_accuracy 0.5787 val_accuracy 0.5212  train_f1 0.5671 val_f1 0.5136\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-615-0.6992.hdf5\n",
      "Epoch 615, train_loss: 0.67310 val_loss 0.69932  train_accuracy 0.5787 val_accuracy 0.5208  train_f1 0.5690 val_f1 0.5042\n",
      "Epoch 616, train_loss: 0.67306 val_loss 0.69936  train_accuracy 0.5787 val_accuracy 0.5211  train_f1 0.5664 val_f1 0.5164\n",
      "Epoch 617, train_loss: 0.67302 val_loss 0.69939  train_accuracy 0.5790 val_accuracy 0.5201  train_f1 0.5681 val_f1 0.5119\n",
      "Epoch 618, train_loss: 0.67298 val_loss 0.69940  train_accuracy 0.5790 val_accuracy 0.5206  train_f1 0.5679 val_f1 0.5051\n",
      "Epoch 619, train_loss: 0.67296 val_loss 0.69947  train_accuracy 0.5790 val_accuracy 0.5199  train_f1 0.5669 val_f1 0.5227\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-620-0.6995.hdf5\n",
      "Epoch 620, train_loss: 0.67291 val_loss 0.69946  train_accuracy 0.5791 val_accuracy 0.5210  train_f1 0.5695 val_f1 0.5041\n",
      "Epoch 621, train_loss: 0.67288 val_loss 0.69948  train_accuracy 0.5791 val_accuracy 0.5199  train_f1 0.5661 val_f1 0.5213\n",
      "Epoch 622, train_loss: 0.67285 val_loss 0.69945  train_accuracy 0.5793 val_accuracy 0.5204  train_f1 0.5694 val_f1 0.5138\n",
      "Epoch 623, train_loss: 0.67281 val_loss 0.69955  train_accuracy 0.5794 val_accuracy 0.5202  train_f1 0.5686 val_f1 0.5201\n",
      "Epoch 624, train_loss: 0.67277 val_loss 0.69947  train_accuracy 0.5794 val_accuracy 0.5213  train_f1 0.5687 val_f1 0.5112\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-625-0.6995.hdf5\n",
      "Epoch 625, train_loss: 0.67274 val_loss 0.69957  train_accuracy 0.5795 val_accuracy 0.5202  train_f1 0.5678 val_f1 0.5197\n",
      "Epoch 626, train_loss: 0.67271 val_loss 0.69949  train_accuracy 0.5795 val_accuracy 0.5207  train_f1 0.5690 val_f1 0.5138\n",
      "Epoch 627, train_loss: 0.67265 val_loss 0.69961  train_accuracy 0.5798 val_accuracy 0.5209  train_f1 0.5696 val_f1 0.5118\n",
      "Epoch 628, train_loss: 0.67262 val_loss 0.69953  train_accuracy 0.5797 val_accuracy 0.5204  train_f1 0.5688 val_f1 0.5117\n",
      "Epoch 629, train_loss: 0.67259 val_loss 0.69980  train_accuracy 0.5798 val_accuracy 0.5198  train_f1 0.5680 val_f1 0.5243\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-630-0.6998.hdf5\n",
      "Epoch 630, train_loss: 0.67256 val_loss 0.69962  train_accuracy 0.5799 val_accuracy 0.5211  train_f1 0.5707 val_f1 0.5144\n",
      "Epoch 631, train_loss: 0.67250 val_loss 0.69966  train_accuracy 0.5799 val_accuracy 0.5208  train_f1 0.5687 val_f1 0.5107\n",
      "Epoch 632, train_loss: 0.67246 val_loss 0.69965  train_accuracy 0.5801 val_accuracy 0.5205  train_f1 0.5695 val_f1 0.5101\n",
      "Epoch 633, train_loss: 0.67243 val_loss 0.69965  train_accuracy 0.5801 val_accuracy 0.5205  train_f1 0.5690 val_f1 0.5113\n",
      "Epoch 634, train_loss: 0.67242 val_loss 0.69995  train_accuracy 0.5802 val_accuracy 0.5186  train_f1 0.5676 val_f1 0.5323\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-635-0.6999.hdf5\n",
      "Epoch 635, train_loss: 0.67241 val_loss 0.69970  train_accuracy 0.5802 val_accuracy 0.5207  train_f1 0.5722 val_f1 0.5065\n",
      "Epoch 636, train_loss: 0.67234 val_loss 0.69983  train_accuracy 0.5804 val_accuracy 0.5204  train_f1 0.5696 val_f1 0.5084\n",
      "Epoch 637, train_loss: 0.67229 val_loss 0.69977  train_accuracy 0.5804 val_accuracy 0.5197  train_f1 0.5683 val_f1 0.5156\n",
      "Epoch 638, train_loss: 0.67226 val_loss 0.69988  train_accuracy 0.5804 val_accuracy 0.5194  train_f1 0.5691 val_f1 0.5178\n",
      "Epoch 639, train_loss: 0.67222 val_loss 0.70005  train_accuracy 0.5806 val_accuracy 0.5208  train_f1 0.5713 val_f1 0.5176\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-640-0.7001.hdf5\n",
      "Epoch 640, train_loss: 0.67219 val_loss 0.69982  train_accuracy 0.5805 val_accuracy 0.5201  train_f1 0.5688 val_f1 0.5180\n",
      "Epoch 641, train_loss: 0.67213 val_loss 0.69988  train_accuracy 0.5805 val_accuracy 0.5196  train_f1 0.5708 val_f1 0.5123\n",
      "Epoch 642, train_loss: 0.67210 val_loss 0.70001  train_accuracy 0.5807 val_accuracy 0.5207  train_f1 0.5690 val_f1 0.5236\n",
      "Epoch 643, train_loss: 0.67208 val_loss 0.69993  train_accuracy 0.5808 val_accuracy 0.5209  train_f1 0.5701 val_f1 0.5119\n",
      "Epoch 644, train_loss: 0.67203 val_loss 0.70000  train_accuracy 0.5809 val_accuracy 0.5201  train_f1 0.5714 val_f1 0.5159\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-645-0.7000.hdf5\n",
      "Epoch 645, train_loss: 0.67204 val_loss 0.70000  train_accuracy 0.5807 val_accuracy 0.5197  train_f1 0.5684 val_f1 0.5166\n",
      "Epoch 646, train_loss: 0.67199 val_loss 0.70006  train_accuracy 0.5810 val_accuracy 0.5209  train_f1 0.5728 val_f1 0.5074\n",
      "Epoch 647, train_loss: 0.67194 val_loss 0.70024  train_accuracy 0.5811 val_accuracy 0.5189  train_f1 0.5686 val_f1 0.5297\n",
      "Epoch 648, train_loss: 0.67195 val_loss 0.69998  train_accuracy 0.5810 val_accuracy 0.5199  train_f1 0.5708 val_f1 0.5149\n",
      "Epoch 649, train_loss: 0.67185 val_loss 0.70011  train_accuracy 0.5812 val_accuracy 0.5201  train_f1 0.5722 val_f1 0.5066\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-650-0.7001.hdf5\n",
      "Epoch 650, train_loss: 0.67183 val_loss 0.70015  train_accuracy 0.5812 val_accuracy 0.5199  train_f1 0.5672 val_f1 0.5235\n",
      "Epoch 651, train_loss: 0.67178 val_loss 0.70010  train_accuracy 0.5814 val_accuracy 0.5199  train_f1 0.5737 val_f1 0.5088\n",
      "Epoch 652, train_loss: 0.67174 val_loss 0.70021  train_accuracy 0.5815 val_accuracy 0.5198  train_f1 0.5709 val_f1 0.5194\n",
      "Epoch 653, train_loss: 0.67169 val_loss 0.70019  train_accuracy 0.5815 val_accuracy 0.5209  train_f1 0.5715 val_f1 0.5168\n",
      "Epoch 654, train_loss: 0.67165 val_loss 0.70017  train_accuracy 0.5817 val_accuracy 0.5198  train_f1 0.5720 val_f1 0.5159\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-655-0.7002.hdf5\n",
      "Epoch 655, train_loss: 0.67162 val_loss 0.70018  train_accuracy 0.5817 val_accuracy 0.5196  train_f1 0.5708 val_f1 0.5136\n",
      "Epoch 656, train_loss: 0.67159 val_loss 0.70029  train_accuracy 0.5816 val_accuracy 0.5200  train_f1 0.5709 val_f1 0.5169\n",
      "Epoch 657, train_loss: 0.67155 val_loss 0.70032  train_accuracy 0.5819 val_accuracy 0.5197  train_f1 0.5734 val_f1 0.5103\n",
      "Epoch 658, train_loss: 0.67151 val_loss 0.70021  train_accuracy 0.5818 val_accuracy 0.5197  train_f1 0.5698 val_f1 0.5127\n",
      "Epoch 659, train_loss: 0.67147 val_loss 0.70028  train_accuracy 0.5820 val_accuracy 0.5193  train_f1 0.5727 val_f1 0.5083\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-660-0.7003.hdf5\n",
      "Epoch 660, train_loss: 0.67145 val_loss 0.70039  train_accuracy 0.5820 val_accuracy 0.5199  train_f1 0.5701 val_f1 0.5164\n",
      "Epoch 661, train_loss: 0.67140 val_loss 0.70030  train_accuracy 0.5822 val_accuracy 0.5191  train_f1 0.5708 val_f1 0.5198\n",
      "Epoch 662, train_loss: 0.67137 val_loss 0.70040  train_accuracy 0.5820 val_accuracy 0.5194  train_f1 0.5737 val_f1 0.5056\n",
      "Epoch 663, train_loss: 0.67132 val_loss 0.70040  train_accuracy 0.5822 val_accuracy 0.5199  train_f1 0.5709 val_f1 0.5166\n",
      "Epoch 664, train_loss: 0.67128 val_loss 0.70048  train_accuracy 0.5823 val_accuracy 0.5194  train_f1 0.5723 val_f1 0.5070\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-665-0.7005.hdf5\n",
      "Epoch 665, train_loss: 0.67125 val_loss 0.70032  train_accuracy 0.5825 val_accuracy 0.5187  train_f1 0.5710 val_f1 0.5124\n",
      "Epoch 666, train_loss: 0.67122 val_loss 0.70051  train_accuracy 0.5824 val_accuracy 0.5196  train_f1 0.5722 val_f1 0.5113\n",
      "Epoch 667, train_loss: 0.67117 val_loss 0.70044  train_accuracy 0.5825 val_accuracy 0.5202  train_f1 0.5721 val_f1 0.5140\n",
      "Epoch 668, train_loss: 0.67115 val_loss 0.70060  train_accuracy 0.5826 val_accuracy 0.5193  train_f1 0.5711 val_f1 0.5222\n",
      "Epoch 669, train_loss: 0.67111 val_loss 0.70053  train_accuracy 0.5825 val_accuracy 0.5193  train_f1 0.5735 val_f1 0.5073\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-670-0.7005.hdf5\n",
      "Epoch 670, train_loss: 0.67106 val_loss 0.70056  train_accuracy 0.5827 val_accuracy 0.5200  train_f1 0.5708 val_f1 0.5152\n",
      "Epoch 671, train_loss: 0.67101 val_loss 0.70060  train_accuracy 0.5828 val_accuracy 0.5191  train_f1 0.5725 val_f1 0.5183\n",
      "Epoch 672, train_loss: 0.67099 val_loss 0.70059  train_accuracy 0.5829 val_accuracy 0.5193  train_f1 0.5735 val_f1 0.5174\n",
      "Epoch 673, train_loss: 0.67097 val_loss 0.70057  train_accuracy 0.5827 val_accuracy 0.5201  train_f1 0.5722 val_f1 0.5116\n",
      "Epoch 674, train_loss: 0.67091 val_loss 0.70068  train_accuracy 0.5830 val_accuracy 0.5205  train_f1 0.5733 val_f1 0.5168\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-675-0.7007.hdf5\n",
      "Epoch 675, train_loss: 0.67088 val_loss 0.70074  train_accuracy 0.5830 val_accuracy 0.5192  train_f1 0.5722 val_f1 0.5248\n",
      "Epoch 676, train_loss: 0.67084 val_loss 0.70065  train_accuracy 0.5830 val_accuracy 0.5186  train_f1 0.5739 val_f1 0.5102\n",
      "Epoch 677, train_loss: 0.67081 val_loss 0.70076  train_accuracy 0.5831 val_accuracy 0.5199  train_f1 0.5731 val_f1 0.5142\n",
      "Epoch 678, train_loss: 0.67076 val_loss 0.70077  train_accuracy 0.5834 val_accuracy 0.5202  train_f1 0.5717 val_f1 0.5194\n",
      "Epoch 679, train_loss: 0.67073 val_loss 0.70086  train_accuracy 0.5834 val_accuracy 0.5192  train_f1 0.5727 val_f1 0.5222\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-680-0.7009.hdf5\n",
      "Epoch 680, train_loss: 0.67071 val_loss 0.70075  train_accuracy 0.5834 val_accuracy 0.5198  train_f1 0.5742 val_f1 0.5023\n",
      "Epoch 681, train_loss: 0.67069 val_loss 0.70072  train_accuracy 0.5834 val_accuracy 0.5197  train_f1 0.5739 val_f1 0.5080\n",
      "Epoch 682, train_loss: 0.67062 val_loss 0.70091  train_accuracy 0.5835 val_accuracy 0.5196  train_f1 0.5722 val_f1 0.5239\n",
      "Epoch 683, train_loss: 0.67061 val_loss 0.70082  train_accuracy 0.5835 val_accuracy 0.5197  train_f1 0.5742 val_f1 0.5066\n",
      "Epoch 684, train_loss: 0.67054 val_loss 0.70099  train_accuracy 0.5835 val_accuracy 0.5195  train_f1 0.5728 val_f1 0.5197\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-685-0.7010.hdf5\n",
      "Epoch 685, train_loss: 0.67051 val_loss 0.70089  train_accuracy 0.5838 val_accuracy 0.5199  train_f1 0.5731 val_f1 0.5196\n",
      "Epoch 686, train_loss: 0.67050 val_loss 0.70091  train_accuracy 0.5838 val_accuracy 0.5193  train_f1 0.5761 val_f1 0.5109\n",
      "Epoch 687, train_loss: 0.67042 val_loss 0.70093  train_accuracy 0.5839 val_accuracy 0.5198  train_f1 0.5740 val_f1 0.5174\n",
      "Epoch 688, train_loss: 0.67038 val_loss 0.70100  train_accuracy 0.5840 val_accuracy 0.5198  train_f1 0.5739 val_f1 0.5186\n",
      "Epoch 689, train_loss: 0.67035 val_loss 0.70091  train_accuracy 0.5841 val_accuracy 0.5198  train_f1 0.5744 val_f1 0.5168\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-690-0.7009.hdf5\n",
      "Epoch 690, train_loss: 0.67031 val_loss 0.70104  train_accuracy 0.5841 val_accuracy 0.5196  train_f1 0.5747 val_f1 0.5167\n",
      "Epoch 691, train_loss: 0.67027 val_loss 0.70104  train_accuracy 0.5842 val_accuracy 0.5194  train_f1 0.5744 val_f1 0.5127\n",
      "Epoch 692, train_loss: 0.67024 val_loss 0.70110  train_accuracy 0.5842 val_accuracy 0.5201  train_f1 0.5738 val_f1 0.5207\n",
      "Epoch 693, train_loss: 0.67020 val_loss 0.70105  train_accuracy 0.5843 val_accuracy 0.5199  train_f1 0.5749 val_f1 0.5167\n",
      "Epoch 694, train_loss: 0.67019 val_loss 0.70116  train_accuracy 0.5844 val_accuracy 0.5201  train_f1 0.5770 val_f1 0.5087\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-695-0.7012.hdf5\n",
      "Epoch 695, train_loss: 0.67013 val_loss 0.70107  train_accuracy 0.5844 val_accuracy 0.5195  train_f1 0.5731 val_f1 0.5121\n",
      "Epoch 696, train_loss: 0.67009 val_loss 0.70137  train_accuracy 0.5845 val_accuracy 0.5193  train_f1 0.5734 val_f1 0.5260\n",
      "Epoch 697, train_loss: 0.67007 val_loss 0.70114  train_accuracy 0.5845 val_accuracy 0.5197  train_f1 0.5767 val_f1 0.5028\n",
      "Epoch 698, train_loss: 0.67005 val_loss 0.70112  train_accuracy 0.5844 val_accuracy 0.5196  train_f1 0.5730 val_f1 0.5113\n",
      "Epoch 699, train_loss: 0.67000 val_loss 0.70152  train_accuracy 0.5845 val_accuracy 0.5193  train_f1 0.5729 val_f1 0.5279\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-700-0.7015.hdf5\n",
      "Epoch 700, train_loss: 0.66997 val_loss 0.70123  train_accuracy 0.5846 val_accuracy 0.5203  train_f1 0.5751 val_f1 0.5045\n",
      "Epoch 701, train_loss: 0.66994 val_loss 0.70127  train_accuracy 0.5848 val_accuracy 0.5192  train_f1 0.5749 val_f1 0.5106\n",
      "Epoch 702, train_loss: 0.66988 val_loss 0.70143  train_accuracy 0.5848 val_accuracy 0.5192  train_f1 0.5724 val_f1 0.5253\n",
      "Epoch 703, train_loss: 0.66986 val_loss 0.70143  train_accuracy 0.5850 val_accuracy 0.5197  train_f1 0.5769 val_f1 0.4939\n",
      "Epoch 704, train_loss: 0.66984 val_loss 0.70163  train_accuracy 0.5849 val_accuracy 0.5184  train_f1 0.5739 val_f1 0.5283\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-705-0.7016.hdf5\n",
      "Epoch 705, train_loss: 0.66978 val_loss 0.70137  train_accuracy 0.5851 val_accuracy 0.5195  train_f1 0.5752 val_f1 0.5132\n",
      "Epoch 706, train_loss: 0.66971 val_loss 0.70141  train_accuracy 0.5851 val_accuracy 0.5190  train_f1 0.5754 val_f1 0.5115\n",
      "Epoch 707, train_loss: 0.66968 val_loss 0.70137  train_accuracy 0.5852 val_accuracy 0.5194  train_f1 0.5753 val_f1 0.5120\n",
      "Epoch 708, train_loss: 0.66964 val_loss 0.70153  train_accuracy 0.5852 val_accuracy 0.5192  train_f1 0.5746 val_f1 0.5193\n",
      "Epoch 709, train_loss: 0.66960 val_loss 0.70145  train_accuracy 0.5854 val_accuracy 0.5195  train_f1 0.5763 val_f1 0.5095\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-710-0.7015.hdf5\n",
      "Epoch 710, train_loss: 0.66956 val_loss 0.70149  train_accuracy 0.5855 val_accuracy 0.5195  train_f1 0.5755 val_f1 0.5048\n",
      "Epoch 711, train_loss: 0.66955 val_loss 0.70165  train_accuracy 0.5855 val_accuracy 0.5190  train_f1 0.5746 val_f1 0.5158\n",
      "Epoch 712, train_loss: 0.66950 val_loss 0.70160  train_accuracy 0.5855 val_accuracy 0.5192  train_f1 0.5746 val_f1 0.5214\n",
      "Epoch 713, train_loss: 0.66948 val_loss 0.70162  train_accuracy 0.5855 val_accuracy 0.5189  train_f1 0.5758 val_f1 0.5166\n",
      "Epoch 714, train_loss: 0.66945 val_loss 0.70169  train_accuracy 0.5855 val_accuracy 0.5200  train_f1 0.5774 val_f1 0.5098\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-715-0.7017.hdf5\n",
      "Epoch 715, train_loss: 0.66939 val_loss 0.70168  train_accuracy 0.5859 val_accuracy 0.5194  train_f1 0.5753 val_f1 0.5190\n",
      "Epoch 716, train_loss: 0.66934 val_loss 0.70162  train_accuracy 0.5859 val_accuracy 0.5194  train_f1 0.5768 val_f1 0.5119\n",
      "Epoch 717, train_loss: 0.66930 val_loss 0.70172  train_accuracy 0.5858 val_accuracy 0.5197  train_f1 0.5756 val_f1 0.5183\n",
      "Epoch 718, train_loss: 0.66928 val_loss 0.70175  train_accuracy 0.5860 val_accuracy 0.5196  train_f1 0.5754 val_f1 0.5139\n",
      "Epoch 719, train_loss: 0.66923 val_loss 0.70176  train_accuracy 0.5860 val_accuracy 0.5191  train_f1 0.5772 val_f1 0.5084\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-720-0.7018.hdf5\n",
      "Epoch 720, train_loss: 0.66920 val_loss 0.70187  train_accuracy 0.5860 val_accuracy 0.5189  train_f1 0.5754 val_f1 0.5263\n",
      "Epoch 721, train_loss: 0.66917 val_loss 0.70180  train_accuracy 0.5862 val_accuracy 0.5187  train_f1 0.5772 val_f1 0.5023\n",
      "Epoch 722, train_loss: 0.66913 val_loss 0.70202  train_accuracy 0.5862 val_accuracy 0.5192  train_f1 0.5751 val_f1 0.5234\n",
      "Epoch 723, train_loss: 0.66909 val_loss 0.70189  train_accuracy 0.5862 val_accuracy 0.5190  train_f1 0.5773 val_f1 0.5184\n",
      "Epoch 724, train_loss: 0.66906 val_loss 0.70191  train_accuracy 0.5864 val_accuracy 0.5188  train_f1 0.5786 val_f1 0.5049\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-725-0.7019.hdf5\n",
      "Epoch 725, train_loss: 0.66901 val_loss 0.70189  train_accuracy 0.5864 val_accuracy 0.5198  train_f1 0.5745 val_f1 0.5164\n",
      "Epoch 726, train_loss: 0.66897 val_loss 0.70203  train_accuracy 0.5865 val_accuracy 0.5194  train_f1 0.5763 val_f1 0.5201\n",
      "Epoch 727, train_loss: 0.66893 val_loss 0.70202  train_accuracy 0.5865 val_accuracy 0.5195  train_f1 0.5779 val_f1 0.5134\n",
      "Epoch 728, train_loss: 0.66888 val_loss 0.70197  train_accuracy 0.5866 val_accuracy 0.5184  train_f1 0.5759 val_f1 0.5127\n",
      "Epoch 729, train_loss: 0.66885 val_loss 0.70206  train_accuracy 0.5867 val_accuracy 0.5187  train_f1 0.5786 val_f1 0.5082\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-730-0.7021.hdf5\n",
      "Epoch 730, train_loss: 0.66883 val_loss 0.70194  train_accuracy 0.5866 val_accuracy 0.5189  train_f1 0.5760 val_f1 0.5027\n",
      "Epoch 731, train_loss: 0.66885 val_loss 0.70221  train_accuracy 0.5866 val_accuracy 0.5190  train_f1 0.5752 val_f1 0.5231\n",
      "Epoch 732, train_loss: 0.66879 val_loss 0.70216  train_accuracy 0.5868 val_accuracy 0.5185  train_f1 0.5777 val_f1 0.5205\n",
      "Epoch 733, train_loss: 0.66873 val_loss 0.70209  train_accuracy 0.5869 val_accuracy 0.5193  train_f1 0.5787 val_f1 0.5060\n",
      "Epoch 734, train_loss: 0.66870 val_loss 0.70219  train_accuracy 0.5870 val_accuracy 0.5185  train_f1 0.5772 val_f1 0.5162\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-735-0.7022.hdf5\n",
      "Epoch 735, train_loss: 0.66864 val_loss 0.70218  train_accuracy 0.5870 val_accuracy 0.5197  train_f1 0.5773 val_f1 0.5033\n",
      "Epoch 736, train_loss: 0.66862 val_loss 0.70217  train_accuracy 0.5871 val_accuracy 0.5193  train_f1 0.5760 val_f1 0.5185\n",
      "Epoch 737, train_loss: 0.66855 val_loss 0.70226  train_accuracy 0.5872 val_accuracy 0.5185  train_f1 0.5780 val_f1 0.5051\n",
      "Epoch 738, train_loss: 0.66853 val_loss 0.70242  train_accuracy 0.5873 val_accuracy 0.5186  train_f1 0.5769 val_f1 0.5269\n",
      "Epoch 739, train_loss: 0.66850 val_loss 0.70230  train_accuracy 0.5874 val_accuracy 0.5184  train_f1 0.5795 val_f1 0.5021\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-740-0.7023.hdf5\n",
      "Epoch 740, train_loss: 0.66846 val_loss 0.70240  train_accuracy 0.5873 val_accuracy 0.5184  train_f1 0.5763 val_f1 0.5239\n",
      "Epoch 741, train_loss: 0.66843 val_loss 0.70234  train_accuracy 0.5874 val_accuracy 0.5190  train_f1 0.5772 val_f1 0.5168\n",
      "Epoch 742, train_loss: 0.66838 val_loss 0.70234  train_accuracy 0.5875 val_accuracy 0.5197  train_f1 0.5788 val_f1 0.5148\n",
      "Epoch 743, train_loss: 0.66834 val_loss 0.70248  train_accuracy 0.5876 val_accuracy 0.5190  train_f1 0.5780 val_f1 0.5166\n",
      "Epoch 744, train_loss: 0.66833 val_loss 0.70247  train_accuracy 0.5876 val_accuracy 0.5189  train_f1 0.5786 val_f1 0.5188\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-745-0.7025.hdf5\n",
      "Epoch 745, train_loss: 0.66827 val_loss 0.70236  train_accuracy 0.5877 val_accuracy 0.5187  train_f1 0.5790 val_f1 0.5043\n",
      "Epoch 746, train_loss: 0.66827 val_loss 0.70271  train_accuracy 0.5874 val_accuracy 0.5186  train_f1 0.5771 val_f1 0.5280\n",
      "Epoch 747, train_loss: 0.66825 val_loss 0.70258  train_accuracy 0.5878 val_accuracy 0.5189  train_f1 0.5785 val_f1 0.5191\n",
      "Epoch 748, train_loss: 0.66819 val_loss 0.70250  train_accuracy 0.5879 val_accuracy 0.5190  train_f1 0.5800 val_f1 0.5086\n",
      "Epoch 749, train_loss: 0.66814 val_loss 0.70245  train_accuracy 0.5879 val_accuracy 0.5183  train_f1 0.5758 val_f1 0.5184\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-750-0.7025.hdf5\n",
      "Epoch 750, train_loss: 0.66809 val_loss 0.70274  train_accuracy 0.5881 val_accuracy 0.5187  train_f1 0.5809 val_f1 0.5148\n",
      "Epoch 751, train_loss: 0.66805 val_loss 0.70252  train_accuracy 0.5881 val_accuracy 0.5186  train_f1 0.5776 val_f1 0.5159\n",
      "Epoch 752, train_loss: 0.66801 val_loss 0.70264  train_accuracy 0.5882 val_accuracy 0.5184  train_f1 0.5805 val_f1 0.5132\n",
      "Epoch 753, train_loss: 0.66798 val_loss 0.70273  train_accuracy 0.5882 val_accuracy 0.5186  train_f1 0.5787 val_f1 0.5210\n",
      "Epoch 754, train_loss: 0.66795 val_loss 0.70272  train_accuracy 0.5883 val_accuracy 0.5187  train_f1 0.5802 val_f1 0.5057\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-755-0.7027.hdf5\n",
      "Epoch 755, train_loss: 0.66796 val_loss 0.70277  train_accuracy 0.5883 val_accuracy 0.5187  train_f1 0.5787 val_f1 0.5160\n",
      "Epoch 756, train_loss: 0.66786 val_loss 0.70288  train_accuracy 0.5884 val_accuracy 0.5187  train_f1 0.5772 val_f1 0.5246\n",
      "Epoch 757, train_loss: 0.66784 val_loss 0.70288  train_accuracy 0.5884 val_accuracy 0.5186  train_f1 0.5794 val_f1 0.5251\n",
      "Epoch 758, train_loss: 0.66782 val_loss 0.70287  train_accuracy 0.5883 val_accuracy 0.5183  train_f1 0.5815 val_f1 0.4956\n",
      "Epoch 759, train_loss: 0.66779 val_loss 0.70283  train_accuracy 0.5885 val_accuracy 0.5187  train_f1 0.5764 val_f1 0.5203\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-760-0.7028.hdf5\n",
      "Epoch 760, train_loss: 0.66772 val_loss 0.70308  train_accuracy 0.5886 val_accuracy 0.5184  train_f1 0.5790 val_f1 0.5231\n",
      "Epoch 761, train_loss: 0.66769 val_loss 0.70281  train_accuracy 0.5887 val_accuracy 0.5182  train_f1 0.5804 val_f1 0.5160\n",
      "Epoch 762, train_loss: 0.66764 val_loss 0.70291  train_accuracy 0.5889 val_accuracy 0.5186  train_f1 0.5804 val_f1 0.5117\n",
      "Epoch 763, train_loss: 0.66762 val_loss 0.70288  train_accuracy 0.5889 val_accuracy 0.5191  train_f1 0.5791 val_f1 0.5150\n",
      "Epoch 764, train_loss: 0.66758 val_loss 0.70301  train_accuracy 0.5890 val_accuracy 0.5185  train_f1 0.5795 val_f1 0.5181\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-765-0.7030.hdf5\n",
      "Epoch 765, train_loss: 0.66755 val_loss 0.70302  train_accuracy 0.5889 val_accuracy 0.5184  train_f1 0.5819 val_f1 0.5007\n",
      "Epoch 766, train_loss: 0.66754 val_loss 0.70296  train_accuracy 0.5888 val_accuracy 0.5188  train_f1 0.5776 val_f1 0.5174\n",
      "Epoch 767, train_loss: 0.66746 val_loss 0.70310  train_accuracy 0.5891 val_accuracy 0.5188  train_f1 0.5798 val_f1 0.5147\n",
      "Epoch 768, train_loss: 0.66744 val_loss 0.70336  train_accuracy 0.5891 val_accuracy 0.5183  train_f1 0.5778 val_f1 0.5336\n",
      "Epoch 769, train_loss: 0.66745 val_loss 0.70305  train_accuracy 0.5891 val_accuracy 0.5180  train_f1 0.5827 val_f1 0.5043\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-770-0.7031.hdf5\n",
      "Epoch 770, train_loss: 0.66737 val_loss 0.70309  train_accuracy 0.5892 val_accuracy 0.5179  train_f1 0.5804 val_f1 0.5091\n",
      "Epoch 771, train_loss: 0.66735 val_loss 0.70320  train_accuracy 0.5892 val_accuracy 0.5184  train_f1 0.5782 val_f1 0.5220\n",
      "Epoch 772, train_loss: 0.66728 val_loss 0.70313  train_accuracy 0.5895 val_accuracy 0.5181  train_f1 0.5815 val_f1 0.5100\n",
      "Epoch 773, train_loss: 0.66725 val_loss 0.70349  train_accuracy 0.5894 val_accuracy 0.5181  train_f1 0.5785 val_f1 0.5278\n",
      "Epoch 774, train_loss: 0.66724 val_loss 0.70319  train_accuracy 0.5893 val_accuracy 0.5182  train_f1 0.5808 val_f1 0.5096\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-775-0.7032.hdf5\n",
      "Epoch 775, train_loss: 0.66720 val_loss 0.70325  train_accuracy 0.5895 val_accuracy 0.5175  train_f1 0.5818 val_f1 0.4966\n",
      "Epoch 776, train_loss: 0.66719 val_loss 0.70323  train_accuracy 0.5895 val_accuracy 0.5186  train_f1 0.5777 val_f1 0.5168\n",
      "Epoch 777, train_loss: 0.66711 val_loss 0.70339  train_accuracy 0.5897 val_accuracy 0.5188  train_f1 0.5797 val_f1 0.5193\n",
      "Epoch 778, train_loss: 0.66706 val_loss 0.70340  train_accuracy 0.5899 val_accuracy 0.5184  train_f1 0.5810 val_f1 0.5132\n",
      "Epoch 779, train_loss: 0.66705 val_loss 0.70331  train_accuracy 0.5898 val_accuracy 0.5181  train_f1 0.5818 val_f1 0.5065\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-780-0.7033.hdf5\n",
      "Epoch 780, train_loss: 0.66699 val_loss 0.70331  train_accuracy 0.5898 val_accuracy 0.5183  train_f1 0.5804 val_f1 0.5062\n",
      "Epoch 781, train_loss: 0.66698 val_loss 0.70347  train_accuracy 0.5899 val_accuracy 0.5185  train_f1 0.5798 val_f1 0.5154\n",
      "Epoch 782, train_loss: 0.66692 val_loss 0.70341  train_accuracy 0.5899 val_accuracy 0.5178  train_f1 0.5803 val_f1 0.5120\n",
      "Epoch 783, train_loss: 0.66688 val_loss 0.70344  train_accuracy 0.5901 val_accuracy 0.5182  train_f1 0.5819 val_f1 0.5023\n",
      "Epoch 784, train_loss: 0.66690 val_loss 0.70351  train_accuracy 0.5899 val_accuracy 0.5178  train_f1 0.5787 val_f1 0.5174\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-785-0.7035.hdf5\n",
      "Epoch 785, train_loss: 0.66685 val_loss 0.70382  train_accuracy 0.5902 val_accuracy 0.5190  train_f1 0.5798 val_f1 0.5272\n",
      "Epoch 786, train_loss: 0.66685 val_loss 0.70355  train_accuracy 0.5901 val_accuracy 0.5173  train_f1 0.5821 val_f1 0.5120\n",
      "Epoch 787, train_loss: 0.66676 val_loss 0.70354  train_accuracy 0.5903 val_accuracy 0.5180  train_f1 0.5820 val_f1 0.5044\n",
      "Epoch 788, train_loss: 0.66676 val_loss 0.70379  train_accuracy 0.5902 val_accuracy 0.5179  train_f1 0.5791 val_f1 0.5269\n",
      "Epoch 789, train_loss: 0.66671 val_loss 0.70369  train_accuracy 0.5903 val_accuracy 0.5178  train_f1 0.5827 val_f1 0.5076\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-790-0.7037.hdf5\n",
      "Epoch 790, train_loss: 0.66664 val_loss 0.70373  train_accuracy 0.5904 val_accuracy 0.5183  train_f1 0.5811 val_f1 0.5130\n",
      "Epoch 791, train_loss: 0.66661 val_loss 0.70388  train_accuracy 0.5903 val_accuracy 0.5180  train_f1 0.5794 val_f1 0.5258\n",
      "Epoch 792, train_loss: 0.66661 val_loss 0.70370  train_accuracy 0.5906 val_accuracy 0.5182  train_f1 0.5840 val_f1 0.5117\n",
      "Epoch 793, train_loss: 0.66654 val_loss 0.70367  train_accuracy 0.5905 val_accuracy 0.5181  train_f1 0.5818 val_f1 0.5145\n",
      "Epoch 794, train_loss: 0.66649 val_loss 0.70384  train_accuracy 0.5908 val_accuracy 0.5176  train_f1 0.5818 val_f1 0.5043\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-795-0.7038.hdf5\n",
      "Epoch 795, train_loss: 0.66648 val_loss 0.70383  train_accuracy 0.5907 val_accuracy 0.5187  train_f1 0.5813 val_f1 0.5042\n",
      "Epoch 796, train_loss: 0.66649 val_loss 0.70398  train_accuracy 0.5906 val_accuracy 0.5178  train_f1 0.5783 val_f1 0.5251\n",
      "Epoch 797, train_loss: 0.66641 val_loss 0.70385  train_accuracy 0.5908 val_accuracy 0.5184  train_f1 0.5834 val_f1 0.5138\n",
      "Epoch 798, train_loss: 0.66638 val_loss 0.70388  train_accuracy 0.5908 val_accuracy 0.5180  train_f1 0.5834 val_f1 0.5111\n",
      "Epoch 799, train_loss: 0.66631 val_loss 0.70394  train_accuracy 0.5910 val_accuracy 0.5180  train_f1 0.5814 val_f1 0.5133\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-800-0.7039.hdf5\n",
      "Epoch 800, train_loss: 0.66628 val_loss 0.70393  train_accuracy 0.5910 val_accuracy 0.5178  train_f1 0.5817 val_f1 0.5192\n",
      "Epoch 801, train_loss: 0.66624 val_loss 0.70410  train_accuracy 0.5911 val_accuracy 0.5180  train_f1 0.5828 val_f1 0.5157\n",
      "Epoch 802, train_loss: 0.66626 val_loss 0.70404  train_accuracy 0.5910 val_accuracy 0.5182  train_f1 0.5838 val_f1 0.5057\n",
      "Epoch 803, train_loss: 0.66618 val_loss 0.70410  train_accuracy 0.5913 val_accuracy 0.5181  train_f1 0.5813 val_f1 0.5196\n",
      "Epoch 804, train_loss: 0.66617 val_loss 0.70407  train_accuracy 0.5913 val_accuracy 0.5183  train_f1 0.5814 val_f1 0.5165\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-805-0.7041.hdf5\n",
      "Epoch 805, train_loss: 0.66611 val_loss 0.70419  train_accuracy 0.5914 val_accuracy 0.5185  train_f1 0.5839 val_f1 0.5153\n",
      "Epoch 806, train_loss: 0.66607 val_loss 0.70412  train_accuracy 0.5914 val_accuracy 0.5180  train_f1 0.5815 val_f1 0.5185\n",
      "Epoch 807, train_loss: 0.66603 val_loss 0.70407  train_accuracy 0.5915 val_accuracy 0.5185  train_f1 0.5831 val_f1 0.5097\n",
      "Epoch 808, train_loss: 0.66602 val_loss 0.70418  train_accuracy 0.5915 val_accuracy 0.5182  train_f1 0.5837 val_f1 0.5092\n",
      "Epoch 809, train_loss: 0.66598 val_loss 0.70428  train_accuracy 0.5916 val_accuracy 0.5178  train_f1 0.5819 val_f1 0.5159\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-810-0.7043.hdf5\n",
      "Epoch 810, train_loss: 0.66594 val_loss 0.70426  train_accuracy 0.5917 val_accuracy 0.5176  train_f1 0.5817 val_f1 0.5233\n",
      "Epoch 811, train_loss: 0.66591 val_loss 0.70425  train_accuracy 0.5917 val_accuracy 0.5178  train_f1 0.5844 val_f1 0.5026\n",
      "Epoch 812, train_loss: 0.66588 val_loss 0.70434  train_accuracy 0.5917 val_accuracy 0.5182  train_f1 0.5807 val_f1 0.5158\n",
      "Epoch 813, train_loss: 0.66582 val_loss 0.70442  train_accuracy 0.5918 val_accuracy 0.5186  train_f1 0.5828 val_f1 0.5168\n",
      "Epoch 814, train_loss: 0.66579 val_loss 0.70431  train_accuracy 0.5920 val_accuracy 0.5182  train_f1 0.5842 val_f1 0.5082\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-815-0.7043.hdf5\n",
      "Epoch 815, train_loss: 0.66576 val_loss 0.70441  train_accuracy 0.5918 val_accuracy 0.5173  train_f1 0.5815 val_f1 0.5216\n",
      "Epoch 816, train_loss: 0.66573 val_loss 0.70439  train_accuracy 0.5920 val_accuracy 0.5182  train_f1 0.5832 val_f1 0.5104\n",
      "Epoch 817, train_loss: 0.66572 val_loss 0.70439  train_accuracy 0.5919 val_accuracy 0.5181  train_f1 0.5837 val_f1 0.5063\n",
      "Epoch 818, train_loss: 0.66568 val_loss 0.70478  train_accuracy 0.5919 val_accuracy 0.5184  train_f1 0.5804 val_f1 0.5332\n",
      "Epoch 819, train_loss: 0.66573 val_loss 0.70454  train_accuracy 0.5919 val_accuracy 0.5186  train_f1 0.5851 val_f1 0.4994\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-820-0.7045.hdf5\n",
      "Epoch 820, train_loss: 0.66567 val_loss 0.70451  train_accuracy 0.5920 val_accuracy 0.5182  train_f1 0.5841 val_f1 0.5127\n",
      "Epoch 821, train_loss: 0.66556 val_loss 0.70474  train_accuracy 0.5923 val_accuracy 0.5171  train_f1 0.5817 val_f1 0.5274\n",
      "Epoch 822, train_loss: 0.66554 val_loss 0.70465  train_accuracy 0.5923 val_accuracy 0.5184  train_f1 0.5854 val_f1 0.4953\n",
      "Epoch 823, train_loss: 0.66556 val_loss 0.70463  train_accuracy 0.5921 val_accuracy 0.5180  train_f1 0.5816 val_f1 0.5165\n",
      "Epoch 824, train_loss: 0.66547 val_loss 0.70476  train_accuracy 0.5924 val_accuracy 0.5169  train_f1 0.5815 val_f1 0.5220\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-825-0.7048.hdf5\n",
      "Epoch 825, train_loss: 0.66544 val_loss 0.70472  train_accuracy 0.5925 val_accuracy 0.5176  train_f1 0.5863 val_f1 0.4974\n",
      "Epoch 826, train_loss: 0.66543 val_loss 0.70499  train_accuracy 0.5923 val_accuracy 0.5175  train_f1 0.5800 val_f1 0.5339\n",
      "Epoch 827, train_loss: 0.66541 val_loss 0.70471  train_accuracy 0.5923 val_accuracy 0.5181  train_f1 0.5856 val_f1 0.5081\n",
      "Epoch 828, train_loss: 0.66532 val_loss 0.70474  train_accuracy 0.5926 val_accuracy 0.5179  train_f1 0.5839 val_f1 0.5092\n",
      "Epoch 829, train_loss: 0.66531 val_loss 0.70489  train_accuracy 0.5924 val_accuracy 0.5174  train_f1 0.5816 val_f1 0.5223\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-830-0.7049.hdf5\n",
      "Epoch 830, train_loss: 0.66525 val_loss 0.70487  train_accuracy 0.5927 val_accuracy 0.5182  train_f1 0.5848 val_f1 0.5141\n",
      "Epoch 831, train_loss: 0.66520 val_loss 0.70482  train_accuracy 0.5927 val_accuracy 0.5177  train_f1 0.5838 val_f1 0.5151\n",
      "Epoch 832, train_loss: 0.66516 val_loss 0.70489  train_accuracy 0.5929 val_accuracy 0.5178  train_f1 0.5850 val_f1 0.5087\n",
      "Epoch 833, train_loss: 0.66514 val_loss 0.70496  train_accuracy 0.5929 val_accuracy 0.5175  train_f1 0.5839 val_f1 0.5203\n",
      "Epoch 834, train_loss: 0.66512 val_loss 0.70480  train_accuracy 0.5928 val_accuracy 0.5174  train_f1 0.5839 val_f1 0.5112\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-835-0.7048.hdf5\n",
      "Epoch 835, train_loss: 0.66509 val_loss 0.70502  train_accuracy 0.5932 val_accuracy 0.5176  train_f1 0.5860 val_f1 0.5135\n",
      "Epoch 836, train_loss: 0.66504 val_loss 0.70518  train_accuracy 0.5929 val_accuracy 0.5169  train_f1 0.5820 val_f1 0.5301\n",
      "Epoch 837, train_loss: 0.66504 val_loss 0.70498  train_accuracy 0.5932 val_accuracy 0.5180  train_f1 0.5871 val_f1 0.5038\n",
      "Epoch 838, train_loss: 0.66499 val_loss 0.70500  train_accuracy 0.5930 val_accuracy 0.5181  train_f1 0.5835 val_f1 0.5108\n",
      "Epoch 839, train_loss: 0.66496 val_loss 0.70531  train_accuracy 0.5931 val_accuracy 0.5173  train_f1 0.5822 val_f1 0.5299\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-840-0.7053.hdf5\n",
      "Epoch 840, train_loss: 0.66498 val_loss 0.70506  train_accuracy 0.5932 val_accuracy 0.5184  train_f1 0.5860 val_f1 0.5005\n",
      "Epoch 841, train_loss: 0.66492 val_loss 0.70519  train_accuracy 0.5933 val_accuracy 0.5181  train_f1 0.5844 val_f1 0.5201\n",
      "Epoch 842, train_loss: 0.66483 val_loss 0.70505  train_accuracy 0.5934 val_accuracy 0.5189  train_f1 0.5849 val_f1 0.5092\n",
      "Epoch 843, train_loss: 0.66479 val_loss 0.70523  train_accuracy 0.5934 val_accuracy 0.5168  train_f1 0.5850 val_f1 0.5249\n",
      "Epoch 844, train_loss: 0.66479 val_loss 0.70513  train_accuracy 0.5935 val_accuracy 0.5182  train_f1 0.5864 val_f1 0.5103\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-845-0.7051.hdf5\n",
      "Epoch 845, train_loss: 0.66473 val_loss 0.70518  train_accuracy 0.5936 val_accuracy 0.5177  train_f1 0.5849 val_f1 0.5129\n",
      "Epoch 846, train_loss: 0.66469 val_loss 0.70527  train_accuracy 0.5937 val_accuracy 0.5172  train_f1 0.5851 val_f1 0.5182\n",
      "Epoch 847, train_loss: 0.66466 val_loss 0.70530  train_accuracy 0.5937 val_accuracy 0.5173  train_f1 0.5854 val_f1 0.5195\n",
      "Epoch 848, train_loss: 0.66465 val_loss 0.70525  train_accuracy 0.5938 val_accuracy 0.5186  train_f1 0.5853 val_f1 0.5114\n",
      "Epoch 849, train_loss: 0.66462 val_loss 0.70527  train_accuracy 0.5937 val_accuracy 0.5179  train_f1 0.5859 val_f1 0.5139\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-850-0.7053.hdf5\n",
      "Epoch 850, train_loss: 0.66457 val_loss 0.70531  train_accuracy 0.5939 val_accuracy 0.5181  train_f1 0.5840 val_f1 0.5156\n",
      "Epoch 851, train_loss: 0.66453 val_loss 0.70545  train_accuracy 0.5940 val_accuracy 0.5177  train_f1 0.5857 val_f1 0.5154\n",
      "Epoch 852, train_loss: 0.66449 val_loss 0.70534  train_accuracy 0.5940 val_accuracy 0.5173  train_f1 0.5858 val_f1 0.5167\n",
      "Epoch 853, train_loss: 0.66445 val_loss 0.70535  train_accuracy 0.5940 val_accuracy 0.5183  train_f1 0.5862 val_f1 0.5092\n",
      "Epoch 854, train_loss: 0.66444 val_loss 0.70555  train_accuracy 0.5940 val_accuracy 0.5171  train_f1 0.5845 val_f1 0.5213\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-855-0.7056.hdf5\n",
      "Epoch 855, train_loss: 0.66441 val_loss 0.70549  train_accuracy 0.5940 val_accuracy 0.5176  train_f1 0.5858 val_f1 0.5189\n",
      "Epoch 856, train_loss: 0.66440 val_loss 0.70552  train_accuracy 0.5941 val_accuracy 0.5174  train_f1 0.5876 val_f1 0.4983\n",
      "Epoch 857, train_loss: 0.66440 val_loss 0.70562  train_accuracy 0.5940 val_accuracy 0.5171  train_f1 0.5825 val_f1 0.5237\n",
      "Epoch 858, train_loss: 0.66436 val_loss 0.70580  train_accuracy 0.5941 val_accuracy 0.5167  train_f1 0.5846 val_f1 0.5239\n",
      "Epoch 859, train_loss: 0.66434 val_loss 0.70554  train_accuracy 0.5942 val_accuracy 0.5190  train_f1 0.5875 val_f1 0.5063\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-860-0.7055.hdf5\n",
      "Epoch 860, train_loss: 0.66429 val_loss 0.70552  train_accuracy 0.5941 val_accuracy 0.5176  train_f1 0.5857 val_f1 0.5126\n",
      "Epoch 861, train_loss: 0.66421 val_loss 0.70554  train_accuracy 0.5945 val_accuracy 0.5174  train_f1 0.5843 val_f1 0.5161\n",
      "Epoch 862, train_loss: 0.66415 val_loss 0.70571  train_accuracy 0.5946 val_accuracy 0.5178  train_f1 0.5860 val_f1 0.5149\n",
      "Epoch 863, train_loss: 0.66412 val_loss 0.70562  train_accuracy 0.5945 val_accuracy 0.5181  train_f1 0.5854 val_f1 0.5130\n",
      "Epoch 864, train_loss: 0.66407 val_loss 0.70566  train_accuracy 0.5947 val_accuracy 0.5183  train_f1 0.5862 val_f1 0.5138\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-865-0.7057.hdf5\n",
      "Epoch 865, train_loss: 0.66405 val_loss 0.70577  train_accuracy 0.5948 val_accuracy 0.5173  train_f1 0.5864 val_f1 0.5180\n",
      "Epoch 866, train_loss: 0.66403 val_loss 0.70576  train_accuracy 0.5947 val_accuracy 0.5188  train_f1 0.5865 val_f1 0.5067\n",
      "Epoch 867, train_loss: 0.66400 val_loss 0.70582  train_accuracy 0.5947 val_accuracy 0.5180  train_f1 0.5856 val_f1 0.5163\n",
      "Epoch 868, train_loss: 0.66395 val_loss 0.70572  train_accuracy 0.5949 val_accuracy 0.5185  train_f1 0.5858 val_f1 0.5147\n",
      "Epoch 869, train_loss: 0.66393 val_loss 0.70592  train_accuracy 0.5947 val_accuracy 0.5180  train_f1 0.5871 val_f1 0.5150\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-870-0.7059.hdf5\n",
      "Epoch 870, train_loss: 0.66391 val_loss 0.70583  train_accuracy 0.5948 val_accuracy 0.5176  train_f1 0.5853 val_f1 0.5184\n",
      "Epoch 871, train_loss: 0.66387 val_loss 0.70585  train_accuracy 0.5949 val_accuracy 0.5183  train_f1 0.5869 val_f1 0.5114\n",
      "Epoch 872, train_loss: 0.66382 val_loss 0.70580  train_accuracy 0.5950 val_accuracy 0.5182  train_f1 0.5870 val_f1 0.5102\n",
      "Epoch 873, train_loss: 0.66382 val_loss 0.70598  train_accuracy 0.5951 val_accuracy 0.5183  train_f1 0.5853 val_f1 0.5129\n",
      "Epoch 874, train_loss: 0.66377 val_loss 0.70599  train_accuracy 0.5951 val_accuracy 0.5178  train_f1 0.5859 val_f1 0.5153\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-875-0.7060.hdf5\n",
      "Epoch 875, train_loss: 0.66373 val_loss 0.70597  train_accuracy 0.5951 val_accuracy 0.5177  train_f1 0.5877 val_f1 0.5066\n",
      "Epoch 876, train_loss: 0.66370 val_loss 0.70599  train_accuracy 0.5949 val_accuracy 0.5183  train_f1 0.5860 val_f1 0.5092\n",
      "Epoch 877, train_loss: 0.66368 val_loss 0.70619  train_accuracy 0.5952 val_accuracy 0.5170  train_f1 0.5851 val_f1 0.5222\n",
      "Epoch 878, train_loss: 0.66362 val_loss 0.70608  train_accuracy 0.5953 val_accuracy 0.5184  train_f1 0.5875 val_f1 0.5142\n",
      "Epoch 879, train_loss: 0.66358 val_loss 0.70613  train_accuracy 0.5953 val_accuracy 0.5176  train_f1 0.5857 val_f1 0.5189\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-880-0.7061.hdf5\n",
      "Epoch 880, train_loss: 0.66357 val_loss 0.70620  train_accuracy 0.5954 val_accuracy 0.5179  train_f1 0.5871 val_f1 0.5159\n",
      "Epoch 881, train_loss: 0.66352 val_loss 0.70617  train_accuracy 0.5954 val_accuracy 0.5182  train_f1 0.5865 val_f1 0.5079\n",
      "Epoch 882, train_loss: 0.66349 val_loss 0.70628  train_accuracy 0.5955 val_accuracy 0.5172  train_f1 0.5867 val_f1 0.5191\n",
      "Epoch 883, train_loss: 0.66348 val_loss 0.70640  train_accuracy 0.5955 val_accuracy 0.5171  train_f1 0.5856 val_f1 0.5276\n",
      "Epoch 884, train_loss: 0.66355 val_loss 0.70631  train_accuracy 0.5954 val_accuracy 0.5176  train_f1 0.5897 val_f1 0.4998\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-885-0.7063.hdf5\n",
      "Epoch 885, train_loss: 0.66342 val_loss 0.70623  train_accuracy 0.5955 val_accuracy 0.5172  train_f1 0.5858 val_f1 0.5172\n",
      "Epoch 886, train_loss: 0.66339 val_loss 0.70659  train_accuracy 0.5957 val_accuracy 0.5167  train_f1 0.5875 val_f1 0.5210\n",
      "Epoch 887, train_loss: 0.66339 val_loss 0.70631  train_accuracy 0.5957 val_accuracy 0.5170  train_f1 0.5879 val_f1 0.5077\n",
      "Epoch 888, train_loss: 0.66331 val_loss 0.70642  train_accuracy 0.5957 val_accuracy 0.5176  train_f1 0.5871 val_f1 0.5151\n",
      "Epoch 889, train_loss: 0.66326 val_loss 0.70657  train_accuracy 0.5959 val_accuracy 0.5180  train_f1 0.5873 val_f1 0.5203\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-890-0.7066.hdf5\n",
      "Epoch 890, train_loss: 0.66324 val_loss 0.70641  train_accuracy 0.5958 val_accuracy 0.5176  train_f1 0.5883 val_f1 0.5090\n",
      "Epoch 891, train_loss: 0.66322 val_loss 0.70644  train_accuracy 0.5960 val_accuracy 0.5171  train_f1 0.5878 val_f1 0.5112\n",
      "Epoch 892, train_loss: 0.66317 val_loss 0.70661  train_accuracy 0.5958 val_accuracy 0.5177  train_f1 0.5856 val_f1 0.5157\n",
      "Epoch 893, train_loss: 0.66312 val_loss 0.70653  train_accuracy 0.5961 val_accuracy 0.5176  train_f1 0.5871 val_f1 0.5157\n",
      "Epoch 894, train_loss: 0.66308 val_loss 0.70673  train_accuracy 0.5960 val_accuracy 0.5170  train_f1 0.5880 val_f1 0.5220\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-895-0.7067.hdf5\n",
      "Epoch 895, train_loss: 0.66308 val_loss 0.70654  train_accuracy 0.5962 val_accuracy 0.5174  train_f1 0.5881 val_f1 0.5101\n",
      "Epoch 896, train_loss: 0.66304 val_loss 0.70662  train_accuracy 0.5962 val_accuracy 0.5175  train_f1 0.5877 val_f1 0.5151\n",
      "Epoch 897, train_loss: 0.66299 val_loss 0.70673  train_accuracy 0.5962 val_accuracy 0.5168  train_f1 0.5883 val_f1 0.5141\n",
      "Epoch 898, train_loss: 0.66297 val_loss 0.70680  train_accuracy 0.5962 val_accuracy 0.5165  train_f1 0.5865 val_f1 0.5248\n",
      "Epoch 899, train_loss: 0.66298 val_loss 0.70676  train_accuracy 0.5964 val_accuracy 0.5178  train_f1 0.5904 val_f1 0.4994\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-900-0.7068.hdf5\n",
      "Epoch 900, train_loss: 0.66297 val_loss 0.70669  train_accuracy 0.5964 val_accuracy 0.5171  train_f1 0.5879 val_f1 0.5151\n",
      "Epoch 901, train_loss: 0.66289 val_loss 0.70700  train_accuracy 0.5965 val_accuracy 0.5168  train_f1 0.5863 val_f1 0.5253\n",
      "Epoch 902, train_loss: 0.66289 val_loss 0.70669  train_accuracy 0.5965 val_accuracy 0.5171  train_f1 0.5896 val_f1 0.5140\n",
      "Epoch 903, train_loss: 0.66282 val_loss 0.70689  train_accuracy 0.5966 val_accuracy 0.5175  train_f1 0.5898 val_f1 0.5079\n",
      "Epoch 904, train_loss: 0.66278 val_loss 0.70692  train_accuracy 0.5966 val_accuracy 0.5173  train_f1 0.5872 val_f1 0.5196\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-905-0.7069.hdf5\n",
      "Epoch 905, train_loss: 0.66275 val_loss 0.70683  train_accuracy 0.5966 val_accuracy 0.5173  train_f1 0.5889 val_f1 0.5120\n",
      "Epoch 906, train_loss: 0.66275 val_loss 0.70682  train_accuracy 0.5968 val_accuracy 0.5174  train_f1 0.5896 val_f1 0.5106\n",
      "Epoch 907, train_loss: 0.66266 val_loss 0.70707  train_accuracy 0.5968 val_accuracy 0.5171  train_f1 0.5890 val_f1 0.5180\n",
      "Epoch 908, train_loss: 0.66265 val_loss 0.70699  train_accuracy 0.5968 val_accuracy 0.5165  train_f1 0.5881 val_f1 0.5214\n",
      "Epoch 909, train_loss: 0.66262 val_loss 0.70700  train_accuracy 0.5968 val_accuracy 0.5172  train_f1 0.5898 val_f1 0.5085\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-910-0.7070.hdf5\n",
      "Epoch 910, train_loss: 0.66258 val_loss 0.70697  train_accuracy 0.5969 val_accuracy 0.5173  train_f1 0.5887 val_f1 0.5127\n",
      "Epoch 911, train_loss: 0.66254 val_loss 0.70704  train_accuracy 0.5971 val_accuracy 0.5164  train_f1 0.5873 val_f1 0.5151\n",
      "Epoch 912, train_loss: 0.66251 val_loss 0.70709  train_accuracy 0.5971 val_accuracy 0.5177  train_f1 0.5897 val_f1 0.5042\n",
      "Epoch 913, train_loss: 0.66248 val_loss 0.70716  train_accuracy 0.5971 val_accuracy 0.5168  train_f1 0.5873 val_f1 0.5206\n",
      "Epoch 914, train_loss: 0.66243 val_loss 0.70717  train_accuracy 0.5972 val_accuracy 0.5170  train_f1 0.5891 val_f1 0.5089\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-915-0.7072.hdf5\n",
      "Epoch 915, train_loss: 0.66242 val_loss 0.70710  train_accuracy 0.5971 val_accuracy 0.5171  train_f1 0.5884 val_f1 0.5147\n",
      "Epoch 916, train_loss: 0.66236 val_loss 0.70714  train_accuracy 0.5973 val_accuracy 0.5176  train_f1 0.5888 val_f1 0.5119\n",
      "Epoch 917, train_loss: 0.66235 val_loss 0.70730  train_accuracy 0.5973 val_accuracy 0.5172  train_f1 0.5897 val_f1 0.5137\n",
      "Epoch 918, train_loss: 0.66232 val_loss 0.70735  train_accuracy 0.5973 val_accuracy 0.5165  train_f1 0.5874 val_f1 0.5241\n",
      "Epoch 919, train_loss: 0.66228 val_loss 0.70727  train_accuracy 0.5974 val_accuracy 0.5169  train_f1 0.5909 val_f1 0.4997\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-920-0.7073.hdf5\n",
      "Epoch 920, train_loss: 0.66229 val_loss 0.70746  train_accuracy 0.5974 val_accuracy 0.5165  train_f1 0.5874 val_f1 0.5240\n",
      "Epoch 921, train_loss: 0.66227 val_loss 0.70750  train_accuracy 0.5973 val_accuracy 0.5164  train_f1 0.5884 val_f1 0.5207\n",
      "Epoch 922, train_loss: 0.66224 val_loss 0.70734  train_accuracy 0.5975 val_accuracy 0.5170  train_f1 0.5911 val_f1 0.5040\n",
      "Epoch 923, train_loss: 0.66217 val_loss 0.70744  train_accuracy 0.5976 val_accuracy 0.5172  train_f1 0.5889 val_f1 0.5180\n",
      "Epoch 924, train_loss: 0.66212 val_loss 0.70740  train_accuracy 0.5977 val_accuracy 0.5170  train_f1 0.5897 val_f1 0.5098\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-925-0.7074.hdf5\n",
      "Epoch 925, train_loss: 0.66210 val_loss 0.70742  train_accuracy 0.5977 val_accuracy 0.5175  train_f1 0.5907 val_f1 0.5086\n",
      "Epoch 926, train_loss: 0.66205 val_loss 0.70747  train_accuracy 0.5977 val_accuracy 0.5168  train_f1 0.5879 val_f1 0.5160\n",
      "Epoch 927, train_loss: 0.66201 val_loss 0.70751  train_accuracy 0.5979 val_accuracy 0.5172  train_f1 0.5895 val_f1 0.5178\n",
      "Epoch 928, train_loss: 0.66199 val_loss 0.70745  train_accuracy 0.5980 val_accuracy 0.5181  train_f1 0.5912 val_f1 0.5077\n",
      "Epoch 929, train_loss: 0.66195 val_loss 0.70770  train_accuracy 0.5979 val_accuracy 0.5175  train_f1 0.5883 val_f1 0.5251\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-930-0.7077.hdf5\n",
      "Epoch 930, train_loss: 0.66194 val_loss 0.70753  train_accuracy 0.5980 val_accuracy 0.5176  train_f1 0.5908 val_f1 0.5092\n",
      "Epoch 931, train_loss: 0.66190 val_loss 0.70746  train_accuracy 0.5980 val_accuracy 0.5167  train_f1 0.5900 val_f1 0.5109\n",
      "Epoch 932, train_loss: 0.66186 val_loss 0.70763  train_accuracy 0.5980 val_accuracy 0.5169  train_f1 0.5886 val_f1 0.5172\n",
      "Epoch 933, train_loss: 0.66182 val_loss 0.70774  train_accuracy 0.5981 val_accuracy 0.5166  train_f1 0.5900 val_f1 0.5232\n",
      "Epoch 934, train_loss: 0.66183 val_loss 0.70765  train_accuracy 0.5982 val_accuracy 0.5171  train_f1 0.5915 val_f1 0.5040\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-935-0.7076.hdf5\n",
      "Epoch 935, train_loss: 0.66181 val_loss 0.70772  train_accuracy 0.5981 val_accuracy 0.5179  train_f1 0.5903 val_f1 0.5062\n",
      "Epoch 936, train_loss: 0.66180 val_loss 0.70770  train_accuracy 0.5981 val_accuracy 0.5165  train_f1 0.5867 val_f1 0.5201\n",
      "Epoch 937, train_loss: 0.66170 val_loss 0.70787  train_accuracy 0.5983 val_accuracy 0.5163  train_f1 0.5897 val_f1 0.5231\n",
      "Epoch 938, train_loss: 0.66168 val_loss 0.70765  train_accuracy 0.5984 val_accuracy 0.5169  train_f1 0.5916 val_f1 0.5059\n",
      "Epoch 939, train_loss: 0.66168 val_loss 0.70779  train_accuracy 0.5983 val_accuracy 0.5169  train_f1 0.5904 val_f1 0.5021\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-940-0.7078.hdf5\n",
      "Epoch 940, train_loss: 0.66169 val_loss 0.70803  train_accuracy 0.5982 val_accuracy 0.5164  train_f1 0.5866 val_f1 0.5273\n",
      "Epoch 941, train_loss: 0.66159 val_loss 0.70786  train_accuracy 0.5985 val_accuracy 0.5166  train_f1 0.5906 val_f1 0.5119\n",
      "Epoch 942, train_loss: 0.66156 val_loss 0.70785  train_accuracy 0.5986 val_accuracy 0.5174  train_f1 0.5916 val_f1 0.5032\n",
      "Epoch 943, train_loss: 0.66157 val_loss 0.70797  train_accuracy 0.5985 val_accuracy 0.5168  train_f1 0.5886 val_f1 0.5143\n",
      "Epoch 944, train_loss: 0.66157 val_loss 0.70803  train_accuracy 0.5985 val_accuracy 0.5167  train_f1 0.5877 val_f1 0.5222\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-945-0.7080.hdf5\n",
      "Epoch 945, train_loss: 0.66147 val_loss 0.70795  train_accuracy 0.5988 val_accuracy 0.5171  train_f1 0.5909 val_f1 0.5112\n",
      "Epoch 946, train_loss: 0.66139 val_loss 0.70807  train_accuracy 0.5987 val_accuracy 0.5166  train_f1 0.5909 val_f1 0.5196\n",
      "Epoch 947, train_loss: 0.66138 val_loss 0.70806  train_accuracy 0.5989 val_accuracy 0.5168  train_f1 0.5927 val_f1 0.5010\n",
      "Epoch 948, train_loss: 0.66137 val_loss 0.70839  train_accuracy 0.5989 val_accuracy 0.5170  train_f1 0.5893 val_f1 0.5263\n",
      "Epoch 949, train_loss: 0.66134 val_loss 0.70807  train_accuracy 0.5990 val_accuracy 0.5172  train_f1 0.5909 val_f1 0.5119\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-950-0.7081.hdf5\n",
      "Epoch 950, train_loss: 0.66128 val_loss 0.70813  train_accuracy 0.5990 val_accuracy 0.5171  train_f1 0.5915 val_f1 0.5123\n",
      "Epoch 951, train_loss: 0.66124 val_loss 0.70814  train_accuracy 0.5990 val_accuracy 0.5166  train_f1 0.5907 val_f1 0.5159\n",
      "Epoch 952, train_loss: 0.66121 val_loss 0.70821  train_accuracy 0.5991 val_accuracy 0.5172  train_f1 0.5913 val_f1 0.5153\n",
      "Epoch 953, train_loss: 0.66118 val_loss 0.70823  train_accuracy 0.5990 val_accuracy 0.5169  train_f1 0.5915 val_f1 0.5153\n",
      "Epoch 954, train_loss: 0.66115 val_loss 0.70824  train_accuracy 0.5991 val_accuracy 0.5168  train_f1 0.5906 val_f1 0.5174\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-955-0.7082.hdf5\n",
      "Epoch 955, train_loss: 0.66113 val_loss 0.70832  train_accuracy 0.5992 val_accuracy 0.5170  train_f1 0.5912 val_f1 0.5165\n",
      "Epoch 956, train_loss: 0.66110 val_loss 0.70832  train_accuracy 0.5994 val_accuracy 0.5172  train_f1 0.5923 val_f1 0.5097\n",
      "Epoch 957, train_loss: 0.66107 val_loss 0.70824  train_accuracy 0.5992 val_accuracy 0.5171  train_f1 0.5909 val_f1 0.5077\n",
      "Epoch 958, train_loss: 0.66103 val_loss 0.70838  train_accuracy 0.5993 val_accuracy 0.5162  train_f1 0.5902 val_f1 0.5154\n",
      "Epoch 959, train_loss: 0.66101 val_loss 0.70840  train_accuracy 0.5994 val_accuracy 0.5167  train_f1 0.5911 val_f1 0.5211\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-960-0.7084.hdf5\n",
      "Epoch 960, train_loss: 0.66099 val_loss 0.70835  train_accuracy 0.5994 val_accuracy 0.5169  train_f1 0.5928 val_f1 0.4998\n",
      "Epoch 961, train_loss: 0.66102 val_loss 0.70843  train_accuracy 0.5995 val_accuracy 0.5171  train_f1 0.5901 val_f1 0.5175\n",
      "Epoch 962, train_loss: 0.66098 val_loss 0.70876  train_accuracy 0.5995 val_accuracy 0.5164  train_f1 0.5890 val_f1 0.5288\n",
      "Epoch 963, train_loss: 0.66094 val_loss 0.70843  train_accuracy 0.5997 val_accuracy 0.5168  train_f1 0.5936 val_f1 0.5107\n",
      "Epoch 964, train_loss: 0.66085 val_loss 0.70850  train_accuracy 0.5996 val_accuracy 0.5180  train_f1 0.5925 val_f1 0.5016\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-965-0.7085.hdf5\n",
      "Epoch 965, train_loss: 0.66087 val_loss 0.70866  train_accuracy 0.5995 val_accuracy 0.5161  train_f1 0.5884 val_f1 0.5253\n",
      "Epoch 966, train_loss: 0.66083 val_loss 0.70858  train_accuracy 0.5996 val_accuracy 0.5162  train_f1 0.5923 val_f1 0.5148\n",
      "Epoch 967, train_loss: 0.66078 val_loss 0.70864  train_accuracy 0.5997 val_accuracy 0.5175  train_f1 0.5937 val_f1 0.5025\n",
      "Epoch 968, train_loss: 0.66073 val_loss 0.70863  train_accuracy 0.5999 val_accuracy 0.5165  train_f1 0.5908 val_f1 0.5126\n",
      "Epoch 969, train_loss: 0.66069 val_loss 0.70864  train_accuracy 0.5998 val_accuracy 0.5165  train_f1 0.5914 val_f1 0.5147\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-970-0.7086.hdf5\n",
      "Epoch 970, train_loss: 0.66065 val_loss 0.70861  train_accuracy 0.5999 val_accuracy 0.5172  train_f1 0.5923 val_f1 0.5100\n",
      "Epoch 971, train_loss: 0.66063 val_loss 0.70871  train_accuracy 0.6000 val_accuracy 0.5170  train_f1 0.5916 val_f1 0.5144\n",
      "Epoch 972, train_loss: 0.66059 val_loss 0.70877  train_accuracy 0.6000 val_accuracy 0.5166  train_f1 0.5913 val_f1 0.5139\n",
      "Epoch 973, train_loss: 0.66059 val_loss 0.70869  train_accuracy 0.5999 val_accuracy 0.5169  train_f1 0.5906 val_f1 0.5145\n",
      "Epoch 974, train_loss: 0.66053 val_loss 0.70871  train_accuracy 0.6001 val_accuracy 0.5172  train_f1 0.5922 val_f1 0.5138\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-975-0.7087.hdf5\n",
      "Epoch 975, train_loss: 0.66051 val_loss 0.70885  train_accuracy 0.6002 val_accuracy 0.5173  train_f1 0.5937 val_f1 0.5064\n",
      "Epoch 976, train_loss: 0.66047 val_loss 0.70890  train_accuracy 0.6002 val_accuracy 0.5157  train_f1 0.5905 val_f1 0.5234\n",
      "Epoch 977, train_loss: 0.66047 val_loss 0.70890  train_accuracy 0.6002 val_accuracy 0.5172  train_f1 0.5931 val_f1 0.5098\n",
      "Epoch 978, train_loss: 0.66047 val_loss 0.70888  train_accuracy 0.6002 val_accuracy 0.5164  train_f1 0.5930 val_f1 0.5014\n",
      "Epoch 979, train_loss: 0.66045 val_loss 0.70920  train_accuracy 0.6003 val_accuracy 0.5150  train_f1 0.5903 val_f1 0.5210\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-980-0.7092.hdf5\n",
      "Epoch 980, train_loss: 0.66036 val_loss 0.70897  train_accuracy 0.6004 val_accuracy 0.5157  train_f1 0.5928 val_f1 0.5200\n",
      "Epoch 981, train_loss: 0.66033 val_loss 0.70894  train_accuracy 0.6004 val_accuracy 0.5168  train_f1 0.5936 val_f1 0.5094\n",
      "Epoch 982, train_loss: 0.66028 val_loss 0.70914  train_accuracy 0.6005 val_accuracy 0.5164  train_f1 0.5924 val_f1 0.5182\n",
      "Epoch 983, train_loss: 0.66024 val_loss 0.70907  train_accuracy 0.6006 val_accuracy 0.5166  train_f1 0.5928 val_f1 0.5159\n",
      "Epoch 984, train_loss: 0.66021 val_loss 0.70905  train_accuracy 0.6006 val_accuracy 0.5170  train_f1 0.5931 val_f1 0.5091\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-985-0.7090.hdf5\n",
      "Epoch 985, train_loss: 0.66018 val_loss 0.70916  train_accuracy 0.6006 val_accuracy 0.5157  train_f1 0.5915 val_f1 0.5181\n",
      "Epoch 986, train_loss: 0.66017 val_loss 0.70922  train_accuracy 0.6008 val_accuracy 0.5172  train_f1 0.5928 val_f1 0.5140\n",
      "Epoch 987, train_loss: 0.66012 val_loss 0.70916  train_accuracy 0.6008 val_accuracy 0.5166  train_f1 0.5920 val_f1 0.5166\n",
      "Epoch 988, train_loss: 0.66010 val_loss 0.70919  train_accuracy 0.6009 val_accuracy 0.5178  train_f1 0.5942 val_f1 0.5003\n",
      "Epoch 989, train_loss: 0.66012 val_loss 0.70913  train_accuracy 0.6007 val_accuracy 0.5155  train_f1 0.5908 val_f1 0.5169\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-990-0.7091.hdf5\n",
      "Epoch 990, train_loss: 0.66010 val_loss 0.70941  train_accuracy 0.6008 val_accuracy 0.5154  train_f1 0.5916 val_f1 0.5206\n",
      "Epoch 991, train_loss: 0.66003 val_loss 0.70943  train_accuracy 0.6007 val_accuracy 0.5169  train_f1 0.5925 val_f1 0.5244\n",
      "Epoch 992, train_loss: 0.66000 val_loss 0.70932  train_accuracy 0.6010 val_accuracy 0.5163  train_f1 0.5950 val_f1 0.5145\n",
      "Epoch 993, train_loss: 0.65995 val_loss 0.70925  train_accuracy 0.6009 val_accuracy 0.5165  train_f1 0.5933 val_f1 0.5117\n",
      "Epoch 994, train_loss: 0.65993 val_loss 0.70930  train_accuracy 0.6011 val_accuracy 0.5174  train_f1 0.5939 val_f1 0.5031\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-995-0.7093.hdf5\n",
      "Epoch 995, train_loss: 0.65994 val_loss 0.70950  train_accuracy 0.6008 val_accuracy 0.5165  train_f1 0.5916 val_f1 0.5204\n",
      "Epoch 996, train_loss: 0.65988 val_loss 0.70947  train_accuracy 0.6010 val_accuracy 0.5154  train_f1 0.5923 val_f1 0.5177\n",
      "Epoch 997, train_loss: 0.65984 val_loss 0.70929  train_accuracy 0.6012 val_accuracy 0.5173  train_f1 0.5942 val_f1 0.5060\n",
      "Epoch 998, train_loss: 0.65981 val_loss 0.70952  train_accuracy 0.6012 val_accuracy 0.5167  train_f1 0.5928 val_f1 0.5144\n",
      "Epoch 999, train_loss: 0.65976 val_loss 0.70948  train_accuracy 0.6012 val_accuracy 0.5163  train_f1 0.5933 val_f1 0.5108\n",
      "saved to  ./output/MLPModel01_20170423_1800_EURUSD_DS3_20092014_LA1_F99_C2_L500_500_DO0-1000-0.7095.hdf5\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 1000\n",
    "hist = model.fit(\n",
    "    X_train.as_matrix(), Y_train, \n",
    "    validation_data=(X_dev.as_matrix(),Y_dev), \n",
    "    max_epochs=max_epochs,\n",
    "    es_patience=1000, \n",
    "    es_min_delta=1e-5,\n",
    "    batch_size=5*1024*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.plot(model.progress_callback.train_losses[-100:], label='train_loss')\n",
    "plt.plot(model.progress_callback.validation_losses[-110:], label='validation_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#plt.plot(model.progress_callback.train_f1s, label='train_f1')\n",
    "#plt.plot(model.progress_callback.validation_f1s, label='validation_f1')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train_pred = model.predict(X_train.as_matrix(), batch_size=1024)\n",
    "# Y_train_pred_class = utils.prediction_to_category2(Y_train_pred)\n",
    "\n",
    "Y_dev_pred = model.predict(X_dev.as_matrix(), batch_size=1024)\n",
    "# Y_dev_pred_class = utils.prediction_to_category2(Y_dev_pred)\n",
    "\n",
    "Y_test_pred = model.predict(X_test.as_matrix(), batch_size=1024)\n",
    "# Y_test_pred_class = utils.prediction_to_category2(Y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random forest model\n",
    "# Y_test_pred = rf_model.predict(X_test)\n",
    "# Y_test_pred_class = utils.prediction_to_category2(Y_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adaboost model\n",
    "# Y_test_pred = adb_model.predict(X_test)\n",
    "# Y_test_pred_class = utils.prediction_to_category2(Y_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gaussian naive bayes model\n",
    "# Y_test_pred = gnb_model.predict(X_test)\n",
    "# Y_test_pred_class = utils.prediction_to_category2(Y_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#performance_report(\"train\", prices_train, lookahead, Y_train, Y_train_pred_class, cum_return_plot=True, heatmap=True, histogram=True)\n",
    "performance_report(\"dev\",  prices_dev,  lookahead, Y_dev, Y_dev_pred, cum_return_plot=True, heatmap=True, histogram=True)\n",
    "#performance_report(\"test\",  prices_test,  lookahead, Y_test, Y_test_pred, cum_return_plot=True, heatmap=True, histogram=True)\n",
    "\n",
    "#train_curve = precision_recall_curve(Y_train, Y_train_pred)\n",
    "#test_curve = precision_recall_curve(Y_test, Y_test_pred)\n",
    "\n",
    "# plt.plot(train_curve[0], train_curve[1], label='train')\n",
    "# plt.plot(test_curve[0], test_curve[1], label='test')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# sns.heatmap(confusion_matrix(Y_test, Y_test_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout = 0  , lookahead = 1\n",
    "\n",
    "| width | layers    | epochs | train | test |\n",
    "| -----: | ---- :     | ---:    | ---:   | ---:  |\n",
    "| 1     | 1         |  100   | .57   | .58  |\n",
    "| 1     | 100       |  100   | .58   | .59  |\n",
    "| 1     | 100       |  500   | .57   | .59  |\n",
    "| 1     | 100 x 100 |  100 | .58   | .59  |\n",
    "| 10    | 1         |  100 |  .57  | .59  |\n",
    "| 10    | 10        |  100 |  .57  | .58  |\n",
    "| 10    | 100       |  100 |  .58 | .58   |\n",
    "| 10    | 100 x 100 |  100 |  .60 | .58   |\n",
    "| 10    | 100 x 100 x 100 |  100 |  .62 | .57   |\n",
    "| 10    | 100 x 100 x 100 x 100 |  100 |  .63 | .56   |\n",
    "| 20    | 1 |  100 |  .56 | .58  |\n",
    "| 20    | 100 |  100 | .58  | .58  |\n",
    "| 40    | 100 |  500 x 500 x 500 | .60  | .61  |\n",
    "\n",
    "Dropout = 0.1, lookahead = 1 \n",
    "\n",
    "| width | layers    | epochs | train | test |\n",
    "| -----: | ---- :     | ---:    | ---:   | ---:  |\n",
    "| 10    | 100 x 100 x 100 x 100 |  100 | .60  | .58  |\n",
    "| 10    | 100 x 100 x 100 x 100 |  200 | .62  | .58  |\n",
    "\n",
    "\n",
    "Dropout = 0.2, lookahead = 1 \n",
    "\n",
    "| width | layers    | epochs | train | test |\n",
    "| -----: | ---- :     | ---:    | ---:   | ---:  |\n",
    "| 10    | 100 x 100 x 100 x 100 |  100 | .59 | .59  |\n",
    "| 10    | 100 x 100 x 100 x 100 |  200 | .60 | .58  |\n",
    "\n",
    "Dropout = 0.5, lookeahead = 1\n",
    "\n",
    "| width | layers    | epochs | train | test |\n",
    "| -----: | ---- :     | ---:    | ---:   | ---:  |\n",
    "| 60    | 500 x 500 x 500 x 500 |  200 | .64 | .62|\n",
    "| 60    | 100 x 100 x 100 x 100 |  200 | .64 | .65\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "- .681 / .57 100^3 / LA:1, W:60\n",
    "- .679 / .569 100^4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print( len(Y_test) )\n",
    "print( len(Y_test_pred_class) )\n",
    "print( len(prices_test))\n",
    "print( len(utils.future_return(prices_test, 1) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idea: look at mean future return on *perfect* 1min prediction. (or on last minute direction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sqrt(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i_s = []\n",
    "returns = (prices_test-prices_test.mean()).pct_change()\n",
    "mean_rets = []\n",
    "for a in np.arange(1,15,.1):\n",
    "    i = int(a*a)\n",
    "    test_returns = utils.future_return(prices_test, i).fillna(0).values\n",
    "    test_returns = test_returns - test_returns.mean()\n",
    "\n",
    "    # just use sign of latest return as predictor\n",
    "    idx = np.sign(returns)\n",
    "\n",
    "    mean_ret = (test_returns * idx).mean() * 1e4\n",
    "    mean_rets.append(mean_ret)\n",
    "    i_s.append(i) #*24))\n",
    "plt.plot(i_s, mean_rets,'.-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i_s = []\n",
    "mean_rets = []\n",
    "for a in np.arange(1,15,.1):\n",
    "    i = int(a*a)\n",
    "    test_returns = utils.future_return(prices_test, i).fillna(0).values\n",
    "    test_returns = test_returns - test_returns.mean()\n",
    "\n",
    "    idx = np.zeros(len(test_returns))\n",
    "    idx[Y_test_pred_class == 0] = -1\n",
    "    idx[Y_test_pred_class == 1] = 1\n",
    "\n",
    "    mean_ret = (test_returns * idx).mean() * 1e4\n",
    "    mean_rets.append(mean_ret)\n",
    "    i_s.append(i) #*24))\n",
    "plt.plot(i_s, mean_rets,'.-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ser = pd.Series(mean_rets, index = pd.DatetimeIndex(np.array(i_s)* 60e9))\n",
    "ser.plot()\n",
    "#plt.plot(i_s, mean_rets,'.-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prices_test.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_trade = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "18000 / (60*24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# load test dataset\n",
    "X_test, Y_test, prices_test = datasets.load(datasets.filename(dataset_name, lookahead, window, sym, 2012))\n",
    "Y_test_pred = model.predict(X_test.as_matrix(), batch_size=1024)\n",
    "\n",
    "performance_report(\"test\",  prices_test,  lookahead, Y_test, Y_test_pred, cum_return_plot=True)\n",
    "\n",
    "\n",
    "\n",
    "# TODO: demean fut_return.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random forest model\n",
    "# load test dataset\n",
    "X_test, Y_test, prices_test = datasets.load(datasets.filename(dataset_name, lookahead, window, sym, 2014))\n",
    "#X_test, Y_test, prices_test, fut_return_test = datasets.prepare_dataset2(df=datasets.random_ohlc(300000),lookahead=1, window=25)\n",
    "\n",
    "performance_report(\"RF model\",  prices_test,  lookahead, Y_test, rf_model.predict(X_test), cum_return_plot=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adaboost model\n",
    "# load test dataset\n",
    "X_test, Y_test, prices_test, fut_return_test = datasets.load(datasets.filename(dataset_name, lookahead, window, sym, 2014))\n",
    "#X_test, Y_test, prices_test, fut_return_test = datasets.prepare_dataset2(df=datasets.random_ohlc(300000),lookahead=1, window=25)\n",
    "Y_test_pred = adb_model.predict(X_test)\n",
    "Y_test_pred_class = utils.prediction_to_category2(Y_test_pred)\n",
    "performance_report(\"test\",  prices_test,  lookahead, Y_test, Y_test_pred_class)\n",
    "(fut_return_test.ix[Y_test_pred_class[:]]+1).cumprod().plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(fut_return_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
